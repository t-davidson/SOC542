---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Interactions"
author: Thomas Davidson
institute: Rutgers University
date: February 27, 2023
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

set.seed(08901)

library(ggplot2)
library(tidyverse)
library(latex2exp)
library(modelsummary)

#options("modelsummary_format_numeric_latex" = "plain")
options(modelsummary_format_numeric_latex = "mathmode")
```

# Plan
- Introducing interactions
- Types of interactions and their interpretations
- Marginal effects

# Updates
- Homework 2 due tomorrow at 5pm
    - Submit using Github
- Project proposals due next Tuesday (3/7) at 5pm
    - Recommend meeting to discuss plan
    - Submit via email as PDF

# Introducing interactions
## What is an statistical interaction?
- Consider the following population model:

$$y = \beta_0 + \beta_1x + \beta_2z + u$$


- The coefficients $\beta_1$ and $\beta_2$ measure the relationship between $x$ and $y$ and $z$ and $y$, respectively.
    - The interpretation of either coefficient requires that we hold the other constant.
- *But what if we expect the effect of $x$ to vary as a function of $z$?*

    
# Introducing interactions
## What is an statistical interaction?
- If we expect there to be an \textbf{interaction} between $x$ and $z$, such that the effect of $x$ on $y$ varies according to the level of $z$, we can add an \textbf{interaction term} into our model formula.

$$y = \beta_0 + \beta_1x + \beta_2z + \beta_3xz + u$$

- $\beta_1$ and $\beta_2$ are now considered as the \textbf{main effects}. 
- $\beta_3$ is the coefficient for the interaction term, representing the effect of $x$ *times* $z$.
  
# Introducing interactions
## A simple population model
```{r simple-int, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
x <- rnorm(N)
z <- rnorm(N)
y <- 3*x + 2*z + -5*(x*z) + rnorm(N, 10)
```

# Introducing interactions
## Comparing models
```{r simple-model, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m.r <- lm(y ~ x + z)
m <- lm(y ~ x + z + x:z)
modelsummary(list(m.r, m), gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Introducing interactions
## Example: intersectional inequalities
- We can use interaction terms as a way to encode theoretical knowledge about the relationship between variables.
- For example, if we expect there to be differences in income related to the interaction between sex and race, we can add an interaction term to a model:

$$Income = \beta_0 + \beta_1Sex + \beta_2Race + \beta_3Age + \beta_4Sex*Race + u$$

# Introducing interactions
## Main effects and interactions
- In general, it is recommended to *include the main effects in any model with interactions*.
    - Type II errors are more likely when interpreting interaction terms with main effects omitted.
    - The interpretation of the model can change substantially if main effects are excluded.\footnote{\footnotesize See this Stata blog for further discussion: https://stats.oarc.ucla.edu/stata/faq/what-happens-if-you-omit-the-main-effect-in-a-regression-model-with-an-interaction/}

# Types of interactions
## Dummy-dummy

$$y = \beta_0 + \beta_1 Male + \beta_2 Degree + \beta_3 Male*Degree + u $$

# Types of interactions
## Dummy-dummy
```{r dd, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Loading data
gss <- haven::read_dta("../../2022/labs/lab-data/GSS2018.dta") %>%
    filter(age <= 89) %>% haven::zap_labels() %>%
    mutate(degree = ifelse(degree >= 3, 1, 0), # resp has BA or grad degree
           sex = ifelse(sex == 1, 1, 0),
           race = as.factor(race),
           realrinc = realrinc/1000)
gss$race <- recode_factor(gss$race, "1" = "White", "2" = "Black", "3" = "Other")

m00 <- lm(realrinc ~ sex, data = gss %>% drop_na(degree))
m0 <- lm(realrinc ~ degree, data = gss)
m <- lm(realrinc ~ sex + degree, data = gss)
m.i <- lm(realrinc ~ sex + degree + sex:degree, data = gss)
modelsummary(list(m00, m0, m, m.i), gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Dummy-dummy

$$y = \beta_0 + \beta_1 Male + \beta_2 Degree + \beta_3 Male*Degree + u $$

- Female and those without a college degree are the reference categories.
- $\beta_1$ and $\beta_2$ represent the main effects of sex and degree on the outcome.
- The coefficient $\beta_3$ represents the expected difference in the effect of degree for men versus women.\footnote{\footnotesize Note the symmetrical interpretation here: the  difference in the effect of sex for college degree versus non-college degree. See McElreath 8.2 for further discussion.}
- The expected income for a male with a degree is $\beta_0 + \beta_1 + \beta_2 + \beta_3$. The same quantity for a female with a degree is $\beta_0 + \beta_2$.

# Types of interactions
## Continuous-dummy
$$y = \beta_0 + \beta_1 Age + \beta_2 Sex + \beta_3 Age * Sex + u $$

# Types of interactions
## Continuous-dummy
```{r cd, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ age + sex, data = gss)
m.i <- lm(realrinc ~ age + sex + age:sex, data = gss)
modelsummary(list(m, m.i), gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Continuous-dummy
$$y = \beta_0 + \beta_1 Age + \beta_2 Sex + \beta_3 Age * Sex + u $$


- The coefficients $\beta_1$ and $\beta_2$ represent the main effects of age and sex on income.
- For females, $\beta_1$ represents the relationship between age and income. For males, the relationship is $\beta_1$ + $\beta_3$.
    - Thus, the interaction term allows the *slope* to vary according to sex.
    
# Types of interactions
## Continuous-continuous
$$y = \beta_0 + \beta_1 Age + \beta_2 Educ + \beta_3 Age * Educ + u $$

# Types of interactions
## Continuous-continuous
```{r cc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ age + educ, data = gss)
m.i <- lm(realrinc ~ age + educ + age:educ, data = gss)
modelsummary(list(m, m.i), gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Continuous-continuous
$$y = \beta_0 + \beta_1 Age + \beta_2 Educ + \beta_3 Age * Educ + u $$

- The intercept no longer has a meaningful interpretation (income when age and education equal zero).
    - GHV 12.2 discuss standardization to make intercepts more interpretable in such contexts.
- $\beta_1$ and $\beta_2$ represent the main effects of age and education.
- The interaction term $\beta_3$ captures how the effect of education on income varies as a function of age.


# Types of interactions
## Continuous-continuous
- The effect of education on income is now also a function of age:

$$\frac{\Delta y}{\Delta_{Educ}} = \beta_2 + \beta_3Age$$

- Similarly,

$$\frac{\Delta y}{\Delta_{Age}} = \beta_1 + \beta_3Educ$$


# Types of interactions
## Continuous-continuous
- If Age changes by $\Delta$Age and Educ by $\Delta$Educ, the expected change in $y$ is:

$$\Delta y = (\beta_1 + \beta_3Educ)\Delta Age + (\beta_2 + \beta_3 Age)\Delta Educ + \beta_3 \Delta Age \Delta Educ$$

- The coefficient $\beta_3$ represents the effect of a unit increase in age *and* education, beyond the sum of the individual effects of unit increases alone.

# Types of interactions
## Dummy-categorical
```{r dc, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + race, data = gss)
m.i <- lm(realrinc ~ sex + race + sex:race, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Dummy-categorical
$$y = \beta_0 + \beta_1Male + \beta_2Black + \beta_3Other + \beta_4Black\:Male + \beta_5 Other\:Male + u$$

- There is a separate coefficient for the interaction between the dummy variable and each of the categories, with the exception of the reference group.
- The interpretation is the same as the dummy-dummy model.

# Types of interactions
## Continuous-categorical
```{r ct, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
gss$race <- as.factor(gss$race)
m <- lm(realrinc ~ age + race, data = gss)
m.i <- lm(realrinc ~ age + race + age:race, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Categorical-categorical
```{r tt, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Ref = word of god, 2 = inspired word, 3 = ancient book
gss.b <- gss %>% filter(bible <= 3) %>% mutate(bible = as.factor(bible))
levels(gss.b$bible) <- c("Word of God", "Inspired Word", "Ancient Book")
m <- lm(realrinc ~ race + bible, data = gss.b)
m.i <- lm(realrinc ~ race + race:bible, data = gss.b)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Three-way interactions
```{r three, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + race + degree, data = gss.b)
m.i <- lm(realrinc ~ sex + race + degree + sex:race:degree, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Interpreting interactions
- Interactions terms make models more challenging to interpret.
    - Like polynomial regression, the effect of a single predictor is represented by more than one coefficient (e.g. $y = \beta_0 + \beta_1x + \beta_2z + \beta_3xz + u$).
- Three-way and more complex interactions are even more difficult to interpret and should be avoided unless there are strong theoretical reasons to use them.

# Marginal effects
## Definitions
- A \textbf{marginal effect} is the relationship between change in single predictor and the dependent variable while *holding other variables constant*.
- The \textbf{average marginal effect (AME)} is the *average* change in the outcome $y$ as a function of a unit change in $x_i$ over all observations.
    - Coefficients in a standard OLS model represent average marginal effects.
- This quantity becomes more complicated to calculate when interaction terms are included, since the effect of a change in $x_i$ now depends on multiple parameters.

# Marginal effects
## Computing marginal effects
- Frequentist marginal effects computed by calculating *partial derivatives* and variance approximations are used to construct confidence intervals.
    - e.g. $ME(x_i) = \frac{\delta y}{\delta x_i}$.
    - We can use the `margins` package in R to do this.\footnote{\footnotesize See  \href{https://cran.r-project.org/web/packages/margins/vignettes/TechnicalDetails.pdf}{documentation} for the margins package for further details.}
- Bayesian marginal effects can be calculated by sampling from the posterior distribution.

# Marginal effects
## Marginal effects and OLS regression
```{r margins-model1, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + educ, data = gss)
modelsummary(list(m), gof_omit=".*", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Marginal effects
## Marginal effects and OLS regression
Note how the average marginal effects are equal to the OLS coefficients.
```{r margins1, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(margins)
me <- margins(m)
summary(me)
```

# Marginal effects
## Marginal effects with non-linear variables
```{r margins-model2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m2 <- lm(realrinc ~ sex + age + I(age^2) + educ, data = gss)
modelsummary(list(m, m2), gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "markdown", estimate = "{estimate}{stars}")
```

<!--Why does age2 coefficient not show?,
\footnote{\footnotesize Note the use of the `I` symbol when computing age-squared. This ensures that the margins command recognizes that this variable is a function of `age`.}
-->

# Marginal effects
## Marginal effects with non-linear variables
The margins commands are the same as above. Note how the AME now represents the total effect of age across the two parameters. There is no separate marginal effect for age squared.
```{r margins-nl, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
me <- margins(m2)
summary(me)
```

# Marginal effects
## Marginal effects with non-linear variables
We can also visualize the marginal effect of age in a continuous space, highlighting how it incorporates the squared term.
```{r margins-nl-plot, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
cplot(m2, "age")
```

# Marginal effects
## Marginal effects with interactions
```{r margins-model31, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + I(age^2) + educ + sex:educ + sex:age,
        data = gss)
```

```{r margins-model32, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list(m), gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "markdown", estimate = "{estimate}{stars}")
```

# Marginal effects
## Marginal effects with interactions
In this case, we can isolate the average marginal effect of each predictor.
```{r margins3, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
me <- margins(m)
summary(me)
```

# Marginal effects
## Plotting marginal effects
The `margins` package includes a `plot()` function to show the results of the table. The output can also be modified using `ggplot2`.
```{r margins-bars, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
plot(me)
```

# Marginal effects
## Plotting conditional marginal effects
The `cplot` function shows conditional marginal effects. Here the ME of sex on income over the range of age.
```{r margins-line2, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
cplot(m, x = "age", dx = "sex", what = "effect")
```

# Marginal effects
## Computing marginal effects the Bayesian way
- Unlike frequentist marginal effects, there is no need for additional calculus.
- Marginal effects can be computed directly from the posterior distribution. 

# Marginal effects
## Bayesian estimation
First, we can use `stan_glm` to estimate the same model.
```{r bayes1, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
library(rstanarm)
m.b <- stan_glm(realrinc ~ sex + age + I(age^2) + educ + sex:educ + sex:age, data = gss, family = "gaussian", chains =1 , refresh = 0)
modelsummary(list("OLS" = m, "Bayesian" = m.b), statistic = "conf.int", 
             gof_omit = "Log.Lik.|ELPD|LOO|WAIC|RMSE|AIC|BIC|F|R2|Num.Obs.",
             coef_omit = c(1),
             stars = F,
             output = "markdown")
```


# Marginal effects
## Bayesian marginal effects
Fortunately for us, the `margins` command also works for Bayesian models! The esimated AMEs are very close.
```{r bayesmargins, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
margins(m) # Frequentist
margins(m.b) # Bayesian
```

# Marginal effects
## Bayesian marginal effects
To obtain the information used in these calculations, we can compute the *expected value* of the outcome at different levels of predictors using `epred_draws`. 
```{r bayes3, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
library(gridExtra)
library(tidybayes)

data.range <- expand_grid(sex = c(0,1),
                         educ = 1:20,
                         age = 18:80)

tidy_epred <- m.b %>% epred_draws(newdata = data.range)
```

# Marginal effects
## Bayesian marginal effects
Like everything else we obtain from a Bayesian model, these marginal effects have a posterior distribution.
```{r bayes.head, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
tail(tidy_epred %>% select(sex, educ, age, .epred))
```


<!-- Old version. Now using cooler posterior density gradient :)
# Marginal effects
## Bayesian marginal effects
```{r bayes4old, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
p1 <- ggplot(tidy_epred %>% filter(sex == 1), aes(x = age, y = .epred)) + stat_lineribbon() + theme_tidybayes() + theme(legend.position="none") + scale_fill_brewer() + ylim(-40,60) + 
    labs(y = "Predicted income (thousands)",
         x = "Age",
         title = "Male")
p2 <- ggplot(tidy_epred %>% filter(sex == 0), aes(x = age, y = .epred)) + stat_lineribbon() + theme_tidybayes() + theme(legend.position="none") + scale_fill_brewer() + ylim(-40,60) + 
    labs(y = "",
         x = "Age",
         title = "Female")
grid.arrange(p1, p2, nrow = 1)
```
-->


# Marginal effects
## Bayesian marginal effects
We can then directly plot conditional marginal effects and associated uncertainty.
```{r bayes4, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
p1 <- ggplot(tidy_epred %>% filter(sex == 1), aes(x = age, y = .epred, fill = after_stat(.width))) + stat_lineribbon(.width = ppoints(50)) + theme_tidybayes() + theme(legend.position="none") + ylim(-40,60) +
    scale_fill_distiller() +
    labs(y = "Predicted income (thousands)",
         x = "Age",
         title = "Male")
p2 <- ggplot(tidy_epred %>% filter(sex == 0), aes(x = age, y = .epred, fill = after_stat(.width))) + stat_lineribbon(.width = ppoints(50)) + theme_tidybayes() + theme(legend.position="none") + ylim(-40,60) +
    scale_fill_distiller() + ylim(-40,60) + 
    labs(y = "",
         x = "Age",
         title = "Female")
grid.arrange(p1, p2, nrow = 1)
```

<!--Single plot with both lines
# Marginal effects
## Bayesian marginal effects
```{r bayes4alt2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(tidy_epred, aes(x = age, y = .epred, color = sex,  group = sex, fill = after_stat(.width))) + stat_lineribbon(.width = ppoints(50)) + theme_tidybayes() + theme(legend.position="none") +
    scale_fill_distiller() +
    labs(y = "Predicted income (thousands)",
         x = "Age",
         title = "Sex differences in age-income curve")
```
-->


# Marginal effects
## Marginal effects and generalized linear models
- In generalized linear models (GLMs), which will be our main focus after spring break, the coefficients often do not have clear interpretations on the outcome scale, making marginal effects even more important for interpretation.

# Next week
## Topic
- Missing data and imputation
- Model robustness

# Lab
- Specifying and interpreting interaction terms 