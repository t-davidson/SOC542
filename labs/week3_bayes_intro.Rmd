---
title: "Week 3 - Bayesian Regression"
author: "Fred Traylor, Lab TA"
date: "2/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstanarm)

if(!require(latex2exp))          # If latex2exp package isn't intalled...
  install.packages("latex2exp")
library(latex2exp)

if(!require(tidybayes))          # If tidybayes package isn't intalled...
  install.packages("tidybayes")
library(tidybayes)

```

We're going to be using the `kidiq` dataset. A subset of data from the National Longitudinal Survey of Youth. You can read a little more about it here: https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html

```{r data, echo=TRUE, include=TRUE}
data(kidiq)
ggplot(kidiq,
       aes(x = mom_iq,
           y = kid_score)) +
  geom_point() 
```
# OLS model
Let's start by estimating a simple OLS model.

```{r ols-model}
model_ols <- lm(kid_score ~ mom_iq, data = kidiq)
summary(model_ols)
```


# Bayesian regression with `stan_glm`
We will use the `stan_glm` function from `rstanarm` to estimate a series of regression models using the Bayesian approach and will consider how the choice of prior affects the results. 

The `family` argument in the code below specifies the type of model. In this case, `gaussian(link = "identity")` corresponds to an OLS model. We will see how this argument is used to specify different kinds of models later in the course.

Let's start with a uniform or "flat" prior. This assumes we are completely ignorant about the parameters of the model. We can specify this by setting `prior = NULL` and `prior_intercept = NULL`. There is also a prior on `sigma` but we will not modify it for now.

Running this will print information showing the output of the Hamiltonian Monte Carlo estimation procedure. In this case, we use a single chain with 2000 iterations. The first 1000 are used to optimize the model, the second 1000 to get estimates from the posterior.
```{r no-prior}
bayes_uniform <- stan_glm(kid_score ~ mom_iq, data = kidiq,
                        family = gaussian(link = "identity"), 
                        prior = NULL,
                        prior_intercept = NULL,
                        seed = 12345,  # Seed makes it reproducible
                        chains = 1,
                        iter = 2000,   # How many total iterations?
                        warmup = 1000) # How many of those are to optimize the model first?
```

We can view a short summary of the results by printing the model object.
```{r unform-prior-summary}
print(bayes_uniform)
```


We can view information about the priors we used by running the `prior_summary` command.
```{r uniform-summary}
prior_summary(bayes_uniform)
```

We can compare the coefficients of this model to the OLS. We see that the estimates are very close.
```{r uniform-compare-ols}
coefficients(bayes_uniform)
coefficients(model_ols)
```

## Viewing the MCMC chains
To understand the estimation procedure we can view the trace plot. This shows how the estimate of the parameters in the model shifted during the estimation process. These plots and various related statistics are often used to diagnose potential issues that can arise in the estimation process. The plot shows the estimated value of the slope at each iteration in the MCMC process.

Unlike the example in lecture with two chains, here we just use a single chain. The models below all use the default 4 chains (see McElreath C9 for discussion of why we might want to vary the number of chains).

This plot also communicates some of the uncertainty surrounding the parameter $\beta_1$. We can see that the estimate seems to bounce around an average value. 

Note: `rstanarm` discards the "warmup" samples used to calibrate the HMC algorithm. We therefore see that the chain has already converged towards the final value.
```{r trace}
plot(bayes_uniform, "trace", pars = "mom_iq") + 
  labs(y = TeX("$\\beta_1$"), 
       x = "Iteration")
```

## Weakly Informative Priors
Let's rerun this same model but with some slightly more informative priors. Let's assume that we expect the coefficient for `mom_iq` to have a Normal distribution with mean 0 and standard deviation 1. In this case, we will ignore the intercept and let `rstanarm` select a default prior. We'll also keep the default arguments for the number of chains and iterations.

The argument `refresh = 0` is added to avoid printing out all of the estimation information we saw in the last example.
```{r weak-prior}
bayes_weak_prior <- stan_glm(kid_score ~ mom_iq, data = kidiq,
                             family = gaussian(link = "identity"),
                             prior = normal(location = 0,   # location = mean
                                            scale = 1),     # scale = sd (not quite, but a good way to think about it)
                             seed = 12345,  # Seed makes it reproducible
                             refresh = 0)
print(bayes_weak_prior)
```

Now let's compare the coefficients across the three models.
```{r weak-prior}
coefficients(model_ols)
coefficients(bayes_uniform)
coefficients(bayes_weak_prior)
```

## Strong priors
Let's see what happens if the priors are even stronger. Let's encode a prior that assumes a strong positive relationship between child and mother's IQ. Think for a moment about the implications of such an assumption. We can encode this by adjusting the prior on the coefficient. 
```{r strong-prior}
bayes_strong_prior <- stan_glm(kid_score ~ mom_iq, data = kidiq,
                               family = gaussian(link = "identity"),
                               prior = normal(location = 10,
                                            scale = 1),
                               seed = 12345, 
                               refresh = 0)
print(bayes_strong_prior)
```


# Comparing the models
Let's compare the three models.
```{r model-comparisons}
library(viridis)
ggplot(kidiq,
       aes(x = mom_iq,
           y = kid_score)) +
  geom_point() +
  geom_abline(aes(slope = bayes_uniform[["coefficients"]][["mom_iq"]],
              intercept = bayes_uniform[["coefficients"]][["(Intercept)"]],
              color = "Uniform Prior"), size = 1, alpha = .75) +
  geom_abline(aes(slope = bayes_weak_prior[["coefficients"]][["mom_iq"]],
              intercept = bayes_weak_prior[["coefficients"]][["(Intercept)"]],
              color = "Weak Prior"), size = 1, alpha = .75) +
  geom_abline(aes(slope = bayes_strong_prior[["coefficients"]][["mom_iq"]],
              intercept = bayes_strong_prior[["coefficients"]][["(Intercept)"]],
              color = "Strong Prior"), size = 1, alpha = .75) +
  geom_abline(aes(slope = model_ols[["coefficients"]][["mom_iq"]],
              intercept = model_ols[["coefficients"]][["(Intercept)"]],
              color = "OLS Model"), size = 1, alpha = .75) +
  scale_color_viridis_d() +
  theme_light()
```

# Post-Estimation Visualization 
We can visualize the uncertainity around estimates from the Bayesian models by analyzing the posterior distribution. The `gather_draws` function is used to produce a table showing the estimates of the coefficient for mom_iq produced by the MCMC estimation procedure. 

The `tidybayes` package has lots of useful functions for plotting these results. In this case, we can see the entire posterior distribution, along with a summary. The point indicates the *median* of the posterior distribution and the bars the 66% and 95% *highest posterior density intervals*.
```{r post-model-viz1}
d1 <- bayes_uniform %>% 
  gather_draws(mom_iq)
head(d1)

d1 %>%
  ggplot(aes(y = .variable, x = .value)) +
  stat_halfeye() +
  theme_tidybayes() + 
  labs(x = "Coefficient", y = "Variable")
```

We can calculate the same summaries for the other models and plot them together.
```{r post-model-viz2}
d2 <- bayes_strong_prior %>% 
  gather_draws(mom_iq)

d3 <- bayes_weak_prior %>% 
  gather_draws(mom_iq)

d1$model <- "Uniform"
d2$model <- "Weak"
d3$model <- "Strong"

d <- bind_rows(d1, d2, d3)

o <- broom::tidy(model_ols)
ols_coef <- o[[2, "estimate"]]
ols_se <- o[[2, "std.error"]]

d %>%
  ggplot() +
  stat_halfeye(aes(
    y = model, 
    x = .value, 
    group = model, 
    fill = model)) +
  geom_pointrange(aes(
    y = "OLS",
    x = ols_coef,
    xmin = ols_coef - 1.96 * ols_se,
    xmax = ols_coef + 1.96 * ols_se,
    )) +
  scale_fill_viridis_d() + 
  theme_tidybayes() +
  labs(y = "Model",
       x = "Coefficient Estimate") +
  theme(legend.position = "none")
  
```

Although the results are slightly different, this demonstrates how the data can overwhelm the prior quite easily. You might want to repeat this exercise with a smaller sample of the data to see how priors make more difference in small samples.

Here's another example of a way we can express the uncertainty contained in the estimates.
```{r post-model-viz3}
d %>%
  ggplot() +
  stat_gradientinterval(aes(
    y = model,
    x = .value, 
    group = model, 
    fill = model)) + 
    geom_pointrange(aes(
    y = "OLS",
    x = ols_coef,
    xmin = ols_coef - 1.96 * ols_se,
    xmax = ols_coef + 1.96 * ols_se,
    )) +
  scale_fill_viridis_d() +
  labs(y = "Model",
       x = "Coefficient Estimate") +
  theme(legend.position = "none")
```

# Posterior predictive checks
One way to evaluate the performance of our models is simulate data from the posterior distribution and to compare it to the original data. This is called *posterior predictive* check. 

The dark line shows the distribution of the outcome, child's IQ. The blue lines show 100 simulated draws from the posterior distribution. We can see a couple of useful takeaways here. First, the shape of the outcome is asymmetrical. Second, we can see that the density of the draws is a little to the left of the peak of the outcome.
```{r posterior-check}
pp_check(bayes_weak_prior)
```

Let's compare the mean and standard deviation for samples and the data to get a better look at this. 
```{r posterior-check}
pp_check(bayes_weak_prior, plotfun = "stat_2d", stat = c("mean", "sd"))
```

# Prior predictive simulations
One of the ways we can understand our priors is to use a *prior predictive simulation*. This allows us to see simulated data from our model before it has seen any data. We can easily do this by setting `prior_PD = TRUE`. 
```{r prior-predictive}
bayes_weak_pp <- stan_glm(kid_score ~ mom_iq, data = kidiq,
                               family = gaussian(link = "identity"),
                               prior = normal(location = 0,
                                            scale = 1),
                             prior_PD = TRUE,
                             seed = 12345, 
                             refresh = 0)
print(bayes_weak_pp)

pp_check(bayes_weak_pp)
```

This shows how Bayesian models are generative, as we can still make predictions from the priors without having seen any data. We can see how the distributions are extremely wide, showing the uncertainty captured by the prior. The posterior shown above is clearly a much more accurate summary of the data.
