---
title: "Week 8 - Regression w/Binary Outcomes"
author: "Fred Traylor, Lab TA"
date: "3/21/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 10)

library(tidyverse)
library(stargazer)
library(naniar)

library(rstanarm)
library(tidybayes)
library(mice)

library(modelsummary)
library(broom.mixed)
  options(modelsummary_get = "broom")

seed <- 12345
```


# Data Loading and Management

Let's play with the GSS again. This week, we'll be investigating factors associated with whether a person supports marijuana legalization. 

```{r gss-loading}
gss2018 <- haven::read_dta("lab-data/GSS2018.dta")

gss <- gss2018 %>% 
  select(grass,                 # Target: Whether marijuana should be legal (1 = yes, 2 = no)  
                                          # https://gssdataexplorer.norc.org/variables/285/vshow
         prestg10,              # Econ Vars
         partyid, polviews,     # Political Vars 
         sex, race, hispanic, age, educ,  # Other Demos
         wtss, ballot           # Weight & Ballot
         ) %>% 
  haven::zap_labels() %>% 
  mutate( 
    
    # New Variables
    legal_grass = case_when(
      grass == 1 ~ 1,
      grass == 2 ~ 0
    ),
    conservatism = polviews, # Rennaming to be more descriptive 
    hisp = case_when(
      hispanic == 1 ~ 0, # Not hisp
      TRUE ~ 1
    ),

    
    # Variables we've used before
    age = case_when(
      age > 88 ~ NaN,
      TRUE ~ age
      ),
    sex = factor(sex,
                 levels = c(1,2),
                 labels = c("Male", "Female")
                 ),
    partyid = case_when(
      partyid %in% c(0:2) ~ "Democrat",
      partyid %in% c(4:6) ~ "Republican",
      partyid %in% c(3,7) ~ "Other Party"
      ),
    republican = case_when(
      partyid == "Republican" ~ 1,
      TRUE ~ 0
      ),
    weight = wtss
    )  %>% 
  select(-wtss)

```

Let's take a look at our two new variables. 1) legal_grass, which asks whether marijuana should be legalized; 2) conservatism, which is a 1 to 7 scale of ideology, with 1 as extremely liberal and 7 as extremely conservative. Let's double check the coding below. 

Looking at our new variable for hispanic, we can see there are people who are hispanic in all three racial categories. For this reason, we should create a new variable for race that accounts for this. 

Then, let's remove the original variables from the dataset. 
```{r var-table}
table(gss$polviews, gss$conservatism) 
table(gss$grass, gss$legal_grass)
table(gss$hisp, gss$hispanic)
table(gss$hisp, gss$race)

gss <- gss %>% 
  mutate(
    race = case_when(
      hisp == 1 ~ 4,
      hisp == 0 ~ race
      ),
    race = factor(race,
                  levels = c(1,2,4,3),
              labels = c("White", "Black", "Hispanic", "Other"))) %>% 
  select(-polviews, -grass, - hisp, -hispanic)
table(gss$race)
```

## Missingness Analysis

```{r miss-upset}
gg_miss_upset(gss)
```

We can see by looking at the variable description on the website that the question was only asked to those in Ballots B and C. Let's remove Ballot A from the dataset and then look at this plot. 

```{r rm-balA}
gss <- gss %>% 
  filter(ballot != 1) 
gg_miss_upset(gss)
gss %>% count()

```
Much better! We still have about 100 people missing responses to our target variable, but there are no noticeable patterns with or among the other variables. While we can (and probably should) do any of the methods from last time to impute missing values, this week, we're just going to use listwise deletion to keep it simple. 

```{r}
gss <- gss %>% drop_na()
```



## Descriptives
As always, let's look at our descriptive statistics. 

```{r desc-tables}
datasummary_skim(gss,
                 type = "numeric",
                 fmt = 2, # Show 2 decimal places 
                 histogram = F,
                 title = "Sample Descriptive Statistics: Continuous Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "data.frame")

datasummary_skim(gss, 
                 type = "categorical",
                 title = "Sample Descriptive Statistics: Categorical Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "data.frame")

```

We could also create a "balance table" that compares demographics on each side of the issue. This isn't traditional to publish in this way but does offer an interesting look. (These are usually put at the beginning of an article using experimental or other two-sample data, allowing readers to see the sample are similar. That said, it'll do in our case for some quick looks.)

```{r baltable}
datasummary_balance(~legal_grass,
                    data = gss)
```

Looking at the column "Diff. in Means," we can see that people who favor marijuana legalization tend to be younger by about 8 years, are more liberal by close to one step, and have about one-half more years of education. We also see Democrats and men are more likely to support legalization, while Hispanics are less likely. 

# Linear Probability Model

```{r lin-prob}
linprob1 <- lm(legal_grass ~ conservatism + age + educ + race + sex,
               data = gss, weights = weight)

stargazer(linprob1,
          single.row = T,
          type = "text")
```


## Interpretation

We can interpret this model as the effect of various factors on the probability a person is in favor of legalizing marijuana. So each additional step of conservatism decreases this probability by .07  and each additional year of age decreases it by .005. 

This is why it is called a "linear probability model:" we're modeling the probabilities using linear regression. 

They're great at certain things, but the biggest downside is that a person could have a predicted probability that is less than 0 or greater than one. 

Below, I create a new dataset for values of conservatism, ranging from 1 to 7, and age, ranging from 18 to 88 by 10's. The function `expand_grid()` creates a "grid" of all possible combinations of these variables. I also set eduction and logincome to their mean values, race to Black, and sex to female.

I then use the `predict()` function to predict values `legal_grass` for each of these combinations. Using the `range()` function, we can see that our predicted values range from .29 to 1.04. 

```{r lin-pred}
linpreddata <- expand_grid(conservatism = seq(1, 7),
                           age = seq(18, 88, 10),
                           educ = mean(gss$educ),
                           race = "Black",
                           sex = "Female")

linpreddata$pred_grass <- predict(linprob1, newdata = linpreddata, type = "response")

head(linpreddata)
range(linpreddata$pred_grass)
```

We can also plot our predicted values with a heatmap. 

```{r lin-pred-graph}

linprobgraph <- ggplot(linpreddata, aes(x = conservatism,
                                        y = age,
                                        fill = pred_grass)) +
  geom_tile() + 
  scale_fill_viridis_c() +
  geom_text(aes(label = round(pred_grass, 2)), color = "white", size = 4) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,8,1)) +
  scale_y_continuous(breaks = seq(18,88,10)) +
  labs(title = "Linear Probability Model",
       subtitle = "Predicted Probability of Favoring Marijuana Legalization")

linprobgraph

```



# Logit Model

Now, let's make a model using logistic regression. (This is also called a logit model.) 

The set up is very similar to how we create a linear regression model. There are two key differences: 
1. We use the `glm()` function instead of `lm()`.
2. We include a line saying `family = "binomial"`. 

These might look vaguely familiar to you already: When we ran Bayesian models before, we used `stan_glm()` and specified that `family = gaussian(link = "identity")`. 

Let's go ahead and run two models. First, let's recreate our previous linear probability model, but this time using `glm()` and `family = gaussian(link = "identity")`. Second, let's run a logistic regression model using `glm()` and `family = "binomial"`. 


```{r logit-mod}
lin_glm1 <- glm(legal_grass ~ conservatism + age + educ + race + sex,
                family = gaussian(link = "identity"),
                data = gss)

logit1 <- glm(legal_grass ~ conservatism + age + educ + race + sex,
              family = "binomial",
               data = gss)

stargazer(linprob1, lin_glm1, logit1,
          type = "text")

```

When we "stargaze," one of the first things we notice is that the standard errors of the coefficients have exploded in size in the logit model! 

You'll also notice that we get different model diagnostics when we use `lm()` and `glm()`. Estimating GLM's gives us log-likelihood scores, which allow us to calculate AIC. 

Let's plot the coefficients and standard errors to see how they compare. 

```{r mod-comp-graph}
compmods <- list("Linear Prob. via OLS" = linprob1,
                 "Linear Prob. via GLM" = lin_glm1,
                 "Logit via GLM" = logit1)

modelplot(compmods, coef_omit = 'Interc') +
  geom_vline(aes(xintercept = 0), 
             linetype = "dotted", size = 1, alpha = .5) +
  theme(legend.position = "bottom")
```

We see here just how big the confidence intervals are in our logit model. We also see that the linear probability model coefficient estimates are typically covered by the logit model's confidence intervals. 

## Interpretation

Let's reproduce just the output for our logit model so we can interpret it.

```{r logit-table}
stargazer(logit1, 
          single.row = T, type = "text")
```


When interpreting our logit model, we're working with log odds. So we say that a person moving one step more conservative decreases the log odds of them favoring marijuana legalization by 0.35 and each additional year of age decreases them by 0.03.

We can also interpret the categorical variables in a similar way. Being Black does not significantly affect the log odds of opinions on marijuana legalization but being hispanic is associated with significantly lower log odds of favoring legalization. 


## Predict

Just like we did earlier with the linear probability model, we can predict a person's likelihood of supporting legalization. 

```{r log-pred}
logitpreddata <- expand_grid(conservatism = seq(1, 7),
                           age = seq(18, 88, 10),
                           educ = mean(gss$educ),
                           race = "Black",
                           sex = "Female")

logitpreddata$pred_grass <- predict(linprob1, newdata = logitpreddata, type = "response")

head(logitpreddata)
range(logitpreddata$pred_grass)
```

We can also plot our predicted values with a heatmap. 

```{r log-pred-graph}

logitprobgraph <- ggplot(logitpreddata, aes(x = conservatism,
                                        y = age,
                                        fill = pred_grass)) +
  geom_tile() + 
  scale_fill_viridis_c() +
  geom_text(aes(label = round(pred_grass, 2)), color = "white", size = 4) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,8,1)) +
  scale_y_continuous(breaks = seq(18,88,10)) +
  labs(title = "Logit Model",
       subtitle = "Predicted Probability of Favoring Marijuana Legalization")

logitprobgraph

```



# Odds Ratios

Because log odds are often difficult to interpret, another way of presenting your output is with odds ratios.

Getting the exponentiated coefficients is easy. Just use `exp()` on the coefficients of your model. Below, I compare the log odds and the odds ratios of our logit1 model. 

```{r odds-ratio}
cbind(Estimate=round(coef(logit1),3),
      OddsRatio=round(exp(coef(logit1)),3)) 
```

So here, we would say that being Hispanic decreases the odds of favoring marijuana legalization. 

$$ \frac{Hispanic}{White}=\frac{.416}{1}$$

To find the percent difference from the odds ratio, we take the difference from one: $ .416-1=-.584 $, so we would say that being Hispanic decreases the odds of favoring legalization by around 58%. 

For continuous variables, such as education, we say that each additional year of education increases the odds by 3.6%: $1-1.036=.036$. 

**If the OR is greater than 1, it is an increase in the odds. If it is less than 1, it is a decrease.**


Because of this easy interpretation, at least as far as applying results, I personally prefer to work with odds ratios. On the other hand, I know others prefer log odds because the simple positive-negative heuristic works for them. 

While exponentiating the coefficients is easy, getting standard errors for these is much more difficult. 

Doing this with stargazer is tricky, but modelsummary makes it easy: just include `exponentiate = T`. Note, however, that you cannot combine exponentiated models with nonexponentiated models in the same modelsummary function. Below, I provide one way to do it, but it's clunky and not great. 

<!-- TOM: Any thought on how to improve this?? -->

```{r OR-table}
# m1 <- modelsummary(logit1,
#              stars = T, output = "data.frame",
#              gof_omit = "log")
# 
# options("modelsummary_get" = "easystats")
# m2 <- modelsummary(logit1,
#              stars = T, output = "data.frame",
#              exponentiate = T) %>% 
#   select("Model 1") %>% rename("Model 2" = "Model 1")
# m <- cbind(m1, m2)
# 
# kableExtra::kable(m)

```


# Bayesian Modeling 
Let's finish by replicating two of our previous analyses using `stan_glm()`. The first is the linear probability model and the second is the logistic model. You'll note that they display the exact same except for the line that tell `rstanarm` what "family" of distributions to pull from. 

```{r bayes}
bmodlogit <- stan_glm(legal_grass ~ conservatism + age + educ + race + sex,
                      data = gss, 
                      weights = weight,
                      family = binomial(link = "logit"),
                      seed = seed, 
                      chains = 1,
                      refresh = 0,
                      iter = 2000,  
                      warmup = 1000)
bmodlinpro <- stan_glm(legal_grass ~ conservatism + age + educ + race + sex,
                       data = gss, 
                       weights = weight,
                       family = gaussian(link = "identity"),
                       seed = seed, 
                       chains = 1,
                       refresh = 0,
                       iter = 2000,  
                       warmup = 1000)
```

Now, let's build a table with all five models together. 
```{r comp-table}
lastmods <- list("LPM" = linprob1,
                 "Linear GLM" = lin_glm1,
                 "Bayes LPM" = bmodlinpro,
                 "Logit" = logit1,
                 "Bayes Logit" = bmodlogit)
coefnames <- c("conservatism" = "Conservatism \n(7 pt scale)", # The "\n" will come in handy in a second
               "age" = "Age (Yrs)",
               "educ" = "Education (Yrs)",
               "sexFemale" = "Female",
               "raceBlack" = "Race: Black",
               "raceOther" = "Race: Other",
               "(Intercept)" = "Constant")

modelsummary(lastmods,
             coef_map = coefnames,
             title = "Model Comparison: Predicing Support for Marijuana Legalization",
             notes = c("Reference Categories: Male, White",
                       "Data: 2018 General Social Survey"))
```

And lastly, let's plot them all together. Remember that special code I used earlier on the conservatism variable? This makes it so now, in the plot, the description will be on a new line. 

```{r comp-plot}
modelplot(lastmods, coef_omit = 'Interc',
          coef_map = rev(coefnames)) +  # Reversing order to match order in table
  geom_vline(aes(xintercept = 0), 
             linetype = "dotted", size = 1, alpha = .5) +
  theme(legend.position = "bottom")
```


