---
title: "Week 9 - Marginal Effects"
author: "Fred Traylor, Lab TA"
date: "3/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 10)

library(tidyverse)
library(stargazer)
library(naniar)

library(rstanarm)
library(tidybayes)
library(mice)

library(modelsummary)
library(broom.mixed)
  options(modelsummary_get = "broom")

library(marginaleffects)

seed <- 12345
```


# Data Loading and Management

Let's use the GSS again. This week, we'll be investigating public opinion on federal spending. TODO: I think this description is incorrect...

```{r gss-loading}
gss2018 <- haven::read_dta("lab-data/GSS2018.dta")

gss <- gss2018 %>% 
  select(

    # Targets: R's Income & Hrs Worked
    conrinc, unemp,
    
    # Demographics 
    prestg10, wrkslf,
    sex, race, hispanic, age, educ, 
    wtss, form 
    ) %>% 
  haven::zap_labels() %>% 
  mutate( 
    
    # New Variables
    unemp = 2-unemp,
    unempfac = factor(unemp,
                      levels = c(0,1),
                      labels = c("No", "Yes")),

    # Variables we've used before
    logincome = log(conrinc),
    # selfemp = case_when(
    #   wrkslf == 1 ~ 1,  # Self-Employed
    #   wrkslf == 2 ~ 0   # Works for someone else 
    #   ),
    selfemp = factor(wrkslf,
                     levels = c(2,1),
                     labels = c("OtherEmp", "SelfEmp")),
    age = case_when(
      age > 88 ~ NaN,
      TRUE ~ age
      ),
    agesq = age^2,
    sex = factor(sex,
                 levels = c(1,2),
                 labels = c("Male", "Female")
                 ),
    hisp = case_when(
      hispanic == 1 ~ 0, # Not hisp
      TRUE ~ 1
    ),
    race = case_when(
      hisp == 1 ~ 4,
      hisp == 0 ~ race
      ),
    race = factor(race,
                  levels = c(1,2,4,3),
                  labels = c("White", "Black", "Hispanic", "Other")),
    weight = wtss
    )  %>% 
  select(-wtss, - hisp, -hispanic, -conrinc, -wrkslf) %>% 
  drop_na()

```


## Descriptives
As always, let's look at our descriptive statistics. 
```{r desc-tables}
datasummary_skim(gss,
                 type = "numeric",
                 fmt = 2, # Show 2 decimal places 
                 histogram = T,
                 title = "Sample Descriptive Statistics: Continuous Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "kableExtra")

datasummary_skim(gss, 
                 type = "categorical",
                 title = "Sample Descriptive Statistics: Categorical Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "huxtable")

```



# A Regression Model

Let's start by creating a set of models that predicts respondent's income.  As before, we use `age`, and a term for the square of that value, `agesq.` In the third equation of this set, I use `I(age^2)` for the squared term. We'll get to this in a minute, but this will let us calculate effects of work experience and work experience-squared at the same time. 

```{r two-mods, message=FALSE}

increg1 <- lm(logincome ~ unempfac + race + age + educ + sex + selfemp,
              data = gss, weights = weight)
increg2 <- lm(logincome ~ unempfac + race + age + agesq + educ + sex + selfemp,
              data = gss, weights = weight)
increg3 <- lm(logincome ~ unempfac + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight)

stargazer(increg1, increg2, increg3,
          single.row = T,
          omit.stat = c("F", "ser"),
          type = "text")

```

Notice that models 2 and 3 are identical, except for what we use as inputs for the squared term of work experience.  
TODO: Shouldn't this be squared term for age?

# Predicted Effects

Last week we used the `predict()` function to predict values at certain points. This time, we're going to use the `predictions()` function in the `marginaleffects` package. 

```{r pred1}
predictions(increg1, variables = c("race", "sex"))
```

Above, we have predicted values of confidence in science (along with standard errors of the prediction) at each level of race and conservatism. The function also held all of our other predictors (age, education, sex) at their means and mode. 

TODO: I think the interpretation is also off here. Isn't it race and sex rather than conservatism?

We can also plot these using the `plot_cap()` function. 
```{r predplot1-educprest}
cowplot::plot_grid(
  plot_cap(increg2, condition = "educ") + ylim(6, 12),
  plot_cap(increg2, condition = "age")  + ylim(6, 12),
  plot_cap(increg2, condition = c("educ", "age")) + ylim(6, 12)
  )
```

TODO: This plot is a little confusing. I'm not sure why age is split into these groups. And not all groups show on the plot. I get warning messages here. I the truncation of the y-axis might be leaving some groups out. Maybe it would be more straightforward just to plot one of these and spend more time describing it.

Remember earlier, when we added in effects of `age`, `agesq`, and `I(age^2)`? Let's now see what that did. 

```{r predplot2-age}
cowplot::plot_grid(
  plot_cap(increg2, condition = "age") + ggtitle("Reg 2"),
  plot_cap(increg2, condition = "agesq")  + ggtitle("Reg 2"),
  plot_cap(increg3, condition = "age") + ggtitle("Reg 3")
  )

```

Above, we can see that work experience increases income, while the square of work experience decreases it. We also see that, when we put these together, we get a parabolic effect, such that income peaks and then declines with work experience (age). 

TODO: i think you mean age rather than work experience...

Also, you can only have two conditions per graph. You might have noticed that these graphs have `ggplot` aesthetics, so all ggplot changes are possible. If you want more than two factors, you can "facet" your graph using ggplot. 

Below, we construct predictions, feed those to ggplot, and build our plot around that. 
```{r predggplot}
predictions(increg3, variables = c("selfemp", "age", "sex")) %>% 
  ggplot(aes(x = age, y = predicted,
             ymin = predicted - std.error, 
             ymax = predicted + std.error)) +
  geom_ribbon(aes(fill = selfemp), alpha = .3) +
  geom_line(aes(color = selfemp), size = 1.5) + 
  facet_wrap(~sex) +
  theme_linedraw() + 
  labs(title = "Predicted Value of Income (Logged)",
       subtitle = "Data: 2018 GSS")
```
TODO: I don't quite understand why these are not smoothed. It seems like some of the age groups have been binned, such that the predictions are constant over a range of age. This might be related to the issue with the plot above. 

# Marginal Effects 
What if we want to see how the predicted value changes when we go from one value to another? We can find the "marginal effect" of a variable to see how the predicted effect changes. This is particularly important for logistic regression, where coefficients do not have a straightforward linear interpretation. 

## Another Set of Models 
Let's create another set of models, this time predicting whether the respondent has ever been unemployed during the past ten years. 

Running this will throw us a warning, but we can ignore that. Another way of running it that won't produce the warning is to use `family = "quasibinomial"` instead of  `"binomial"`, but the output  will be the same. The big difference  in the two is that SE's will be larger, and we won't get a log-Likelihood (or AIC). 

```{r workmods}
work1 <- glm(unemp ~ logincome + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, family = "binomial")
work2 <- glm(unemp ~ logincome + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight, family = "quasibinomial")
stargazer(work1, work2, single.row = T,
          omit.stat = c("F", "ser"), type = "text")
```
TODO: Unless I'm missing something here, I would avoid using logincome as a predictor of unemployment. If someone is currently unemployed then income will be zero. I'm not sure how much of the sample this applies to (since some of the people are just formerly unemployed) but this could artificially inflate model performance. Same goes for self-employed. It may be more straightforward for intepretation to just consider the demographics.

## Marginal Effects 
```{r mfx1}
marginaleffects(work1)
```

Notice the column "dydx." If you've ever taken calculus, you'll recognize this as the change in y (delta y) over the change in x (delta x). In other words, this is the change in predicted value at this point along the line. 

The values above show the marginal effect of each independent variable, for each observation. In this case, there are nine regressors (income, Black, Hispanic, other, age, education, sex, and self-employed) and 897 observations: $8*897 = 7176$

This is good info, but it's also a ton of info, and not in a form that is very useful. 

## Average Marginal Effect 

One way to summarize this information is to calculate the average of the marginal effects. We can get the marginal effects and then take the average of that across different levels. 

```{r ame}
work_mfx <- marginaleffects(work1) 
summary(work_mfx)
```

We can also display these in a table. Because we have contrasts (from categorical variables), we need to tell `modelsummary` how to group our terms. 
```{r ame-table}
mfx <- lapply(list(work1, increg3), marginaleffects)
modelsummary(mfx, stars = T,
             group = term + contrast ~ model,
             output = "huxtable")
```

## Conditional Marginal Effects

We can also consider marginal effects conditional on the value of other covariates. 

Below, I construct the marginal effects where education is set at 12 years. The equation is the same as above, `marginaleffects(model)`,  but with an argument for `newdata = datagrid()`, into which we input our conditions.

```{r cme}
marginaleffects(work1, newdata = datagrid(educ = 12))
```

We can also specify multiple conditions, like where race is Black, age is 35, and education is at both 12 and 16.

```{r cmegrid}
marginaleffects(work1, newdata = datagrid(educ = c(12,16),
                                          race = "Black",
                                          age = 35))
```

Notice above that each `rowid` has 2 levels, one where `educ==12` and one where `educ==16`. 

TODO: Move the Conditional Marginal Effects Plots section here.

### Marginal Effects at the Mean 

You'll also notice in the above two tables that all the other factors were held at their means. This is actually a very common method, to hold all variables at their means. This is called the marginal effect at the mean (MEM). For each variable, it calculates the marginal effect when all other variables are held at their means. (Categorical variables are set at their modes.) This is more useful to interpret. 

If we want to hold them at their means, we set it to `datagrid()`, with nothing inside, which defaults to everything at the mean (or mode).
```{r mem}
marginaleffects(work1, newdata = datagrid())
```

So above, we see that the average effect of a one percent increase in income is a  .07 decrease in the log odds of having been unemployed, when everything else is held at their means (or modes). 

For categorical variables, we use the contrast column to specify the effect. So being female decreases the log odds of being unemployed by .07, compared to men, and being Black increases them by .06 compared to white. 

TODO: Move this section up, after Average Marginal Effects. I didn't move this since the beginning of the section references the previous section and I didn't want to introduce any confusion.

### Conditional Marginal Effects Plots 

Conditional marginal effects are useful, but it often makes more sense to plot them to see what is going on. We often use this to illustrate and create interactions. 

```{r cmeplot}
cowplot::plot_grid(
  plot_cme(work1, effect = "age", condition = c("logincome")) + ylab("ME of Age") + 
    geom_hline(yintercept = 0, linetype = "dotted"),
  plot_cme(work1, effect = "age", condition = c("age")) + ylab("ME of Age") + 
    geom_hline(yintercept = 0, linetype = "dotted"),
  plot_cme(work1, effect = "logincome", condition = c("logincome")) + ylab("ME of LogInc") + 
    geom_hline(yintercept = 0, linetype = "dotted"),
  plot_cme(work1, effect = "logincome", condition = c("age")) + ylab("ME of LogInc") + 
    geom_hline(yintercept = 0, linetype = "dotted")
  )
```

In the top left, we see that the effects of age on the probability of having been unemployed vary by income, becoming less significantly negative at higher incomes. In other words, at higher incomes, being older provides less of a buffer against being unemployed. Similarly, at higher ages, having a higher income provides less of a buffer against being unemployed (bottom right). 

In the interactions, we see that the effect of age changes at different ages. Being over forty offers some sense of protection against being unemployed. Meanwhile, income doesnt' have much of a change in its effect across different levels of itself.

TODO: I think this interpretation makes sense. I appreciate the multiple comparisons but it might be more straightforward if you picked one version of the plot and walked through it in more detail. Otherwise you're presenting students with a lot of information at once. 

We can also plot the marginal effects conditional on multiple variables, including categorical ones. 
```{r cmeplot-cat}
plot_cme(work1, effect = "age", condition = c("selfemp","sex"))
```


The effect of work experience/age on the odds of somebody being unemployed is slightly higher for other-employed females than for everybody else, although this difference is not statistically significant. 

# Marginal Effects with Bayesian Models 
## Model Estimation
```{r bayes}
bmod_work <- stan_glm(unemp ~ logincome + race + age + I(age^2) + educ + sex,
                      data = gss, 
                      weights = weight,
                      family = binomial(link = "logit"),
                      seed = seed, 
                      chains = 1,
                      refresh = 0,
                      iter = 2000,  
                      warmup = 1000)
bmod_inc <- stan_glm(logincome ~ unempfac + race + age + I(age^2) + educ + sex,
                     data = gss,
                     weights = weight,
                     family = gaussian(link = "identity"),
                     seed = seed, 
                     chains = 1,
                     refresh = 0,
                     iter = 2000,  
                     warmup = 1000)

modelsummary(list(bmod_work,bmod_inc),
             output = "huxtable")

```

## Predictions
Fortunately, we can use the same functions above to predict from bayesian models. 

```{r bayespred}
predictions(bmod_inc, variables = c("educ", "sex"))
predictions(bmod_work, variables = c("educ", "sex"))
```

We can also plot from them as well. 
```{r bayespredplot}
cowplot::plot_grid(
  plot_cap(increg3, condition = "age") + ggtitle("OLS Income"),
  plot_cap(work1, condition = "age") + ggtitle("MLE Unemployment"),
  plot_cap(bmod_inc, condition = "age") + ggtitle("Bayes Income"),
  plot_cap(bmod_work, condition = "age") + ggtitle("Bayes Unemployment")
  )
```
TODO: I'm not sure exactly what happened here but there is no error region around the top right plot. I tried to plot it separately to see if it was a scale issue, but this didn't seem to be the cause. I thought this might be because you were using the quasi-binomial model (work2) rather than work1, but this doesn't seem to be the cause after fixing it. After doing some tinkering, it appears that this is because of the "weights" argument. This seems to prevent the marginal effects from being properly calculated. I don't have time to figure out the appropriate solution right now, so I would recommend avoiding weights. Finally, I think the OLS label is incorrect. The unemployment model is estimated via maximum likelihood. I have changed this.

## Marginal Effects 
We can also use them to find the marginal effects. 
```{r bmfx}
marginaleffects(bmod_inc)
```

Since we have 1000 samples from the posterior distribution, we also get 1000 posterior samples of each marginal effect. We can summarize this uncertainty by calculating average marginal effects.

```{r bmfxsum}
summary(marginaleffects(bmod_work))
```
TODO: It would be useful to pick one or two of these effects and compare to the frequentist ones.

```{r bcmeplot}
plot_cme(bmod_work, effect = "sex", condition = c("age"))
```
TODO: I played around with this a little, but feel free to change it. I think it makes most sense to focus on two different variables rather than to consider variation in the ME of one variable. 

TODO: Finally, you'll see I switched these last two to the binary outcome model. I would like you to focus more on this than linear outcomes.
