---
title: "Week 9 - Marginal Effects"
author: "Fred Traylor, Lab TA"
date: "3/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 10)

library(tidyverse)
library(stargazer)
library(naniar)

library(rstanarm)
library(tidybayes)
library(mice)

library(modelsummary)
library(broom.mixed)
  options(modelsummary_get = "broom")

library(marginaleffects)

seed <- 12345
```


# Data Loading and Management

Let's play with the GSS again. This week, we'll be investigating public opinion on federal spending. 

```{r gss-loading}
gss2018 <- haven::read_dta("lab-data/GSS2018.dta")

gss <- gss2018 %>% 
  select(

    # Targets: R's Income & Hrs Worked
    conrinc, unemp,
    
    # Demographics 
    prestg10, wrkslf,
    sex, race, hispanic, age, educ, 
    wtss, form 
    ) %>% 
  haven::zap_labels() %>% 
  mutate( 
    
    # New Variables
    unemp = 2-unemp,
    unempfac = factor(unemp,
                      levels = c(0,1),
                      labels = c("No", "Yes")),

    # Variables we've used before
    logincome = log(conrinc),
    # selfemp = case_when(
    #   wrkslf == 1 ~ 1,  # Self-Employed
    #   wrkslf == 2 ~ 0   # Works for someone else 
    #   ),
    selfemp = factor(wrkslf,
                     levels = c(2,1),
                     labels = c("OtherEmp", "SelfEmp")),
    age = case_when(
      age > 88 ~ NaN,
      TRUE ~ age
      ),
    agesq = age^2,
    sex = factor(sex,
                 levels = c(1,2),
                 labels = c("Male", "Female")
                 ),
    hisp = case_when(
      hispanic == 1 ~ 0, # Not hisp
      TRUE ~ 1
    ),
    race = case_when(
      hisp == 1 ~ 4,
      hisp == 0 ~ race
      ),
    race = factor(race,
                  levels = c(1,2,4,3),
                  labels = c("White", "Black", "Hispanic", "Other")),
    weight = wtss
    )  %>% 
  select(-wtss, - hisp, -hispanic, -conrinc, -wrkslf) %>% 
  drop_na()

```


## Descriptives
As always, let's look at our descriptive statistics. 

```{r desc-tables}
datasummary_skim(gss,
                 type = "numeric",
                 fmt = 2, # Show 2 decimal places 
                 histogram = T,
                 title = "Sample Descriptive Statistics: Continuous Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "kableExtra")

datasummary_skim(gss, 
                 type = "categorical",
                 title = "Sample Descriptive Statistics: Categorical Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "huxtable")

```



# A Regression Model

Let's start by creating a set of models that predicts respondent's income.  As before, we use `age`, and a term for the square of that value, `agesq.` In the third equation of this set, I use `I(age^2)` for the squared term. We'll get to this in a minute, but this will let us calculate effects of work experience and work experience-squared at the same time. 

```{r two-mods, message=FALSE}

increg1 <- lm(logincome ~ unempfac + race + age + educ + sex + selfemp,
              data = gss, weights = weight)
increg2 <- lm(logincome ~ unempfac + race + age + agesq + educ + sex + selfemp,
              data = gss, weights = weight)
increg3 <- lm(logincome ~ unempfac + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight)

stargazer(increg1, increg2, increg3,
          single.row = T,
          omit.stat = c("F", "ser"),
          type = "text")

```

Notice that models 2 and 3 show the  exact same everything, except for what we use as inputs for the squared term of work experience. 

# Predicted Effects

Remember last week, when we use the `predict()` function to predict values at certain points? Let's do that again this week. But this time, we're going to use the `predictions()` function in the `marginaleffects` package. 

```{r pred1}
predictions(increg1, variables = c("race", "sex"))
```

Above, we have predicted values of confidence in science (along with SE's of that prediction) at each level of race and conservatism. The function also held all of our other predictors (age, education, sex) at their means and mode. 

We can also plot these using the `plot_cap()` function. 

```{r predplot1-educprest}
cowplot::plot_grid(
  plot_cap(increg2, condition = "educ") + ylim(6, 12),
  plot_cap(increg2, condition = "age")  + ylim(6, 12),
  plot_cap(increg2, condition = c("educ", "age")) + ylim(6, 12)
  )
```
TOM: Am I supposed to get getting ribbons on here? 

Remember earlier, when we added in effects of `age`, `agesq`, and `I(age^2)`? Let's now see what that did. 

```{r predplot2-age}
cowplot::plot_grid(
  plot_cap(increg2, condition = "age") + ggtitle("Reg 2"),
  plot_cap(increg2, condition = "agesq")  + ggtitle("Reg 2"),
  plot_cap(increg3, condition = "age") + ggtitle("Reg 3")
  )

```

Above, we can see that work experience increases income while the square of work experience decreases it. We also see that, when we put these together, we get a parabolic effect, such that income peaks and then declines with work experience (age). 

Also, you can only have two conditions per graph. You might've noticed that the output of these graphs is ggplot, so all ggplot changes are possible. If you want more than two factors, you can "facet" your graph using ggplot. 

Below, we construct predictions, feed those to ggplot, and build our plot around that. 

```{r predggplot}
predictions(increg3, variables = c("selfemp", "age", "sex")) %>% 
  ggplot(aes(x = age, y = predicted,
             ymin = predicted - std.error, 
             ymax = predicted + std.error)) +
  geom_ribbon(aes(fill = selfemp), alpha = .3) +
  geom_line(aes(color = selfemp), size = 1.5) + 
  facet_wrap(~sex) +
  theme_linedraw() + 
  labs(title = "Predicted Value of Income (Logged)",
       subtitle = "Data: 2018 GSS")
```


# Marginal Effects 
What if we want to see how the predicted value changes when we go from one value to another? We can find the "marginal effect" of a variable to see how the predicted effect changes. Logistic regression models are the perfect time to do this, so let's create some now. 

## Another Set of Models 
Let's create another set of models, this time predicting whether the respondent has ever been unemployed during the past ten years. 

Running this will throw us a warning, but we can ignore that. Another way of running it that won't produce the warning is to use `family = "quasibinomial"` instead of  `"binomial"`, but the output  will be the same. The big difference  in the two is that SE's will be larger, and we won't get a log-Likelihood (or AIC). 

```{r workmods}
work1 <- glm(unemp ~ logincome + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight, family = "binomial")
work2 <- glm(unemp ~ logincome + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight, family = "quasibinomial")
stargazer(work1, work2, single.row = T,
          omit.stat = c("F", "ser"), type = "text")
```


## Marginal Effects 
```{r mfx1}
marginaleffects(work1)

```

Notice the column "dydx." If you've ever taken calculus, you'll recognize this as the change in y (delta y) over the change in x (delta x). In other words, this is the change in predicted value at this point along the line. 

The values above show the marginal effect of each independent variable, for each observation. In this case, there are nine regressors (income, Black, Hispanic, other, age, education, sex, and self-employed) and 897 observations: $8*897 = 7176$

This is good info, but it's also a ton of info, and not in a form that is very useful. 

## Conditional Marginal Effects

More exciting than a simple marginal effect, however, is the conditional marginal effect. This is the marginal effect, conditional on the value of something else. 

Below, I construct the marginal effects where education is set at 12 years. The equation is the same as above, `marginaleffects(model)`,  but with an argument for `newdata = datagrid()`, into which we input our conditions.

```{r cme}
marginaleffects(work1, newdata = datagrid(educ = 12))

```

We can also specify multiple conditions, like where race is Black, age is 35, and education is at both 12 and 16.

```{r cmegrid}
marginaleffects(work1, newdata = datagrid(educ = c(12,16),
                                          race = "Black",
                                          age = 35))
```

Notice above that each `rowid` has 2 levels, one where `educ==12` and one where `educ==16`. 

### Marginal Effects at the Mean 

You'll also notice in the above two tables that all the other factors were held at their means. This is actually a very common method, to hold all variables at their means. This is called, appropriately enough, the marginal effect at the mean (MEM).  For each variable, it calculates the marginal effect when all other variables are held at their means. (For categorical variables, they'll be set at their modes.) This is more useful to interpret. 

If we want to hold them at their means, we set it to `datagrid()`, with nothing inside, which defaults to everything at the mean. 

```{r mem}
marginaleffects(work1, newdata = datagrid())
```

So above, we see that the average effect of a one percent increase in income is a  .07 decrease in the log odds of having been unemployed, when everything else is held at their means (or modes). 

For categorical variables, we use the contrast column to specify the effect. So being female decreases the log odds of being unemployed by .07, compared to men, and being Black increases them by .06 compared to white. 

### Conditional Marginal Effects Plots 

Conditional marginal effects are useful, but it often makes more sense to plot them to see what is going on. We often use this to illustrate and create interactions. 

```{r cmeplot}
cowplot::plot_grid(
  plot_cme(work1, effect = "age", condition = c("logincome")) + ylab("ME of Age") + 
    geom_hline(yintercept = 0, linetype = "dotted"),
  plot_cme(work1, effect = "age", condition = c("age")) + ylab("ME of Age") + 
    geom_hline(yintercept = 0, linetype = "dotted"),
  plot_cme(work1, effect = "logincome", condition = c("logincome")) + ylab("ME of LogInc") + 
    geom_hline(yintercept = 0, linetype = "dotted"),
  plot_cme(work1, effect = "logincome", condition = c("age")) + ylab("ME of LogInc") + 
    geom_hline(yintercept = 0, linetype = "dotted")
  )
```

In the top left, we see that the effects of age on the probability of having been unemployed vary by income, becoming less significantly negative at higher incomes. In other words, at higher incomes, being older provides less of a buffer against being unemployed. Similarly, at higher ages, having a higher income provides less of a buffer against being unemployed (bottom right). 

In the interactions, we see that the effect of age changes at different ages. Being over forty offers some sense of protection against being unemployed. Meanwhile, income doesnt' have much of a change in its effect across different levels of itself.

TOM: Does this make sense? 

We can also plot the marginal effects conditional on multiple variables, including  categorical ones. 
```{r cmeplot-cat}
plot_cme(work1, effect = "age", condition = c("selfemp","sex"))
```


So above, we can see that the effect of work experience/age on the odds of somebody being unemployed is slightly higher for other-employed females than for everybody else, although this difference is nonsignificant. 


## Average Marginal Effect 

The average marginal effect is exactly what it sounds like. We can get the marginal effects and then take the average of that across different levels. 

```{r ame}
work_mfx <- marginaleffects(work1) 
summary(work_mfx)
```

We can also display these in a table. Because we have contrasts (from categorical variables), we need to tell `modelsummary` how to group our terms. 
```{r ame-table}
mfx <- lapply(list(work1, increg3), marginaleffects)
modelsummary(mfx, stars = T,
             group = term + contrast ~ model,
             output = "huxtable")
```




# Marginal Effects with Bayesian Models 

## Model Creation 
```{r bayes}
bmod_work <- stan_glm(unemp ~ logincome + race + age + I(age^2) + educ + sex,
                      data = gss, 
                      weights = weight,
                      family = binomial(link = "logit"),
                      seed = seed, 
                      chains = 1,
                      refresh = 0,
                      iter = 2000,  
                      warmup = 1000)
bmod_inc <- stan_glm(logincome ~ unempfac + race + age + I(age^2) + educ + sex,
                     data = gss,
                     weights = weight,
                     family = gaussian(link = "identity"),
                     seed = seed, 
                     chains = 1,
                     refresh = 0,
                     iter = 2000,  
                     warmup = 1000)

modelsummary(list(bmod_work,bmod_inc),
             output = "huxtable")

```

## Predictions
Fortunately, we can use the same functions above to predict from bayesian models. 

```{r bayespred}
predictions(bmod_inc, variables = c("educ", "sex"))
predictions(bmod_work, variables = c("educ", "sex"))
```

We can also plot from them as well. 
```{r bayespredplot}
cowplot::plot_grid(
  plot_cap(increg3, condition = "age") + ggtitle("OLS Income"),
  plot_cap(work2, condition = "age") + ggtitle("OLS Unemployment"),
  plot_cap(bmod_inc, condition = "age") + ggtitle("Bayes Income"),
  plot_cap(bmod_work, condition = "age") + ggtitle("Bayes Unemployment")
  )

```

## Marginal Effects 
We can also use them to find the marginal effects. 

```{r bmfx}
marginaleffects(bmod_inc)
```

But, because we have 1000 models, we also get 1000 of each marginal effect. Let's calculate the average marginal effects instead.

```{r bmfxsum}
summary(marginaleffects(bmod_inc))
```

```{r bcmeplot}
plot_cme(bmod_inc, effect = "age", condition = c("age"))
```

