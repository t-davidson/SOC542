---
title: "Week 9 - Marginal Effects"
author: "Fred Traylor, Lab TA"
date: "3/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 10)

library(tidyverse)
library(stargazer)
library(naniar)

library(rstanarm)
library(tidybayes)
library(mice)

library(modelsummary)
library(broom.mixed)
  options(modelsummary_get = "broom")
  
# Making sure we have the most recent version 
if((packageVersion("marginaleffects") != '0.4.0.9000'))  
  remotes::install_github("vincentarelbundock/marginaleffects")
## MAKE SURE YOU RESTART R AFTER THIS INSTALL 

library(marginaleffects)

seed <- 12345
```


# Data Loading and Management

Let's use the GSS again. This week, we'll be looking again at factors that influence a person's income and factors that predict whether that person has been unemployed in the past ten years. 

```{r gss-loading}
gss2018 <- haven::read_dta("lab-data/GSS2018.dta")

gss <- gss2018 %>% 
  select(

    # Targets: R's Income & Hrs Worked
    conrinc, unemp,
    
    # Demographics 
    prestg10, wrkslf,
    sex, race, hispanic, age, educ, 
    wtss, form 
    ) %>% 
  haven::zap_labels() %>% 
  mutate( 
    
    # New Variables
    unemp = 2-unemp,

    # Variables we've used before
    logincome = log(conrinc),
    selfemp = factor(wrkslf,
                     levels = c(2,1),
                     labels = c("OtherEmp", "SelfEmp")),
    age = case_when(
      age > 88 ~ NaN,
      TRUE ~ age
      ),
    agesq = age^2,
    sex = factor(sex,
                 levels = c(1,2),
                 labels = c("Male", "Female")
                 ),
    hisp = case_when(
      hispanic == 1 ~ 0, # Not hisp
      TRUE ~ 1
    ),
    race = case_when(
      hisp == 1 ~ 4,
      hisp == 0 ~ race
      ),
    race = factor(race,
                  levels = c(1,2,4,3),
                  labels = c("White", "Black", "Hispanic", "Other")),
    weight = wtss
    )  %>% 
  select(-wtss, - hisp, -hispanic, -conrinc, -wrkslf) %>% 
  drop_na()

```


## Descriptives
As always, let's look at our descriptive statistics. 
```{r desc-tables}
datasummary_skim(gss,
                 type = "numeric",
                 fmt = 2, # Show 2 decimal places 
                 histogram = T,
                 title = "Sample Descriptive Statistics: Continuous Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "kableExtra")

datasummary_skim(gss, 
                 type = "categorical",
                 title = "Sample Descriptive Statistics: Categorical Variables",
                 notes = "Data: 2018 General Social Survey",
                 output = "huxtable")

```



# A Regression Model

Let's start by creating a set of models that predicts respondent's income.  As before, we use `age`, and a term for the square of that value, `agesq.` In the third equation of this set, I use `I(age^2)` for the squared term. We'll get to this in a minute, but this will let us calculate effects of work experience and work experience-squared at the same time. 

```{r two-mods, message=FALSE}

increg1 <- lm(logincome ~ race + age + educ + sex + selfemp,
              data = gss, weights = weight)
increg2 <- lm(logincome ~ race + age + agesq + educ + sex + selfemp,
              data = gss, weights = weight)
increg3 <- lm(logincome ~ race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight)

stargazer(increg1, increg2, increg3,
          single.row = T,
          omit.stat = c("F", "ser"),
          type = "text")

```

Notice that models 2 and 3 are identical, except for what we use as inputs for the squared term of age. 

# Predicted Effects

Last week we used the `predict()` function to predict values at certain points. This time, we're going to use the `predictions()` function in the `marginaleffects` package. 

```{r pred1}
predictions(increg3, variables = c("race", "sex"))
```

Above, we have predicted values of income at each level of race and sex, along with standard errors of these predictions. The function also held all of our other predictors (age, education, sex) at their means and mode. 

We can also predict income at different values of education, like so:

```{r pred2}
predictions(increg3, variables = "educ")
```

The default in this case is to use Tukey's five number summary: minimum, 25th percentile, median, 75th percentile, and maximum. This keeps it simple instead of showing every possible value of education. 

Let's now plot the predicted values of income based on education using the `plot_cap()` function. 
```{r predplot1-educprest}
plot_cap(increg3, condition = "educ")

```

This plot shows the predicted values of income along various points of education, when everything else is held at the mean. (More on that in a minute.) If we go back up to our predictions from above, we see that the predicted (log) income at 0 years of education was 8.11 and at 20 years it was 10.89. The three other points follow in a straight line because this is a linear prediction from a linear model. 

What about a nonlinear effect, though? 

Remember that earlier we saw a curvilinear effect of age and age-squared on income. Our second model used these as two separate terms, `age` and `agesq`, while our third model used them as one term `age` and a term constructed within the equation from it `I(age^2)`. Let's now see what that did. 

You might have noticed that these graphs have `ggplot` aesthetics, so all `ggplot` changes are possible. Here, I added titles to each graph, and also plotted the actual data with `ggplot` as well. 

```{r predplot2-age}
cowplot::plot_grid(
  plot_cap(increg2, condition = "age") + ggtitle("Model 2"),
  plot_cap(increg2, condition = "agesq")  + ggtitle("Model 2"),
  plot_cap(increg3, condition = "age") + ggtitle("Model 3"),
  ggplot(gss, aes(x = age, y = logincome)) + 
    geom_smooth(color = "black", size = .5) +
    theme_minimal() + ggtitle("Actual")
  )

```

Above, we can see that age is associated with an increase in income, while the square of age decreases it. We also see that, when we put these together, we get a parabolic effect, such that income peaks and then declines with work experience (age). When using predictions, we want to use these constructed terms like `I(age^2)` to ensure that we can map the effect of age on income like this. 

When we compare model 3's relationship of age to income, we see it is much more in line with the relationship we see in our actual data. 

## Plotting Predictions w/Two or More Variables

You can also use the `plot_cap()` function to plot predictions with two or more variables. Below, we see the effects of education and sex together on income. 

```{r plotcap-multi}
plot_cap(increg3, condition = c("educ", "sex"))
```

Just note, however, you can only have two conditions per graph. If you want more than two factors, you can "facet" your graph using ggplot. 

Below, we construct predictions, feed those to ggplot, and build our plot around that. 
```{r predggplot}
incpreds <- predictions(increg3, 
                        newdata = datagrid(
                          selfemp = c("OtherEmp","SelfEmp"),
                          age = seq(18,88,10),
                          sex = c("Female", "Male"))) 
head(incpreds)
ggplot(incpreds,
       aes(x = age, y = predicted,
           ymin = predicted - std.error, 
           ymax = predicted + std.error)) +
  geom_ribbon(aes(fill = selfemp), alpha = .3) +
  geom_line(aes(color = selfemp), size = 1.5) + 
  facet_wrap(~sex) +
  theme_minimal() + 
  labs(title = "Predicted Value of Income (Logged)",
       subtitle = "Data: 2018 GSS",
       caption = "Shaded regions show one standard error from the mean.")
```


# Marginal Effects 
What if we want to see how the predicted value changes when we go from one value to another? We can find the "marginal effect" of a variable to see how the predicted effect changes. This is particularly important for logistic regression, where coefficients do not have a straightforward linear interpretation. 

## Another Model
Let's create another model, this time predicting whether the respondent has ever been unemployed during the past ten years.

Running this will throw us a warning, but we can ignore that. Another way of running it that won't produce the warning is to use `family = "quasibinomial"` instead of  `"binomial"`, but the output  will be the same. The big difference  in the two is that SE's will be larger, and we won't get a log-Likelihood (or AIC). 

```{r workmods}
unemp1 <- glm(unemp ~ logincome + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight, family = "binomial")
unemp2 <- glm(unemp ~ logincome + race + age + I(age^2) + educ + sex + selfemp,
              data = gss, weights = weight, family = "quasibinomial")
stargazer(unemp1, unemp2, single.row = T,
          omit.stat = c("F", "ser"), type = "text")
```

## Marginal Effects 
```{r mfx1}
marginaleffects(unemp1)
```

Notice the column "dydx." If you've ever taken calculus, you'll recognize this as the change in y (delta y) over the change in x (delta x). In other words, this is the change in predicted value at this point along the line. 

The values above show the marginal effect of each independent variable, for each observation. In this case, there are nine regressors (Black, Hispanic, other, age, education, sex, and self-employed) and 897 observations: $7*897 = 6279$

This is good info, but it's also a ton of info, and not in a form that is very useful. 

## Average Marginal Effect 

One way to summarize this information is to calculate the average of the marginal effects. We can get the marginal effects and then take the average of that across different levels. 

```{r ame}
work_mfx <- marginaleffects(unemp1) 
summary(work_mfx)
```

We can also display these in a table. Because we have contrasts (from categorical variables), we need to tell `modelsummary` how to group our terms. (We're going to work on `modelsummary`'s grouping terms more in two weeks, so no need to fully understand what happening on that line.) 
```{r ame-table}
mfx <- lapply(list(unemp1, increg3), marginaleffects)
modelsummary(mfx, stars = T,
             group = term + contrast ~ model, 
             output = "huxtable")
```

## Marginal Effects at the Mean 

You might've noticed that, in most models above, all the other factors were held at their means. This is actually a very common method, to hold all variables at their means. This is called the marginal effect at the mean (MEM). For each variable, it calculates the marginal effect when all other variables are held at their means. (Categorical variables are set at their modes.) This makes it both useful and easy useful to interpret. 

If we want to hold them at their means, we set `newdata` to `datagrid()`, with nothing inside, which defaults to everything at the mean (or mode).
```{r mem}
marginaleffects(unemp1, newdata = datagrid())
```

So above, we see that the average effect of a one year increase in education is a  0.01 decrease in the log odds of having been unemployed, when everything else is held at their means (or modes). Similarly, although we know the effect of age changes, on average, it decreases the log odds by 0.007.

For categorical variables, we use the contrast column to specify the effect. So being female increases the log odds of being unemployed by .006, compared to men, and being Black increases them by 0.058 compared to white. 

## Conditional Marginal Effects

We can also consider marginal effects conditional on the value of other covariates. 

Below, I construct the marginal effects where education is set at 12 years. The equation is the same as above, `marginaleffects(model), newdata = datagrid()`,  but with an argument into which we input our conditions.

```{r cme}
marginaleffects(unemp1, newdata = datagrid(educ = 12))
```

We can also specify multiple conditions, like where race is Black, age is 35, and education is at either 12 or 16. 

```{r cmegrid}
marginaleffects(unemp1, newdata = datagrid(educ = c(12,16),
                                           race = "Black",
                                           age = 35))
```

Notice above that each `rowid` has 2 levels, one where `educ==12` and one where `educ==16`. So row 1 is the marginal effect of Black on unemployment where race is Black, age is 35, and education is 12; row 2 is the same, but where education is 16. We do see a (very) slight difference in the marginal effect ("dydx") between these two points. 

### Conditional Marginal Effects Plots 

Conditional marginal effects tables are useful, but it often makes more sense to plot them to see what is going on. We often use this to illustrate and create interactions. 

```{r cmeplot}
plot_cme(unemp1, effect = "educ", condition = c("age")) + 
  geom_hline(yintercept = 0, linetype = "dotted")

```

In this graph, we see that education provides some sort of buffer against unemployment, such that each additional year of education decreases the log odds of having been unemployed in the past decade by about .015. This effect is fairly stable until we reach older ages, at which point it becomes nonsignificant (SE ribbon crossing the zero-line) and the buffer appears to go away. 

We can also plot the marginal effects conditional on multiple variables, including categorical ones. 
```{r cmeplot-cat}
plot_cme(unemp1, effect = "age", condition = c("selfemp","sex"))
```


This plot displays the marginal effects of age age on the odds of somebody being unemployed. We see that the effect is slightly higher for other-employed females than for everybody else, although this difference is neither large nor statistically significant. 


# Marginal Effects with Bayesian Models 
Lastly, let's explore creating predictions and marginal effects using Bayesian models. 

## Model Estimation

Let's create two models that replicate the ones we created before, `increg3` and `unemp1`.
```{r bayes}
bmod_unemp <- stan_glm(unemp ~ race + age + I(age^2) + educ + sex + selfemp,
                       data = gss, weights = weight, 
                       family = binomial(link = "logit"),
                       seed = seed, 
                       chains = 1,
                       refresh = 0,
                       iter = 2000,  
                       warmup = 1000)
bmod_inc <- stan_glm(logincome ~ race + age + I(age^2) + educ + sex + selfemp,
                     data = gss, weights = weight,
                     family = gaussian(link = "identity"),
                     seed = seed, 
                     chains = 1,
                     refresh = 0,
                     iter = 2000,  
                     warmup = 1000)

modelsummary(list(bmod_unemp,bmod_inc),
             output = "huxtable")

```

## Predictions
Fortunately, we can use the same functions above to predict from bayesian models. 

```{r bayespred}
predictions(bmod_unemp, variables = c("educ", "sex"))
```

We can also plot from them as well. 
```{r bayespredplot}
cowplot::plot_grid(
  plot_cap(increg3, condition = "age") + ggtitle("OLS Income"),
  plot_cap(unemp1, condition = "age") + ggtitle("MLE Unemployment"),
  plot_cap(bmod_inc, condition = "age") + ggtitle("Bayes Income"),
  plot_cap(bmod_unemp, condition = "age") + ggtitle("Bayes Unemployment")
  )
```

## Marginal Effects 
We can also use them to find the marginal effects. 
```{r bmfx}
marginaleffects(bmod_inc)
```

Since we have 1000 samples from the posterior distribution, we also get 1000 posterior samples of each marginal effect. We can summarize this uncertainty by calculating average marginal effects.

```{r bmfxsum}
summary(marginaleffects(bmod_unemp))
```

Let's also remind ourselves what the frequentist model's average marginal effects looked like and combine those into a data frame with the Bayesian ones. 

```{r mfx-comp}
mfxwork <- summary(marginaleffects(unemp1)) %>% 
  mutate(model = "MLE") %>% select(model, term, contrast, estimate)
bmfxwork <- summary(marginaleffects(bmod_unemp)) %>% 
  mutate(model = "Bayes") %>% select(model, term, contrast, estimate)

rbind(mfxwork, bmfxwork) %>% arrange(term, contrast)
```

Starting from the top, we see that the effects of each term are fairly similar, which is what we expected. For example, each additional year of education decreases the probability of having been unemployed by 0.013 in the MLE model and by .014 in the Bayesian model. 

The contrasts are slightly larger for categorical variables. For example, the effect of being self-employed is .05 in the Bayesian model compared to .01 in the MLE one. 

Lastly, let's plot the marginal effect of sex conditional on age:

```{r bcmeplot}
plot_cme(bmod_unemp, effect = "sex", condition = c("age")) + 
  geom_hline(yintercept = 0, linetype = "dotted")
```

In this plot, we see that the effect of sex on the probability of having been unemployed doesn't change much in direction or magnitude, but it does become less variable at somebody gets older. 

