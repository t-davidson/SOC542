---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Missing Data & Model Checking and Robustness"
author: Thomas Davidson
institute: Rutgers University
date: March 7, 2022
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: FALSE
      incremental: FALSE
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

set.seed(08901)

library(ggplot2)
library(tidyverse)
library(latex2exp)
library(kableExtra)
library(modelsummary)
library(viridis)
library(cowplot)
library(mice)
library(reshape2)
library(rstanarm)

options("modelsummary_format_numeric_latex" = "plain")
```

# Plan
- Missing data
- Model checking
- Model robustness
        
# Missing data
## What is missing data?
- Missing data occurs when we do not have an record of any data for one or more variables for an observation.
- This can occur for a variety of reasons including
    - Skip patterns
    - Survey non-response
    - Survey error
    - Data entry errors
    - Data handling errors
- The severity of the problem depends on why the data are missing and the amount of missingness

# Missing data
## Missing completely at random (MCAR)
- \textbf{Missing Completely at Random (MCAR)}
    - Probability $x_i$ is missing is constant across all observations
- Discarding missing cases does not result in any bias


# Missing data
## Missing at random (MAR)
- \textbf{Missing at Random (MAR)}
    - Probability $x_i$ is missing depends on observed variables.
- Discarding missing cases does not result in any bias if predictors of missingness are adjusted for.



# Missing data
## Missing not at random
- Missingness that depends on the missing value is considered \textbf{Missing not at Random (MNAR)}
    - e.g. Higher income respondents less likely to report income


# Missing data
## Simulating missingness
- We can use simulations to better understand the effects to different kinds of missingness
- Consider the following population model

$$z = N(0,1)$$
$$x = N(0,1)$$
$$y = 2x - 2z + N(0,10)$$


- We can vary the kind of missingess in $x$ and analyze how it effects $\hat{\beta_1}$ and $\hat{\sigma}$.

```{r missing-sims, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
results <- tibble()
for (i in 1:1E3) {
    z <- rnorm(N, mean = 1)
    x <- rnorm(N, mean = 1)
    y <- 2*x - 2*z + rnorm(N, sd = 10)
    x.mcar <- ifelse(rbinom(N,1,prob = 1-0.6), x, NA)
    x.mar <- ifelse(z > 0, ifelse(rbinom(N,1,prob = 1-0.8), x, NA), x)
    x.mc <- ifelse(x > 0, ifelse(rbinom(N,1,prob = 1-0.8), x, NA), x)
    m1 <- lm(y ~ x + z)
    m2 <- lm(y ~ x.mcar + z)
    m3 <- lm(y ~ x.mar + z)
    m4 <- lm(y ~ x.mar)
    m5 <- lm(y ~ x.mc +  z)
    coefs <- c(m1$coefficients[[2]],
               m2$coefficients[[2]],
               m3$coefficients[[2]],
               m4$coefficients[[2]],
               m5$coefficients[[2]])
    SE <- c(summary(m1)$coefficients[[2,2]],
               summary(m2)$coefficients[[2,2]],
               summary(m3)$coefficients[[2,2]],
               summary(m4)$coefficients[[2,2]],
               summary(m5)$coefficients[[2,2]])
    names <- c("x", "MCAR", "MAR", "MAR - Omitted", "MNAR")
    idx <- rep(i,5)
    df <- as_tibble(cbind(coefs, SE, names, idx))
    results <- bind_rows(results, df)
}
```

# Missing data
## MCAR, $p(Missing) = 0.6$
```{r mcar-plot, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
b <- ggplot(results %>% filter(names == "x" | names == "MCAR"), aes(x = as.numeric(coefs), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + theme(legend.position = "none")  + labs(x = TeX("$\\hat{\\beta_1}$"))

s <- ggplot(results %>% filter(names == "x" | names == "MCAR"), aes(x = as.numeric(SE), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + labs(y = "", x = TeX("$\\hat{SE}$"), fill = "Model")
s2 <- s + theme(legend.position = "none")
l <- get_legend(s)
plot_grid(b, s2, l, nrow= 1)
```


# Missing data
## MAR, $p(Missing) = 0.8$ if $z > 0$
```{r mar-plot, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
b <- ggplot(results %>% filter(names == "x" | names == "MAR"), aes(x = as.numeric(coefs), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + theme(legend.position = "none")  + labs(x = TeX("$\\hat{\\beta_1}$"))

s <- ggplot(results %>% filter(names == "x" | names == "MAR"), aes(x = as.numeric(SE), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + labs(y = "", x = TeX("$\\hat{SE}$"), fill = "Model")
s2 <- s + theme(legend.position = "none")
l <- get_legend(s)
plot_grid(b, s2, l, nrow= 1)
```

# Missing data
## MAR, $z$ omitted
```{r mar-plot2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
b <- ggplot(results %>% filter(names == "x" | names == "MAR" | names == "MAR - Omitted"), aes(x = as.numeric(coefs), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + theme(legend.position = "none")  + labs(x = TeX("$\\hat{\\beta_1}$"))

s <- ggplot(results %>% filter(names == "x" | names == "MAR" | names == "MAR - Omitted"), aes(x = as.numeric(SE), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + labs(y = "", x = TeX("$\\hat{SE}$"), fill = "Model")
s2 <- s + theme(legend.position = "none")
l <- get_legend(s)
plot_grid(b, s2, l, nrow= 1)
```

# Missing data
## MNAR, $p(Missing) = 0.8$ if $x > 0$
```{r mfx-plot, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
b <- ggplot(results %>% filter(names == "x" | names == "MNAR"), aes(x = as.numeric(coefs), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + theme(legend.position = "none")  + labs(x = TeX("$\\hat{\\beta_1}$"))

s <- ggplot(results %>% filter(names == "x" | names == "MNAR"), aes(x = as.numeric(SE), group = names, fill = names)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal() + labs(y = "", x = TeX("$\\hat{SE}$"), fill = "Model")
s2 <- s + theme(legend.position = "none")
l <- get_legend(s)
plot_grid(b, s2, l, nrow= 1)
```

<!--TODO: Make a better simulation. The disparities are not very clear in some cases-->

# Missing data
## Missingness in practice
- It is often difficult to determine why data are missing
    - Knowledge of domain and data generation process helpful
- MCAR is a very strong assumption
- MAR is more reasonable, but it is difficult to determine whether missingness is due to an unobserved factor
    - Including more predictors in a model helps to reduce concerns
    
# Missing data
## Addressing missingness
- Three approaches
    1. Delete missing cases
    2. \textit{Simple} imputation
    3. \textit{Multiple} imputation
    
# Missing data
## Deleting missing data
- \textbf{Complete-case analysis / listwise deletion}
    - Delete all rows where $y$ or $x$ or $z$ is missing
- If missingess is not MCAR then results could be biased
- If many predictors, sample size can reduce substantially

# Missing data
## Deleting missing data
- \textbf{Available-case analysis / pairwise deletion}
    - Make comparisons where data are available
- In the previous example, one might use the following model to get an estimate of the effect of $z$ on $y$
    
    $$y = \beta_0 + \beta_1z + u$$
    
    
- The variable $x$ is ignored, otherwise we drop cases where $x$ is missing.
- One might also estimate the model including $x$ and compare the results.
- Like listwise deletion, bias can occur if systematic differences between missing and non-missing cases.

# Missing data
## Simple imputation: Guessing the mean
- Instead of removing data, we can try to "guess" the missing values.
- A good guess is the mean of the observed data, \textbf{mean imputation}. 
- But this can distort the distribution and underestimate the standard deviation.

# Missing data
## Simple imputation
```{r guess, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
X <- rnorm(N)
print(mean(X))
print(sd(X))
X.m <- ifelse(rbinom(N, 1, 1-0.2), X, NA)
X.g <- ifelse(!is.na(X.m), X, mean(X.m[!is.na(X.m)]))
print(mean(X.g))
print(sd(X.g))
```

# Missing data
## Simple imputation
```{r guess-plot, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
d <- cbind(X, X.m, X.g)
colnames(d) <- c("Complete", "Missing", "Mean Imp.")
d2 <- melt(d)
colnames(d2) <- c("ID", "X", "x")
    
ggplot(d2, aes(x = x, group = X, fill = X)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal()
```

# Missing data
## Simple imputation: Random imputation
- We can address some of the limitations of mean imputation by using the full distribution of the observed variable. This is known as \textbf{random imputation}
- However, the imputed values will not necessarily reflect the underlying association between variables and we do not make use of other information.

# Missing data
## Simple imputation: Random imputation
```{r guess-random, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
missing <- is.na(X.m)
X.g2 <- X.m
X.g2[missing] <- sample(X.m[!is.na(X.m)], 
                        length(X.m[missing]), 
                        replace = TRUE)
```


# Missing data
## Simple imputation: Random imputation
```{r guess-plot2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
d <- cbind(X, X.m, X.g2)
colnames(d) <- c("Complete", "Missing", "Random Imp.")
d2 <- melt(d)
colnames(d2) <- c("ID", "X", "x")
    
ggplot(d2, aes(x = x, group = X, fill = X)) +  geom_density(alpha = 0.5) + scale_fill_viridis_d(option = "cividis") + theme_minimal()
```


# Missing data
## Simple imputation: Prediction with regression
- We can improve our imputation model by making use of additional information in other variables.
    - For example, we could estimate a model to predict $x$ using other covariates and then use these predictions in place of missing vaulues
    
# Missing data
## Simple imputation: Prediction with regression
```{r impute-model, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
v <- rnorm(N)
w <- rnorm(N)
x <- 0.5*v + 0.5*w + rnorm(N)
y <- x - 2*w + rnorm(N)

x.m <- ifelse(rbinom(N, 1, 0.4), x, NA) # MCAR

imp.model <- lm(x.m ~ v + w)
```


# Missing data
## Simple imputation: Prediction with regression
```{r impute-model2, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
preds <- predict(imp.model, 
                 newdata = as.data.frame(cbind(v, w)))

x.imp <- x.m
for (i in 1:length(x.m)) {
    if (is.na(x.m[i])) {
        x.imp[i] <- preds[i]
    }
}
```


# Missing data
## Simple imputation: Prediction with regression
```{r impute-model-results, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
missing <- is.na(x.m)
x.mean <- x.m
x.mean[missing] <- mean(x.m[!is.na(x.m)])
x.r <- x.m
x.r[missing] <- sample(x.m[!is.na(x.m)], length(x.r[missing]), replace = TRUE)
d1 <- as.data.frame(cbind(y,x,w))
m1 <- lm(y ~ x + w, data = d1)
d2 <- as.data.frame(cbind(y,x.m,w)) %>% rename(x = x.m)
mm <- lm(y ~ x + w, data = d2)
d3 <- as.data.frame(cbind(y,x.mean,w)) %>% rename(x = x.mean)
m2 <- lm(y ~ x + w, data = d3)
d4 <- as.data.frame(cbind(y,x.r,w)) %>% rename(x = x.r)
m3 <- lm(y ~ x + w, data = d4)
d5 <- as.data.frame(cbind(y,x.imp, w)) %>% rename(x = x.imp)
m4 <- lm(y ~ x + w, data = d5)

modelsummary(list("Complete" = m1, "Listwise" = mm, "Mean" = m2, "Random" = m3, "Predicted"=  m4),
             star = TRUE, gof_omit = "Log|AIC|BIC|F", stars = FALSE, output = "latex")
```

# Missing data
## Simple imputation
- Limitations
    - Each process becomes cumbersome if we have multiple variables with missing data and observations with more than one variable missing.
        - e.g. How do we predict $x$ if we are missing other variables?
    - These approaches are *deterministic*, failing to take into account the uncertainity in the imputations.

# Missing data
## Multiple imputation
- \textbf{Multiple imputation (MI)} methods address both issues:
    - MI models use observed data to predict missing values.
    - Multiple missing variable can be imputed simultaneously.
    - Prediction uncertainity can be incorporated into the estimates.
    
# Missing data
## Multiple imputation: Algorithms
- Multiple imputation algorithms work by using existing data to predict missing values across the entire dataset
- Generally, these algorithms use iterative procedures, predicting a subset of the missing values at a time
- These algorithms converge when the distributions of the predicted datasets look like the distributions in the original data
- Often these algorithms use Bayesian techniques such as MCMC sampling\footnote{\tiny See the \href{https://stats.oarc.ucla.edu/stata/seminars/mi_in_stata_pt1_new/}{mi command in Stata} for example.}

# Missing data
## Multiple imputation: MI in R
- There are a number of different MI packages available in R
- Two commonly used packages are
    - MICE (Multivariate Imputation via Chained Equations)
    - Amelia
        - Particularly useful for panel data
    
# Missing data
## Multiple imputation: Pooling
- MI algorithms can produce $M$ imputed datasets. 
    - In each case, we can compute an estimate, $\hat{\beta_1}_m$ and a standard error $\hat{SE}_m$.
- The overall estimate is an average over the $M$ datasets, known as a \textbf{pooled} estimate:\footnote{\tiny See GHV p. 326 for the formula for the standard error of the pooled estimate.}


$$\hat{\beta} = \frac{1}{M} \sum_{m=1}^M \hat{\beta_1}_m$$

<!--
- The standard error needs to account for the variance within and between imputations:

$$SE_\beta = \sqrt{W + (1 +\frac{1}{M})B}$$

where within variance $W = \frac{1}{M} \sum_{m=1}^M SE_m^2$ and between variance $B = \frac{1}{M-1} \sum_{m=1}^M (\hat{\beta}_m -\hat{\beta})^2$.\footnote{\tiny See GHV p. 326.}
-->


# Missing data
## Multiple imputation with MICE
```{r mice, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
x <- x.m
data <- cbind(y, v, w, x)

M <- mice(data, m=10, method = "pmm",
          seed=08901, 
          printFlag = FALSE)
```

# Missing data
## Multiple imputation with MICE
```{r mice2, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
# Impute using m=1
simple.imp <- complete(M,1)
mi.s <- lm(y ~ x + w, data = simple.imp)

# Pool over all M
fits <- with(M, lm(y ~ x + w))
mi.M <- pool(fits)
```

# Missing data
## Multiple imputation with MICE
```{r mice3, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Complete" = m1, "Predicted"=  m4,
                  "MI m" = mi.s, "MI M" = mi.M),
             star = TRUE, gof_omit = "Log|AIC|BIC|F", stars = FALSE, output = "latex")
```
<!-- Note: The MI algorithm fits poorly unless y is included.-->

# Model checking
## Diagnostics
- We have already covered several different diagnostics for model checking
    - Residuals and standard error of the residuals
    - Predicted values
    - $R^2$ and adjusted $R^2$
    - Standard errors on coefficients and p-values
    - F-statistic
    
# Model checking
## Outliers
- Outliers are extreme data points that deviate from the distribution of other values
    - Scatterplots of raw data and residual can be helpful for identifying these
- An outlier has \textbf{leverage} if the addition of the observation results in a change in the slope of the regression line
- Such cases are considered *influential* if they result in substantial changes to the regression results
    - e.g. Differences in statistical sigificance, sign, magnitude

# Model checking
## Outliers
```{r outlier1, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 100
X <- rnorm(N)
Y <- 0.5*X + rnorm(N)
```


# Model checking
## Outliers
```{r outlier2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
d <- as.data.frame(cbind(X,Y))
ggplot(data = d, aes(x=X, y=Y)) + geom_point() + geom_smooth(method = "lm", se = F) + theme_minimal()
```

# Model checking
## Outliers: Outlier with low leverage
```{r outlier3, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
X[N+1] <- 5
Y[N+1] <- 0.5*X[N+1]
d <- as.data.frame(cbind(X,Y))

ggplot(data = d, aes(x=X, y=Y)) + geom_point(color = dplyr::case_when(X >=4 ~ "red", X < 4 ~ "black")) + geom_smooth(method = "lm", se = F) + theme_minimal()
```


# Model checking
## Outliers: Outlier with high leverage
```{r outlier4, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
X[N+1] <- 5
Y[N+1] <- -10
d <- as.data.frame(cbind(X,Y))

ggplot(data = d, aes(x=X, y=Y)) + geom_point(color = dplyr::case_when(X >=5 ~ "red", X < 5 ~ "black")) + geom_smooth(method = "lm", se = F) + theme_minimal()
```


# Model checking
## Underfitting
- A model is \textbf{underfit} if it does not sufficiently explain the variance in the outcome.
- Consider the following population model:

$$y = \beta_0 + \beta_1x + \beta_2x^2 + u$$

- The following model underfits because it does not account for the quadratic relationship:

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x + u$$

- In short, we fail to observe the *signal* in the data (Molina and Garip 2019)

# Model checking
## Overfitting
- A model is \textbf{overfit} if it also explains *noise* in addition to the signal in the data (Molina and Garip 2019).
- Using the previous population, consider we estimate the following model:

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x + \hat{\beta_2}x^2  + \hat{\beta_3}z + \hat{\beta_4}z^2  + u$$

- The additional parameters $\hat{\beta_3}$ and $\hat{\beta_4}$ do not correspond to any of the population parameters, but may still explain some of the random variance in the outcome.

# Model checking
## Underfitting, overfitting, and generalization error
- Models suffering from under and overfitting will fail to generalize
    - Underfit models do not sufficiently explain variation in the outcome in the sample or population
    - Overfit models explain patterns in the sample that do not generalize to the population
- \textbf{Generalization error} refers to the expected prediction error when a model is applied to new data.

# Model checking
## Cross-validation
- \textbf{Cross-validation} is an approach used in machine-learning to assess the extent to which a predictive model can generalize to unseen data.
- The technique allows us to measure generalization error:
    1. Estimate an model using a sample $X$.
    2. Use the fitted model to predict the outcome for a new dataset $X'$.
    3. Compare the predictive accuracy (e.g. mean squared error) across the two datasets:
        - Model generalizes well if $MSE(\hat{y}=f(X)) \approx MSE(\hat{y}=f(X'))$ and both are low
        - Model overfits if $MSE(\hat{y}=f(X)) << MSE(\hat{y}=f(X'))$
        - Model underfit if both MSE scores are high

# Model checking
## Cross-validation
- Different kinds of cross-validation procedures are often used to evaluate generalization error:
    - \textbf{k-fold cross-validation}: data are split into $k$ subsets. Models are estimated using $k-1$ subsets and predictions made for held-out set. Prediction error is averaged over $k$ held-out sets.
    - \textbf{Leave-one-out cross-validation}: same procedure where each subset is a single datapoint. Requires estimation of $N$ models.
    - \textbf{Temporal cross-validation}: Useful with panel data. Estimate a model with data from time $t$ the assess predictions for data recorded at $t+1$.
    
# Model checking
## Cross-validation: `loo`
- The `rstanarm` package includes a function `loo`, which computes an approximation of LOO-CV, avoiding the need to fit N models.
- Models can be compared using the *expected log pointwise predictive density* (ELPD), a quantity that captures the predictive accuracy of the model (McElreath 7.2-4, GHV 11.8). 
- The function can also be used to compute a k-fold CV score and an information theoretic measure WAIC.\footnote{\tiny See the \href{http://mc-stan.org/rstanarm/reference/loo.stanreg.html}{documentation} and \href{https://mc-stan.org/rstanarm/articles/rstanarm.html}{vignette} for further details on implementation.}

    
# Model checking
## Cross-validation: LOO-CV
```{r loo, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
x <- rnorm(N)
z <- rnorm(N)
y <- 0.1*x + 2*(x^2) + rnorm(N)
df <- as.data.frame(y,x,z)

m <- stan_glm(y ~ x + I(x^2),  data = df, 
                family = "gaussian", chains =1 , refresh = 0)
m.u <- stan_glm(y ~ x,  data = df, 
                family = "gaussian", chains =1 , refresh = 0)
m.o <- stan_glm(y ~ x + I(x^2) + z + I(z^2),  data = df, 
                family = "gaussian", chains =1 , refresh = 0)
```


# Model checking
## Cross-validation: LOO-CV
<!--TODO: Discuss ElPD-->
The `loo` function provides an approximation of the LOO-CV for an estimated model
```{r loo3, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
print(loo(m))
```

# Model checking
## Cross-validation: LOO-CV
Individual points are scored using Pareto-Smoothed Importance Sampling (PSIS). High *pareto k* values ($k > .7$) indicate observations with high leverage.
```{r loo4, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
plot(loo(m.u))
```

# Model checking
## Cross-validation: LOO-CV
- `loo_compare` can be used to compare different models. The results rank the models from best to worst.
```{r loo5, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
loo_compare(loo(m), loo(m.u), loo(m.o))
```


# Model checking
## Posterior predictive checks
- We can also evaluate Bayesian models by examining the posterior predictive distribution
- Recall that Bayesian models are *generative*;
    - We can use the posterior distribution to make predictions, creating new, hypothetical datasets
- These predictions can be used to evaluate how well the models fit the data, including key statistics

# Model checking
## Posterior predictive checks
```{r ppc, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pp_check(m)
```

# Model checking
## Posterior predictive checks
```{r ppc2, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pp_check(m, plotfun = "stat_2d", stat = c("mean", "sd"))
```

# Model checking
## Posterior predictive checks
```{r ppc3, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pp_check(m.u)
```

# Model checking
## Posterior predictive checks
```{r ppc4, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pp_check(m.u, plotfun = "stat_2d", stat = c("mean", "sd"))
```

# Model checking
## Prediction and explanation
- Predictive models are designed to predict $\hat{y}$'s with high accuracy.
- In sociology, we typically estimate explanatory models where the primary goal is to estimate one or more $\hat{\beta}$'s and prediction is typically used to explore relationships between variables.
- Nonetheless, generalization error can be a useful heuristic for comparing and selecting different models, particularly when we do not have strong theory to guide model specification.\footnote{\tiny See Watts, Duncan J. 2014. “Common Sense and Sociological Explanations.” American Journal of Sociology 120 (2): 313–51. https://doi.org/10.1086/678271 for further elaboration of this idea and Mullainathan, Sendhil, and Jann Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106. https://doi.org/10.1257/jep.31.2.87 for some discussion of the limitations of predictive modeling.
}

# Model checking
## Multicollinearity
- Recall that multicollinearity occurs when predictors are highly correlated and results in increased variance
- High pairwise correlations might indicate *potential* collinearity, but the issue can only be diagnosed after controlling for all relevant predictors

# Model checking
## Multicollinearity: VIF
- The \textbf{Variance Inflation Factor (VIF)} can be used to diagnose highly collinear predictors
- Consider the regression model $y = \beta_0 + \beta_ix_i + \beta_jx_j + ... + \beta_kx_k + u$
- A score is calculated for each *independent variable* using the following approach:
    - For $x_i$ in $x_{i=1},...,x_k$, regress $x_i = \alpha_0 + \alpha_1x_{j} + ... + \alpha_2x_{k} + u$
    - Use $R^2$ from the model to calculate $VIF(\hat{\beta_i}) = \frac{1}{1 - R^2_i}$
- VIF scores greater than $\approx 5-10$ indicate that a predict is highly collinear with one or more of the other predictors


# Model checking
## Multicollinearity: VIF
```{r vif, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
x <- rnorm(N)
x2 <- rnorm(N)
x3 <- 0.8*x + rnorm(N)
y <- x + 2*x2 + 0.5*x3 + rnorm(N)

m <- lm(y ~ x + x2 + x3)
library(car)
vif(m)
```

<!--
# Model checking
## Multicollinearity: Bayesian approaches

-->

<!--
# Model checking
## Additional Bayesian diagnostics
- There are several other diagnostics specific to Bayesian inference and help to determine whether the MCMC algorithms have converged\footnote{\tiny See the \href{https://mc-stan.org/misc/warnings}{Stan documentation} for discussion of these issues and how to address them.}
    - \textbf{R-hat values} indicate whether a model has converged. 
        - Values should be 1.01 or less
    - \textbf{Divergent transitions} indicate problems with the MCMC estimation
        - The sampler is making large "jumps" rather than smoothly exploring the posterior distribution
    - \textbf{Trace plots} are useful for assessing MCMC chains
        - Ideally, chains should be moving randomly, without strong autocorrelation
-->

# Model robustness
## Defining robustness
- \textbf{Model robustness} refers to how *robust* the results of a given model are to alternative specifications.
    - The concern is that a result (such as $p < 0.05$) is sensitive to a particular specification of a model
- Often we try to mitigate such concerns by estimating several different specifications of a model

# Model robustness
## How robust are our results?
- Recent work calls for greater attention to specification issues as a way to address robustness concerns (Young and Holsteen 2017, Muñoz and Young 2018)
    - Critique: Reporting a handful of ad hoc specifications is insufficient to ensure robustness
    - Solution: Estimate models with *every possible combination of independent variables* and assess the distribution of coefficients
    - The goal is to explore the entire *model space* and to construct a distribution of estimates
    
# Model robustness
## How robust are our results?
```{r hy, out.width="70%",out.height="70%", fig.align="center"}
include_graphics('../img/young_holsteen.png')
```
\tiny \centering Young and Holsteen 2017.

# Model robustness
## How robust are our results?
```{r my, out.width="70%",out.height="70%", fig.align="center"}
include_graphics('../img/young_munoz.png')
```
\tiny \centering Muñoz and Young 2018.
    
# Model robustness
## Bayesian Model Averaging
- Bayesians have proposed a similar idea known as Bayesian Model Averaging
    - Estimate several models and construct an average across the models, weighted by the model fit, i.e. higher weights to better models
- There has been some debate about whether this approach is preferable to the Young-Holsteen-Muñoz technique.
    - The latter argue that it is problematic to weight different models if we do not know which is better a priori and that weighting requires more assumptions.\footnote{\tiny See Slez' 2017 comment on Young and Holsteen and the rejoinder by the latter and Bruce Western's 2018 \href{https://doi.org/10.1177/0081175018799095}{comment} on Muñoz and Young}


# Conclusions
- Missing data
    - Carefully examine any patterns of missing data
    - Choose an appropriate strategy to address the problem
- Model checking
    - Use diagnostic checks to identify and address potential issues with models
- Model robustness
    - Estimate multiple specifications to ensure results are robust

# Next week
- Spring break!
- After spring break
    - Generalized linear models
