---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Interactions"
author: Thomas Davidson
institute: Rutgers University
date: February 28, 2022
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

set.seed(08901)

library(ggplot2)
library(tidyverse)
library(latex2exp)
library(kableExtra)
library(modelsummary)

options("modelsummary_format_numeric_latex" = "plain")
```

# Plan
- Introducing interactions
- Types of interactions and their interpretations
- Marginal effects

# Introducing interactions
## What is an statistical interaction?
- Consider the following population model:

$$y = \beta_0 + \beta_1x + \beta_2z + u$$


- The coefficients $\beta_1$ and $\beta_2$ measure the relationship between $x$ and $y$ and $z$ and $y$, respectively.
    - The interpretation of either coefficient requires that we hold the other constant.
- What if we expect the effect of $x$ to vary as a function of $z$?

    
# Introducing interactions
## What is an statistical interaction?
- If we expect there to be an \textbf{interaction} between $x$ and $z$, such that the effect of $x$ on $y$ varies according to the level of $z$, we can add an \textbf{interaction term} into our model formula.

$$y = \beta_0 + \beta_1x + \beta_2z + \beta_3xz + u$$

- $\beta_0$ and $\beta_1$ are now considered as the \textbf{main effects}. 
- $\beta_3$ is the coefficient for the interaction term, representing the effect of $x$ times $z$.
  
# Introducing interactions
## A simple population model
```{r simple-int, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
x <- rnorm(N)
z <- rnorm(N)
y <- 3*x + 2*z + -5*(x*z) + rnorm(N, 10)
```

# Introducing interactions
## Comparing models
```{r simple-model, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m.r <- lm(y ~ x + z)
m <- lm(y ~ x + z + x:z)
modelsummary(list(m.r, m), gof_omit = "Log|AIC|BIC", stars = TRUE, output = "latex")
```

# Introducing interactions
## Example: intersectional inequalities
- We can use interaction terms as a way to encode theoretical knowledge about the relationship between variables.
- For example, if we expect there to be differences in income related to the interaction between sex and race, we can add an interaction term to a model:

$$Income = \beta_0 + \beta_1Sex + \beta_2Race + \beta_3Age + \beta_4Sex*Race + u$$

# Introducing interactions
## Main effects and interactions
- In general, it is recommended to *include the main effects in any model with interactions*.
    - Type II errors are more likely when interpreting interaction terms with main effects omitted.
    - The interpretation of the model can change substantially if main effects are excluded.\footnote{\footnotesize See this Stata blog for further discussion: https://stats.oarc.ucla.edu/stata/faq/what-happens-if-you-omit-the-main-effect-in-a-regression-model-with-an-interaction/}

# Types of interactions
## Dummy-dummy

$$y = \beta_0 + \beta_1 Male + \beta_2 Union + \beta_3 Male*Union + u $$

# Types of interactions
## Dummy-dummy
```{r dd, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Loading data
gss <- haven::read_dta("../labs/lab-data/GSS2018.dta") %>%
    filter(age <= 89) %>% haven::zap_labels() %>%
    mutate(unions = ifelse(union == 1, 1, 0),
           sex = ifelse(sex == 1, 1, 0),
           race = as.factor(race))
gss$race <- recode_factor(gss$race, "1" = "White", "2" = "Black", "3" = "Other")

m <- lm(realrinc ~ sex + union, data = gss)
m.i <- lm(realrinc ~ sex + union + sex:union, data = gss)
modelsummary(list(m, m.i), gof_omit = "Log|AIC|BIC", stars = TRUE, output = "latex")
```

# Types of interactions
## Dummy-dummy

$$y = \beta_0 + \beta_1 Male + \beta_2 Union + \beta_3 Male*Union + u $$


- Female and non-unionized are the reference categories.
- $\beta_1$ and $\beta_2$ represent the main effects of sex and union membership on the outcome.
- The coefficient $\beta_3$ represents the expected difference in the effect of union membership for men versus women.\footnote{\footnotesize Note the symmetrical interpretation here: the  difference in the effect of sex for union members versus non-members. See McElreath 8.2 for further discussion.}
- The expected income for a male unionized worker is $\beta_0 + \beta_1 + \beta_2 + \beta_3$. The same quantity for a female unionized worker is $\beta_0 + \beta_2$.

# Types of interactions
## Continuous-dummy
$$y = \beta_0 + \beta_1 Age + \beta_2 Sex + \beta_3 Age * Sex + u $$

# Types of interactions
## Continuous-dummy
```{r cd, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ age + sex, data = gss)
m.i <- lm(realrinc ~ age + sex + age:sex, data = gss)
modelsummary(list(m, m.i), gof_omit = "Log|AIC|BIC", stars = TRUE, output = "latex")
```

# Types of interactions
## Continuous-dummy
$$y = \beta_0 + \beta_1 Age + \beta_2 Sex + \beta_3 Age * Sex + u $$


- The coefficients $\beta_1$ and $\beta_2$ represent the main effects of age and sex on income.
- For females, $\beta_1$ represents the relationship between age and income. For males, the relationship is $\beta_1$ + $\beta_3$.
    - Thus, the interaction term allows the *slope* to vary according to sex.
    
# Types of interactions
## Continuous-continuous
$$y = \beta_0 + \beta_1 Age + \beta_2 Educ + \beta_3 Age * Educ + u $$

# Types of interactions
## Continuous-continuous
```{r cc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ age + educ, data = gss)
m.i <- lm(realrinc ~ age + educ + age:educ, data = gss)
modelsummary(list(m, m.i), gof_omit = "Log|AIC|BIC", stars = TRUE, output = "latex")
```

# Types of interactions
## Continuous-continuous
$$y = \beta_0 + \beta_1 Age + \beta_2 Educ + \beta_3 Age * Educ + u $$

- The intercept no longer has a meaningful education (income when age and education equal zero).
    - GHV 12.2 discuss standardization as an approach to make intercepts more interpretable in such contexts.
- $\beta_1$ and $\beta_2$ represent the main effects of age and education.
- The interaction term $\beta_3$ captures how the effect of education on income varies as a function of age.


# Types of interactions
## Continuous-continuous
- The effect of education on income is now also a function of age:

$$\frac{\Delta y}{\Delta_{Educ}} = \beta_2 + \beta_3Age$$

- Similarly,

$$\frac{\Delta y}{\Delta_{Age}} = \beta_1 + \beta_3Educ$$


# Types of interactions
## Continuous-continuous
- If Age changes by $\Delta$Age and Educ by $\Delta$Educ, the expected change in $y$ is:

$$\Delta y = (\beta_1 + \beta_3Educ)\Delta Age + (\beta_2 + \beta_3 Age)\Delta Educ + \beta_3 \Delta Age \Delta Educ$$

- The coefficient $\beta_3$ represents the effect of a unit increase in age *and* education, beyond the sum of the individual effects of unit increases alone.

# Types of interactions
## Dummy-categorical
```{r dc, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + race, data = gss)
m.i <- lm(realrinc ~ sex + race + sex:race, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = TRUE, output = "latex")
```

# Types of interactions
## Dummy-categorical
$$y = \beta_0 + \beta_1Male + \beta_2Black + \beta_3Other + \beta_4Black\:Male + \beta_5 Other\:Male + u$$


- There is a separate coefficient for the interaction between the dummy variable and each of the categories, with the exception of the reference group.
- The interpretation is the same as the dummy-dummy model.

# Types of interactions
## Continuous-categorical
```{r ct, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
gss$race <- as.factor(gss$race)
m <- lm(realrinc ~ age + race, data = gss)
m.i <- lm(realrinc ~ age + race + age:race, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = TRUE, output = "latex")
```

# Types of interactions
## Categorical-categorical
```{r tt, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Ref = word of god, 2 = inspired word, 3 = ancient book
gss.b <- gss %>% filter(bible <= 3) %>% mutate(bible = as.factor(bible))
m <- lm(realrinc ~ race + bible, data = gss.b)
m.i <- lm(realrinc ~ race + race:bible, data = gss.b)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = TRUE, output = "latex")
```

# Types of interactions
## Three-way interactions
```{r three, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + race + union, data = gss.b)
m.i <- lm(realrinc ~ sex + race + union + sex:race:union, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", statistic = NULL, stars = TRUE, output = "latex")
```

# Types of interactions
## Interpreting interactions
- Interactions terms make models more challenging to interpret.
    - Like polynomial regression, the effect of a single predictor is represented by more than one coefficient (e.g. $y = \beta_0 + \beta_1x + \beta_2z + \beta_3xz + u$).
- Three-way and more complex interactions are even more difficult to interpret and should be avoided unless there are strong theoretical reasons to use them.

# Marginal effects
## Definitions
- A \textbf{marginal effect} is the relationship between change in single predictor and the dependent variable while *holding other variables constant*.
- The \textbf{average marginal effect (AME)} is the *average* change in the outcome $y$ as a function of a unit change in $x_i$ over all observations.
    - Coefficients in a standard OLS model represent average marginal effects.
- This quanity becomes more complicated when interaction terms are included, since the effect of a change in $x_i$ now depends on multiple parameters.

# Marginal effects
## Computing marginal effects
- Frequentist marginal effects computed by calculating *partial derivatives* and approximating variance.
    - e.g. $ME(x_i) = \frac{\delta y}{\delta x_i}$.
    - We can use the `margins` package in R to do this.\footnote{\footnotesize See Thomas Leeper's \href{https://cran.r-project.org/web/packages/margins/vignettes/TechnicalDetails.pdf}{documentation} for the margins package for further details.}
- Bayesian marginal effects can be calculated by sampling from the posterior distribution.


    
# Marginal effects
## Marginal effects and OLS regression
```{r margins-model1, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + educ, data = gss)
modelsummary(list(m), gof_omit=".*", stars = TRUE, output = "latex")
```

# Marginal effects
## Marginal effects and OLS regression
Note how the average marginal effects are equal to the OLS coefficients.
```{r margins1, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(margins)
me <- margins(m)
summary(me)
```

# Marginal effects
## Marginal effects with non-linear variables
```{r margins-model2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + I(age^2) + educ, data = gss)
modelsummary(list(m), gof_omit=".*", statistic = NULL, stars = TRUE, output = "latex")
```
\footnote{\footnotesize Note the use of the `I` symbol when computing age-squared. This ensures that the margins command recognizes that this variable also relates to age.}
<!--Discuss why age is specified in this way-->

# Marginal effects
## Marginal effects with non-linear variables
The margins commands are the same as above. Note how the AME now represents the total effect of age across the two parameters.
```{r margins-nl, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
me <- margins(m)
summary(me)
```

# Marginal effects
## Marginal effects with non-linear variables
We can also visualize the marginal effect of age in a continuous space.
```{r margins-nl-plot, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
cplot(m, "age")
```

# Marginal effects
## Marginal effects with interactions
```{r margins-model3, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + I(age^2) + educ + sex:educ + sex:age, data = gss)
modelsummary(list(m), gof_omit=".*", statistic = NULL, stars = TRUE, output = "latex")
```

# Marginal effects
## Marginal effects with interactions
```{r margins3, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
me <- margins(m)
summary(me)
```

# Marginal effects
## Plotting marginal effects
The `margins` package includes a `plot()` function to show the results of the table. The output can also be modified using `ggplot2`.
```{r margins-bars, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
plot(me)
```

# Marginal effects
## Plotting conditional marginal effects
The `cplot` function can be used to plot the marginal effect while conditioning on another predictor. In this case, the marginal effect of sex on income over the range of age.
```{r margins-line2, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
cplot(m, x = "age", dx = "sex", what = "effect")
```

<!-- TODO: Return to this when we discuss GLMs.
# Marginal effects
## Bayesian estimation
```{r bayes1, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
library(rstanarm)
m.b <- stan_glm(realrinc ~ sex + age + I(age^2) + educ + sex:educ + sex:age, data = gss, family = "gaussian", chains =1 , refresh = 0)
print(m.b$coefficients)
```

# Marginal effects
## Bayesian marginal effects
The `ggeffects` package can calculate \textbf{marginal effects at means (MEM)}. Here the result show the effect of sex on income, holding age and education at the mean value.
```{r bayes2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
library(ggeffects)
df <- ggpredict(m.b)
df$sex
```

# Marginal effects
## Bayesian marginal effects
To get *average marginal effects*, we need to compute the expected value of the outcome at different levels of predictors.
```{r bayes3, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
library(gridExtra)
library(tidybayes)
fake.data <- expand_grid(sex = c(0,1),
                         educ = 1:20,
                         age = 18:80)

tidy_epred <- m.b %>% epred_draws(newdata = fake.data)

p1 <- ggplot(tidy_epred %>% filter(sex == 1), aes(x = age, y = .epred)) + stat_lineribbon() + theme_tidybayes() + theme(legend.position="none")
p2 <- ggplot(tidy_epred %>% filter(sex == 0), aes(x = age, y = .epred)) + stat_lineribbon() + theme_tidybayes() + theme(legend.position="none")
grid.arrange(p1, p2, nrow = 1)
```

-->

# Marginal effects
## Marginal effects and generalized linear models
- Marginal effects are even more important when we consider generalized linear models (e.g. logistic regression) since coefficients often do not have clear interpretations on the outcome scale.\footnote{\footnotesize See the recommended reading, Mize 2019, for further discussion.}

# Next week
## Topic
- Missing data
- Model specification, comparison, and robustness