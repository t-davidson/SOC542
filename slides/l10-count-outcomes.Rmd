---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Count outcomes"
author: Thomas Davidson
institute: Rutgers University
date: April 4, 2022
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: FALSE
      incremental: FALSE
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")
library(ggplot2)
library(tidyverse)
library(latex2exp)
library(kableExtra)
library(modelsummary)
library(viridis)
library(cowplot)
library(mice)
library(reshape2)
library(rstanarm)
# library(huxtable) # Causes problems knitting, including ! LaTeX Error: Environment centerbox undefined.

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

kable <- function(data) {
  knitr::kable(data, digits = 3) %>% 
    kable_styling(position = "center")
}

set.seed(08901)

options("modelsummary_format_numeric_latex" = "plain")
```


# Course updates
- Homework 4 will be released new week
    - Count outcomes
    - Categorical and ordered outcomes

# Plan
- Count outcomes
- Poisson regression
- Overdispersion and negative-binomial regression
- Offsets
- Zero-inflated models

# Count outcomes
- Count outcomes are variables defined as *non-negative integers*.
    - Values must be 0 or greater.
    - Numbers must not contain any fractional component.
    
    
# Count outcomes   
- In general, we obtain count variables by counting discrete events over space and time. Many social processes produce counts:
    - How many people live in a census tract? 
    - How many siblings does someone have?
    - How many times has someone been arrested?
    
# Count outcome
## Modeling counts using OLS
- We could treat counts like continuous variables and model them using OLS.
- Such a strategy might be appropriate if a count variable is normally distributed.
    - This could occur if a continuous variable was rounded.
- But like the LPM, we might run into problems when making predictions:

# Data
## Twitter and political parties in Europe
```{r load-data, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
data <- read_csv("data/twitter_parties_2016.csv")#
data <- data %>%
    replace_na(list(left_right = 5,
                    seats_per = 0))
```

<!-- Removed table, this causes a crash when trying to knit, somethign about iteration numbers
# Data
## Twitter and political parties in Europe
```{r descriptives, echo = FALSE, mysize=TRUE, size='\\footnotesize', eval = FALSE}
datasummary_skim(data = data %>% dplyr::select(tweet_count, retweet_total, populist, left_right, seats_per),
                 type = "numeric",
                 fmt = 2, # Show 2 decimal places 
                 histogram = F,
                 title = "Descriptive statistics",
                 output = "huxtable")
```

I also tried to drop unique and missing columns. I tried to follow instructions https://vincentarelbundock.github.io/modelsummary/articles/datasummary.html but get error when I try to specify a formua like in the examples
-->

# Data
## Twitter and political parties in Europe
```{r hist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
#ggplot(aes(x = tweet_count), data = data) + geom_hist(alpha = 0.5) + theme_minimal()
hist(data$tweet_count, breaks = 50)
```

# Count outcome
## Modeling counts using OLS
```{r ols, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ols.s <- lm(tweet_count ~ populist + left_right + seats_per,
         data = data)
ols.fe <- lm(tweet_count ~ populist + left_right + seats_per + country,
         data = data)
ols.fe.log <- lm(log(tweet_count) ~ populist + left_right + seats_per + country,
         data = data)
```

```{r table1, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("OLS" = ols.s, "OLS FE" = ols.fe, "OLS FE (Log)" = ols.fe.log), stars = TRUE, gof_omit = "AIC|BIC|RMSE", output = "latex", coef_omit = "country*", note = "Country FE omitted.")
```

# Count outcome
## Making predictions with OLS
```{r hist-ols, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(predict(ols.fe, data), breaks = 50)
```


# Count outcome
## Making predictions with OLS
```{r hist-log, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(exp(predict(ols.fe.log, data)), breaks = 50)
```

# Poisson regression
## Modeling counts as Poisson processes
- The \textbf{Poisson} distribution is a discrete probability distribution that indicates the number of events in a fixed time or space. These counts can be considered as rates of events per unit.\footnote{The distribution gets its name from French mathematician SimÃ©on Denis Poisson [1781-1840])}.
- The *probability mass function* is defined by a single parameter $\lambda$, where the probability of observing $k$ events is equal to

$$P(x=k) = \frac{\lambda^ke^-\lambda}{k!}$$

- For any Poisson distributed random variable, $x$

$$E(x) = \lambda = Var(x)$$


# Poisson regression
## Modeling counts as Poisson processes
- Let's say the average number of visits to the dentist in a single year is 1.6.
- We can model the probabilities of observing different numbers of visits given $\lambda = 1.6$:

$$P(k \text{ visits a year}) = \frac{1.6^k e^{-1.6}}{k!}$$

$$P(0 \text{ visits a year}) = \frac{1.6^0 e^{-1.6}}{0!} = \frac{e^{1.6}}{1} \approx 0.2$$

$$P(1 \text{ visits a year}) = \frac{1.6^1 e^{-1.6}}{1!} = \frac{1.6e^{-1.6}}{1} \approx 0.4$$
<!--$$P(2 \text{ visits a year}) = \frac{1.6^2 e^{-1.6}}{2!} = \frac{2.56 e^{-1.6}}{1} \approx 0.52$$-->

# Poisson regression
## Poisson distributions
```{r poisson-dist, echo = FALSE, mysize=TRUE, size='\\footnotesize' }
x <- rpois(1000, 1.6)
X <- as.data.frame(x)
colnames(X) <- c("x")
ggplot(aes(x = x), data = X) + geom_histogram(bins = max(x), fill = "lightblue") + theme_minimal() + labs(x = "Visits", y = "Count", caption = TeX("1000 draws from Poisson($\\lambda = 1.6$)"))  + scale_x_continuous(breaks=0:max(x))
```

# Poisson regression
## Poisson distributions, $E[x] = \lambda = Var(x)$
```{r poisson-dist-stats, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
round(mean(x),2)
round(var(x),2)
```

# Poisson regression
- The Poisson regression model assumes that the outcome is Poisson distributed, conditional on the observed predictions. 

$$y \sim Poisson(\lambda)$$

- To ensure that our estimates are positive, we can use a logarithmic *link function*, thus

$$y = log(\lambda) = \beta_0 + \beta_1x_{1} + \beta_2x_{1} + ... + \beta_kx_{k}$$

- Like logistic regression, this equation can equivalently be expressed using the *inverse* of the logarithm function:

$$\lambda = e^{\beta_0 + \beta_1x_{1} + \beta_2x_{1} + ... + \beta_kx_{k}}$$

# Poisson regression
## Fitting a model
```{r poisson-reg, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pois <- glm(tweet_count ~ populist + left_right +
                seats_per + country,
         data = data, family = poisson(link = "log"))
```

# Poisson regression
```{r poisson-table, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("OLS FE (Log)" = ols.fe.log, "Poisson" = pois), stars = TRUE, gof_omit = "AIC|BIC|RMSE|F", output = "latex", coef_omit = "country*", note = "Country FE omitted.")
```

# Poisson regression
## Interpretation
- The intercept $\beta_0$ is the *logged* average value of the outcome when all other predictors are equal to zero.
- Each coefficient $\beta_i$ indicates the effect of a unit change of $x_i$ on the *logarithm* of the outcome.
    - e.g., $\beta_{populism} = 0.354$ implies that the expected log number of tweets for populist parties is higher than non-populists by 0.354.
- Coefficients can be interpreted as *multiplicative* changes after exponentiation
    - e.g., $e^{\beta_{populism}} = e^{0.354} \approx 1.425$. This implies that populist parties tweet 1.425 times as frequently or 42.5% more frequently than non-populists.
    - These coefficients are sometimes referred to as textbf{incident rate ratios (IRRs)}.
    

# Poisson regression
```{r poisson-table-ex, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson" = pois), stars = TRUE, gof_omit = "AIC|BIC|RMSE", output = "latex", coef_omit = "country*", note = "Country FE omitted.", exponentiate = T)
```

# Poisson regression
```{r poisson-preds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
resids.p <- residuals(pois, data, type = "response")
preds.p <- predict(pois, data, type = "response")
observed <- data$tweet_count
populist <- data$populist
temp <- as.data.frame(cbind(resids.p, preds.p, observed, populist))
ggplot(aes(x = observed, y = preds.p, color =as.factor(populist)), data = temp) + geom_point(alpha = 0.5) + theme_minimal() +  scale_color_viridis_d() + labs(color = "populist", y = "Predicted tweets", x = "Observed tweets")
```


# Poisson regression
```{r poisson-preds2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(aes(x = observed, y = preds.p, color =as.factor(populist)), data = temp) + geom_point(alpha = 0.5) + scale_y_log10() + scale_x_log10() + theme_minimal() +  scale_color_viridis_d() + labs(color = "populist", y = "Predicted tweets", x = "Observed tweets")
```


# Poisson regression
```{r poisson-resids, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(aes(x = observed, y = resids.p, color =as.factor(populist)), data = temp) + geom_point(alpha = 0.5) + scale_x_log10() + theme_minimal() +  scale_color_viridis_d() + labs(color = "populist", y = "Residual", x = "Observed tweets")
```


# Poisson regression
```{r poisson-density, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
temp2 <- melt(temp %>% dplyr::select(observed, preds.p))
ggplot(aes(x = value, group = variable, fill = variable), data = temp2) + geom_density(alpha = 0.5) + theme_minimal() +  scale_fill_viridis_d(option = "cividis") + labs(color = "Variable", y = "Density", x = "Tweet count")
```


# Overdispersion
- A random variable is \textbf{overdispersed} if the observed variability is greater than the variability expected by the underlying probability model.
- In this case, we can see that the variance is far larger than the mean.
    - We could see this in the descriptive statistics, but the issue can only be properly diagnosed after fitting a model (note that the variance of the data is more than two times as large as the predicted values)
- \textbf{Underdispersion} occurs if the variability is lower than expected, but it is rarely an issue.

# Overdispersion
```{r poisson-negbin-dist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(MASS)
X$x2 <- rnegbin(1000, mu = 1.6, theta=1)
X.tmp <- melt(X)
levels(X.tmp$variable) <- c("Poisson", "Overdispersed")
ggplot(aes(x = value, group = variable, fill = variable), data = X.tmp) + geom_histogram(bins = max(X$x2), alpha = 0.8) + theme_minimal() + labs(x = "Visits", y = "Count", caption = TeX("Poisson($\\lambda = 1.6$) and NegBin($\\mu = 1.6$, $\\theta = 1$)"), fill = "DGP")  + scale_x_continuous(breaks=0:max(X$x2))
```


# Overdispersion
## Negative binomial distribution and regression
- The \textbf{negative binomial} distribution (aka the gamma-Poisson distribution) includes an additional parameter $\theta$ to account for dispersion, referred to as a \textbf{scale parameter}.

$$y = NegativeBinomial(\lambda, \theta)$$

- In negative binomial regression, $\theta$ is estimated from the data. The value must be positive.
    - Lower values indicate greater overdispersion.
    - Negative binomial becomes Poisson as $\lim_{\theta \rightarrow \infty}$.

# Overdispersion
## Fitting a negative binomial regression
The procedure for estimating a negative binomial regression via Maximum Likelihood is not implemented in `glm`. Instead, we use the modified `glm.nb` function from the `MASS` package.
```{r nb-reg, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(MASS)
nb <- glm.nb(tweet_count ~ populist + left_right +
                 seats_per + country,
         data = data)
```

# Comparing Poisson and negative binomial regression
```{r negbin-table, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson" = pois, "Negative binomial" = nb), stars = TRUE, gof_omit = "AIC|BIC|RMSE", output = "latex", coef_omit = "country*", note = "Country FE omitted. Exponentiated coefficients.", exponentiate = T)
```

# Negative binomial regression
```{r negbin-theta, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
nb$theta
nb$SE.theta
```


# Negative binomial regression
```{r negbin-preds2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
temp$preds.nb <- predict(nb, data, type = "response")
temp$resids.nb <- residuals(nb, data, type = "response")
ggplot(aes(x = observed, y = resids.nb, color =as.factor(populist)), data = temp) + geom_point(alpha = 0.5) + scale_x_log10() + theme_minimal() +  scale_color_viridis_d() + labs(color = "populist", y = "Residuals", x = "Observed tweets")
```


# Negative binomial regression
```{r negbin-density, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(reshape2)
temp2 <- melt(temp %>% dplyr::select(preds.p, preds.nb))
ggplot(aes(x = value, group = variable, fill = variable), data = temp2) + geom_density(alpha = 0.5) + theme_minimal() + labs(color = "Variable", y = "Density", x = "Tweet count")
```


# Negative binomial regression
## Bayesian estimation
```{r nb-bayes, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pois.b <- stan_glm(tweet_count ~ populist + left_right +
                       seats_per + country,
        data = data,
        family = poisson,
       seed = 08901, chains = 1, refresh = 0)

nb.b <- stan_glm(tweet_count ~ populist + left_right +
                     seats_per + country,
        data = data,
        family = neg_binomial_2(),
       seed = 08901, chains = 1, refresh = 0)
```


# Poisson posterior predictive check
```{r poisson-ppc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(bayesplot)
ppc_dens_overlay(y = data$tweet_count,
                 yrep = posterior_predict(pois.b, draws = 100)) + xlim(0,50000)
```


# Negative binomial posterior predictive check
```{r nb-ppc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ppc_dens_overlay(y = data$tweet_count,
                 yrep = posterior_predict(nb.b, draws = 100)) + xlim(0, 50000)
```


# Poisson PSIS plot
```{r loo1, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
l.pois <- loo(pois.b)
plot(l.pois)
```


# Negative binomial PSIS plot
```{r loo2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
l.nb <- loo(nb.b)
plot(l.nb)
```

# Negative binomial regression
## Comparing Poisson and negative binomial models
```{r loo-compare, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
loo_compare(l.pois, l.nb)
```


# Negative binomial regression
## Bayesian estimate of $\theta$
```{r plot-theta, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(tidybayes)
nb.b %>% gather_draws(reciprocal_dispersion) %>% 
    ggplot(aes(x = .value)) +
  stat_halfeye() + theme_tidybayes() + labs(y = "Posterior density", x = TeX("$\\theta$"), title = 
                                                TeX("Posterior dist. of $\\theta$"))
```


# Offsets
## Intuition
- Assume a count outcome $y$ is measured over varying time intervals $t$. The level of $y$ will vary both as a function of the underlying count process and the length of \textbf{exposure}.\footnote{\tiny The same logic would apply if we measured quantities over varying spatial units, e.g. counting people in blocks versus census tracts.}
- We can add an \textbf{offset} to our model to account for varying exposures.
- The outcome of a model with an offset is now $\frac{y}{t}$.


# Offsets
## Explanation
- The mean of a Poisson process, $\lambda$ is implicitly $\lambda = \frac{\mu}{\tau}$, the expected number of events, $\mu$, over the duration $\tau$.
- Assume a Poisson process where $\lambda_i$ is the expected number of events for the $i^{th}$ observation. We can write the link function as 

$$y = Poisson(\lambda)$$
$$log(\lambda) = log(\frac{\mu}{\tau}) = \beta_0 + \beta_1x$$

- This can be re-written as

$$ = log(\mu) -log(\tau) = \beta_0 + \beta_1x$$


# Offsets
## Explanation
- We can think of $\tau$ as the number of \textbf{exposures} for each observation. Thus, we can write out a new model for $\mu$:

$$y \sim Poisson(\mu)$$

$$log(\mu) = log(\tau) + \beta_0 + \beta_1x$$


# Offsets
## Simulated example
```{r offset-example, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
tweets <- sample(c(1:100), N, replace = TRUE)
ideology <- rbinom(N,1,0.4)
likes <- c()
for (i in 1:N) {
    y <- sum(rpois(tweets[i], exp(1 + 1*ideology + rnorm(1))))
    likes[i] <- y
}
sims <- as_tibble(cbind(tweets, likes, ideology))
```

# Offsets
## Simulated example
```{r offset-example-head, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
head(sims)
```

# Offsets
## Specification and interpretation
- The model is specified by adding the logarithm of exposures (e.g. $log(\tau)$) as an \textbf{offset} using the `offset` function.
    - The coefficient for the logarithm of exposures is fixed to $\beta_{offset} = 1$.
- The model is now interpreted as predicting a *rate* rather than a count.
- We could also directly include the logarithm of exposures as a predictor and let the model determine the coefficient.


# Offsets
## Simulated example
```{r offset-models, echo = TRUE,   mysize=TRUE, size='\\footnotesize'}
m1 <- glm(likes ~ 1 + ideology, 
          data = sims, family = poisson(link = "log"))
m2 <- glm(likes ~ ideology + log(tweets), 
          data = sims, family = poisson(link = "log"))
m3 <- glm(likes ~ ideology + offset(log(tweets)), 
          data = sims, family = poisson(link = "log"))
```

# Offsets
```{r offset-table, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson" = m1, "Poisson (Log exposure)" = m2, "Poisson (Offset)" = m3), stars = TRUE, gof_omit = "AIC|BIC|RMSE", output = "latex", exponentiate = F)
```

# Offsets
```{r offset-model-exp, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson" = m1, "Poisson (Log exposure)" = m2, "Poisson (Offset)" = m3), stars = TRUE, gof_omit = "AIC|BIC|RMSE", output = "latex", exponentiate = T, note = "Exponentiated coefficients.")
```

# Offsets
## Example: Predicting retweet rates
- Three models of yearly retweets
    - No offset
    - Log(tweets) included as predictor
    - Log(tweets) included as offset
    

# Offsets
## Example: Predicting retweet rates
```{r offset-model, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
nb.rt <- glm.nb(retweet_total ~ 
                    populist + left_right + seats_per + country,
                    data = data)
nb.rt.e <- glm.nb(retweet_total ~ log(tweet_count) + 
                      populist + left_right + seats_per + country,
                      data = data)
nb.rt.o <- glm.nb(retweet_total ~ offset(log(tweet_count)) +
                      populist + left_right + seats_per + country,
                      data = data)
```

# Offsets
```{r retweet, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("NB" = nb.rt, "NB (Log exposure)" = nb.rt.e, "NB (Offset)" = nb.rt.o), stars = TRUE, gof_omit = "AIC|BIC|RMSE", output = "latex", exponentiate = F, coef_omit = c("country*"))
```

# Offsets
## Using offsets
- Include an offset if there are differences in measurement intervals across observations.
- Offsets allow models to be interpreted as rates rather than counts.
- The logarithm of exposures can also be directly modeled, but interpretation is less intuitive.

# Zero-inflated models
## Intuition
- Some count outcomes have high rates of zeros. What if the outcomes with a value of zero are generated by a different kind of process?
- \textbf{Zero-inflated models} allow us to separately model the process determining whether counts are non-zero and the expected count for each observations.

# Zero-inflated models
## Specification
- The zero-inflated Poisson model consists of a mixture of two linear models, a logistic regression predicting the probability of a zero and a Poisson model predicting the count outcome.

$$y_i = ZIPoisson(p, \lambda)$$
$$logit(p) = \beta_{0p} + \beta_{1p}x$$

$$log(\lambda)  = \beta_{0\lambda} + \beta_{1\lambda}x$$

- Each model has its own parameters. These can be specified to model each process.

# Zero-inflated models
## Example: Books borrowed from the library
```{r library, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 100
prob_lib <- 0.6
lib <- rbinom(N, 1, prob_lib)
sum(lib)/N
```

# Zero-inflated models
## Example: Books borrowed from the library
```{r library-books, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
x <- rnorm(N)

books <- c()
for (i in 1:N) {
    if (lib[i] == 1) {
        b <- rpois(1, lambda = exp(1 + 0.3*x[i] + rnorm(1)))
        books[i] <- b
    }
    else {books[i] <- 0}
}
mean(books)
max(books)
```

# Zero-inflated models
## Two kinds of zeros
```{r books-zeros, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
sum(books == 0)
sum(books == 0 & lib == 1)
sum(books == 0 & lib == 0)
```

# Zero-inflated models
```{r books-hist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(books, breaks = max(books))
```

# Zero-inflated models
## Estimating a Poisson model
```{r library-books-nb, echo = TRUE,   mysize=TRUE, size='\\footnotesize'}
book.data <- as.data.frame(cbind(books, x))
pois.m <- stan_glm(books ~ x, data = book.data, family = poisson(),
       seed = 08901, chains = 1, refresh = 0)
```

# Zero-inflated models
```{r library-books-nb-preds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ppc_dens_overlay(y = books,
                 yrep = posterior_predict(pois.m, draws = 50)) + xlim(0, 15) +
    labs(caption = 'Truncated to focus on lower predicted values')
```

# Zero-inflated models
```{r library-books-nb-preds-z, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
pred.books <- posterior_predict(pois.m)
ppc_stat(y=books, pred.books, stat=function(y) mean(y==0)) + labs(title = "Predicted number of zeros")
```

# Zero-inflated models
## Estimating a zero-inflated Poisson model
We must use the `brms` library to implement Bayesian zero-inflated Poisson regression.
```{r zinb, echo = TRUE,  mysize=TRUE, size='\\footnotesize'}
library(brms)
zip <- brm(books ~ x,
                  data = book.data,
                  family = zero_inflated_poisson(link = "log",
                                                link_zi = "logit"),
                  seed = 08901, refresh = 0, chains = 1)
```

# Zero-inflated models
```{r zip-preds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
pp_check(zip, ndraws = 50) + xlim(0, 20) +
    labs(caption = 'Truncated to focus on lower predicted values')
```

# Zero-inflated models
```{r zip-preds-z, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
pred.books <- posterior_predict(zip)
ppc_stat(y=books, pred.books, stat=function(y) mean(y==0)) + labs(title = "Predicted number of zeros")
```

# Zero-inflated models
## Comparing standard and zero-inflated models
```{r zip-loo, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
loo_compare(loo(zip), loo(pois.m))
```

# Summary
- Standard linear models are generally unsuitable for count data
- Poisson regression can be used for most count outcomes
- Overdispersion occurs when variation higher than expected under Poisson model
    - Negative binomial regression includes a scale parameter
- Zero-inflated models are used to decompose processes generating zeros and counts

# Next week
- Categorical outcomes
    - Multinomial and ordered logistic regression