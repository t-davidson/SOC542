---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Interactions"
author: Thomas Davidson
institute: Rutgers University
date: March 3, 2025
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

set.seed(08901)

library(ggplot2)
library(tidyverse)
library(latex2exp)
library(modelsummary)
library(rstanarm)
library(gridExtra)
library(tidybayes)

options("modelsummary_format_numeric_latex" = "plain")
#options(modelsummary_format_numeric_latex = "mathmode")
```

# Plan
- Introducing interactions
- Types of interactions and their interpretations
- Marginal effects

# Updates
- Grading Homework 2
- Project proposals due Friday (3/7) at 5pm
    - See last week's slides for details
    - Recommend meeting to discuss plan
    - Submit via email as PDF

# Introducing interactions
## What is an statistical interaction?
- Consider the following population model:

$$y = \beta_0 + \beta_1 x + \beta_2 z + u$$


- The coefficients $\beta_1$ and $\beta_2$ measure the relationship between $x$ and $y$ and $z$ and $y$, respectively.
    - The interpretation of either coefficient requires that we hold the other constant.
- *But what if we expect the effect of $x$ to vary as a function of $z$?*

    
# Introducing interactions
## What is an statistical interaction?
- If we expect there to be an \textbf{interaction} between $x$ and $z$, such that the effect of $x$ on $y$ varies according to the level of $z$, we can add an \textbf{interaction term} into our model formula.

$$y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 x \cdot z + u$$

- $\beta_1$ and $\beta_2$ are now considered as the \textbf{main effects}. 
- $\beta_3$ is the coefficient for the interaction term, representing the effect of $x$ *times* $z$.
  
# Introducing interactions
## A simple population model
```{r simple-int, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000
x <- rnorm(N)
z <- rnorm(N)
y <- 3*x + 2*z + -5*(x*z) + rnorm(N, 10)
```

# Introducing interactions
## Comparing models
```{r simple-model, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m.r <- lm(y ~ x + z)
m <- lm(y ~ x + z + x:z)
modelsummary(list(m.r, m), gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Introducing interactions
## Why use interactions?
- We can use interaction terms as a way to encode theoretical knowledge about the relationship between variables, which is often important for answering theoretical questions.
- For example, if we expect there to be differences in income related to intersectional inequalities involving sex and race, we can add an interaction term to a model:

$$Income = \beta_0 + \beta_1Sex + \beta_2Race + \beta_3Sex \cdot Race + u$$

# Introducing interactions
## Why use interactions?
- Block et al. 2023 make the case that interactional frameworks are a necessary condition for making claims about intersectionality:
    - If $\beta_3=0$ $\rightarrow$  "no interaction effect, no intersectionality" (p.801)
- If $\beta_3\neq0$ then there are intersectional differences
- These differences are symmetric:
    - The effect of sex depends on race
    - The effect of race depends on sex

<!-- Removing this as it can be confusing and is an aside here
# Introducing interactions
## Main effects and interactions
- In general, it is recommended to *include the main effects in any model with interactions*.
    - Type II errors are more likely when interpreting interaction terms with main effects omitted.
    - The interpretation of the model can change substantially if main effects are excluded.\footnote{\footnotesize See this Stata blog for further discussion: https://stats.oarc.ucla.edu/stata/faq/what-happens-if-you-omit-the-main-effect-in-a-regression-model-with-an-interaction/}
-->

# Types of interactions
## Dummy-dummy

$$y = \beta_0 + \beta_1 Male + \beta_2 Degree + \beta_3 Male \cdot Degree + u $$

# Types of interactions
## Dummy-dummy
```{r dd, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Loading data
gss <- haven::read_dta("../../2022/labs/lab-data/GSS2018.dta") %>%
    filter(age <= 89) %>% haven::zap_labels() %>%
    mutate(degree = ifelse(degree >= 3, 1, 0), # resp has BA or grad degree
           sex = ifelse(sex == 1, 1, 0),
           race = as.factor(race),
           realrinc = realrinc/1000)
gss$race <- recode_factor(gss$race, "1" = "White", "2" = "Black", "3" = "Other")

m00 <- lm(realrinc ~ sex, data = gss %>% drop_na(degree))
m0 <- lm(realrinc ~ degree, data = gss)
m <- lm(realrinc ~ sex + degree, data = gss)
m.i <- lm(realrinc ~ sex + degree + sex:degree, data = gss)
modelsummary(list(m00, m0, m, m.i),
             coef_rename = c("sex" = "Male", "degree" = "Degree"),
             gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Dummy-dummy

$$y = \beta_0 + \beta_1 Male + \beta_2 Degree + \beta_3 Male \cdot Degree + u $$

- Female and people without a college degree are the reference categories.
- $\beta_1$ and $\beta_2$ represent the main effects of sex and degree on the outcome, but they only tell a partial story unless $\beta_3=0$.
- The coefficient $\beta_3$ represents the expected difference in the effect of degree for men versus women.\footnote{\footnotesize Note the symmetrical interpretation here: the  difference in the effect of sex for college degree versus non-college degree. See McElreath 8.2 for further discussion.}
- If $\beta_3\neq0$, the expected income for a male with a degree is $\beta_0 + \beta_1 + \beta_2 + \beta_3$. The same quantity for a female with a degree is $\beta_0 + \beta_2$.

# Types of interactions
## Dummy-dummy: Evaluating intersectional claims
$$y = \beta_0 + \beta_1 Female + \beta_2 Black + \beta_3 Female \cdot Black + u$$
```{r int, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
gss_ <- gss %>% filter(race != "Other") %>% mutate(
    sex_ = ifelse(sex == 1, "Male", ifelse(
        sex == 0, "Female", NA
    )))

gss_$sex_ <- fct_relevel(gss_$sex_, "Male")
    
m <- lm(realrinc ~ sex_ + race, data = gss_)
m.i <- lm(realrinc ~ sex_+ race + sex_:race, data = gss_)
modelsummary(list(m, m.i),
             coef_rename = c("sex_Female" = "Female", "raceBlack" = "Black"),
             gof_omit="AIC|BIC|Log.Lik|RMSE", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Dummy-dummy: Evaluating intersectional claims
- Since $\beta_3\neq0$ we can reject the null hypothesis of no interesectionality
- But this means that the other coefficients do not tell us the "separate, unconditional, independent, or average effects of gender and race" (Block et al. 2023)

# Types of interactions
## Dummy-dummy: Evaluating intersectional claims
The effect of gender depends on race:

$$\frac{\Delta Income}{\Delta Female} = \beta_1 + \beta_3 \cdot Black$$

The effect of race depends on gender:
$$\frac{\Delta Income}{\Delta Black} = \beta_2 + \beta_3 \cdot Female$$

# Types of interactions
## Reformulating the model
- Block et al. 2023 show how we could specify an equivalent, alternative model:
- Assuming White Female is the reference category, we could write this as:

$$y = \beta_0 + \gamma_1 BlackFemale + \gamma_2 BlackMale + \gamma_3 WhiteFemale + u$$ 

# Comparing frameworks
```{r alt, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Create a new categorical variable for intersectional groups
gss_ <- gss_ %>% mutate(
    rs = as.factor(paste(race, sex_, sep = "_")))
gss_$rs <- fct_relevel(gss_$rs, "White_Male")

# Fit the regression model using the reformulated approach
m_block <- lm(realrinc ~ rs, data = gss_)
modelsummary(list(m.i, m_block), 
             coef_rename = c("sex_Female" = "Female", "raceBlack" = "Black",
                             "rsBlack_Female" = "Black Female",
                             "rsBlack_Male" = "Black Male",
                             "rsWhite_Female" = "White Female"),
             gof_omit="AIC|BIC|Log.Lik|RMSE", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```


# Reformulating interactions
```{r, out.width="80%",out.height="70%", fig.align="center"}
include_graphics('../../img/interpreting_interactions.png')
```

# Reformulating interactions
- Testing intersectional claims requires postestimation calculations to calculate different quantities:
    - Intersectional effect for gender and race
    - Effect of being female among White people
    - Effect of being female among Black people
    - Effect of being Black among men
    - Effect of being Black among women
    
# Types of interactions
## Continuous-dummy
$$y = \beta_0 + \beta_1 Age + \beta_2 Sex + \beta_3 Age \cdot Sex + u $$

# Types of interactions
## Continuous-dummy
```{r cd, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ age + sex, data = gss)
m.i <- lm(realrinc ~ age + sex + age:sex, data = gss)
modelsummary(list(m, m.i), 
             coef_rename = c("age" = "Age", "sex" = "Male"),
             gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Continuous-dummy
$$y = \beta_0 + \beta_1 Age + \beta_2 Sex + \beta_3 Age \cdot Sex + u $$


- The coefficients $\beta_1$ and $\beta_2$ represent the main effects of age and sex on income.
- For females, $\beta_1$ represents the relationship between age and income. For males, the relationship is $\beta_1$ + $\beta_3$.
    - Thus, the interaction term allows the *slope* to vary according to sex.

# Types of interactions
## Continuous-dummy
```{r plot slopes, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
# Extract coefficients from the interaction model
coefs <- coef(m.i)
b0 <- coefs[1]  # Intercept
b1 <- coefs[2]  # Age coefficient
b2 <- coefs[3]  # Sex coefficient (Male = 1, Female = 0)
b3 <- coefs[4]  # Interaction coefficient (Age * Sex)

# Create a range of ages for plotting
age_range <- seq(min(gss$age, na.rm = TRUE), max(gss$age, na.rm = TRUE), length.out = 100)

# Compute fitted values manually
predicted_f <- b0 + b1 * age_range 
predicted_m <- b0 + b1 * age_range + b2 + b3 * age_range

# Create a data frame for plotting regression lines
plot_data <- data.frame(
  age = rep(age_range, 2),
  income = c(predicted_f, predicted_m),
  sex = rep(c("Female", "Male"), each = length(age_range))
)

# Plot scatter points and regression lines
ggplot(gss, aes(x = age, y = realrinc)) +
  geom_point(alpha = 0.3, color = "gray50") +
  geom_line(data = plot_data, aes(x = age, y = income, color = sex), size = 1) +  
  scale_color_manual(values = c("Female" = "#CC2936", "Male" = "#388697")) +
  ylim(0,75) +
  labs(title = "Interaction between age and sex",
       x = "Age",
       y = "Real Income", 
       color = "Sex",
       caption = "Income truncated to 75k to emphasize trends.") +
  theme_minimal()

```


# Types of interactions
## Continuous-continuous
$$y = \beta_0 + \beta_1 Age + \beta_2 HoursWorked + \beta_3 Age \cdot HoursWorked + u $$

# Types of interactions
## Continuous-continuous
```{r cc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ age + hrs1, data = gss)
m.i <- lm(realrinc ~ age + hrs1 + age:hrs1, data = gss)
modelsummary(list(m, m.i), 
             coef_rename = c("age" = "Age", "hrs1" = "Hours Worked"),
             gof_omit = "Log|AIC|BIC|RMSE", stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Continuous-continuous
$$y = \beta_0 + \beta_1 Age + \beta_2 WorkHrs + \beta_3 Age \cdot WorkHrs + u $$

- The intercept no longer has a meaningful interpretation (income when age and work hours equal zero).
    - GHV 12.2 discuss standardization to make intercepts more interpretable in such contexts.
- $\beta_1$ and $\beta_2$ represent the main effects of age and work hours.
- The interaction term $\beta_3$ captures how the effect of work hours on income varies as a function of age.


# Types of interactions
## Continuous-continuous
- The effect of work hours on income is now also a function of age:

$$\frac{\Delta y}{\Delta_{WorkHrs}} = \beta_2 + \beta_3 Age$$

- Similarly,

$$\frac{\Delta y}{\Delta_{Age}} = \beta_1 + \beta_3 WorkHrs$$


# Types of interactions
## Continuous-continuous
- If Age changes by $\Delta$Age and WorkHrs by $\Delta$WorkHrs, the expected change in $y$ is:

$$\Delta y = (\beta_1 + \beta_3 WorkH) \Delta Age + (\beta_2 + \beta_3 Age) \Delta WorkH + \beta_3 \Delta Age \cdot \Delta WorkH$$

- The coefficient $\beta_3$ represents the effect of a unit increase in age *and* work hours, beyond the sum of the individual effects of unit increases alone.

# Types of interactions
## Dummy-categorical
```{r dc, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + race, data = gss)
m.i <- lm(realrinc ~ sex + race + sex:race, data = gss)
modelsummary(list(m, m.i),
             coef_rename = c("sex" = "Male", "raceBlack" = "Black", "raceOther" = "Other race"),
             gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Dummy-categorical
$$y = \beta_0 + \beta_1 Male + \beta_2 Black + \beta_3 Other + \beta_4 Black \cdot Male + \beta_5 Other \cdot Male + u$$

- There is a separate coefficient for the interaction between the dummy variable and each of the categories, with the exception of the reference group.
- The interpretation is the same as the dummy-dummy model.

# Types of interactions
## Continuous-categorical

$$y = \beta_0 + \beta_1 Age + \beta_2 Black + \beta_3 Other + \beta_4 Black \cdot Age + \beta_5 Other \cdot Age + u$$
```{r ct, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
gss$race <- as.factor(gss$race)
m <- lm(realrinc ~ age + race, data = gss)
m.i <- lm(realrinc ~ age + race + age:race, data = gss)
modelsummary(list(m, m.i),
             coef_rename = c("age" = "Age", "raceBlack" = "Black", "raceOther" = "Other race"),
             gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Categorical-categorical
```{r tt, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
# Ref = word of god, 2 = inspired word, 3 = ancient book
gss.b <- gss %>% filter(bible <= 3) %>% mutate(bible = as.factor(bible))
levels(gss.b$bible) <- c("Word of God", "Inspired Word", "Ancient Book")
m <- lm(realrinc ~ race + bible, data = gss.b)
m.i <- lm(realrinc ~ race + race:bible, data = gss.b)
modelsummary(list(m, m.i),
             coef_rename = c("raceWhite" = "White", "raceBlack" = "Black", "raceOther" = "Other race", "bibleInspired Word" = "Inspired Word", 
                             "bibleAncient Book" = "Ancient Book"),
             gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Three-way interactions
```{r three, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + race + degree, data = gss.b)
m.i <- lm(realrinc ~ sex + race + degree + sex:race:degree, data = gss)
modelsummary(list(m, m.i), gof_omit=".*", 
             coef_rename = c("sex" = "Male", "raceWhite" = "White", "raceBlack" = "Black", "raceOther" = "Other race", "degree" = "Degree"),
             statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Types of interactions
## Interpreting interactions
- Interactions terms make models more challenging to interpret.
    - Like polynomial regression, the effect of a single predictor is represented by more than one coefficient (e.g. $y = \beta_0 + \beta_1x + \beta_2z + \beta_3 x \cdot z + u$).
- Three-way and more complex interactions are even more difficult to interpret and should be avoided unless there are strong theoretical reasons to use them.

# Marginal effects
## Definitions
- A \textbf{marginal effect} is the relationship between change in single predictor and the dependent variable while *holding other variables constant*.
- The \textbf{average marginal effect (AME)} is the *average* change in the outcome as a function of a unit change in $x$.
    - Coefficients in a standard OLS model represent average marginal effects.
- This quantity becomes more complicated to calculate when interaction terms are included, since the effect of a change in $x$ now depends on multiple parameters.

# Marginal effects
## Computing marginal effects
- Frequentist marginal effects computed by calculating *partial derivatives* and variance approximations are used to construct confidence intervals.

$$ME(x_i) = \frac{\Delta y}{\Delta x_i}$$


- We can use the `margins` package in R to do this.\footnote{\footnotesize See  \href{https://cran.r-project.org/web/packages/margins/vignettes/TechnicalDetails.pdf}{documentation} for the margins package for further details.}
- Bayesian marginal effects can be calculated by sampling from the posterior distribution.

# Marginal effects
## Marginal effects and OLS regression
```{r margins-model1, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + race, data = gss)
modelsummary(list(m), gof_omit=".*",
             coef_rename = c("age" = "Age", "sex" = "Male", "raceBlack" = "Black", "raceOther" = "Other race"),
             stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex")
```

# Marginal effects
## Marginal effects using `margins`
Note how the average marginal effects are equal to the OLS coefficients.
```{r margins1, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(margins)
me <- margins(m)
summary(me)
```

# Marginal effects
## Marginal effects using `marginaleffects`
Note how the average marginal effects are equal to the OLS coefficients.
```{r margins1me, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(marginaleffects)
me <- avg_slopes(m)
print(me)
```

# Marginal effects
## Marginal effects with non-linear variables
```{r margins-model2, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
m2 <- lm(realrinc ~ sex + age + I(age^2) + race, data = gss)
modelsummary(list(m, m2), 
             coef_rename = c("age" = "Age", "I(age^2)" = "Age^2", "sex" = "Male", "raceBlack" = "Black", "raceOther" = "Other race"),
             gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex", estimate = "{estimate}{stars}")
```


# Marginal effects
## Marginal effects with non-linear variables
The margins commands are the same as above. Note how the AME now represents the total effect of age across the two parameters. There is no separate marginal effect for age squared.
```{r margins-nl, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
me <- avg_slopes(m2)
print(me)
```

# Marginal effects
## Marginal effects with non-linear variables
```{r margins-nl-plot, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
plot_predictions(m2, condition = "age") + 
    theme_classic()
```

# Marginal effects
## Marginal effects with interactions
```{r margins-model31, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
m <- lm(realrinc ~ sex + age + I(age^2) + race + sex:race + sex:age,
        data = gss)
```

```{r margins-model32, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list(m), 
             coef_rename = c("age" = "Age", "I(age^2)" = "Age^2", "sex" = "Male", "raceBlack" = "Black", "raceOther" = "Other race"),
             gof_omit=".*", statistic = NULL, stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), output = "latex", estimate = "{estimate}{stars}")
```

# Marginal effects
## Marginal effects with interactions
In this case, we can isolate the average marginal effect of each predictor.
```{r margins3, echo =FALSE, mysize=TRUE, size='\\footnotesize'}
me <- avg_slopes(m)
print(me)
```


# Marginal effects
## Conditional marginal effects (effect of sex by race)
```{r margins-line2, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
plot_slopes(m, variables = "sex", condition = "race") +
    theme_classic() + geom_hline(yintercept=0, linetype = "dashed")
```

# Marginal effects
## Computing marginal effects for Bayesian models
- Unlike frequentist marginal effects, there is no need for additional calculus.
- Marginal effects can be computed directly from the posterior distribution. 

# Marginal effects
```{r bayes1, echo =FALSE, mysize=TRUE, size='\\tiny'}
m.b <- stan_glm(realrinc ~ sex + age + I(age^2) + race + sex:race + sex:age, data = gss, family = "gaussian", chains =1 , refresh = 0)
modelsummary(list("OLS" = m, "Bayesian" = m.b), statistic = "conf.int", 
             coef_rename = c("age" = "Age", "I(age^2)" = "Age^2", "sex" = "Male", "raceBlack" = "Black", "raceOther" = "Other race"),
             gof_omit = "Log.Lik.|ELPD|LOO|WAIC|RMSE|AIC|BIC|F|R2|Num.Obs.",
             coef_omit = c(1),
             stars = F,
             output = "latex")
```


# Marginal effects
## Bayesian marginal effects
Fortunately for us, the `margins` and `marginaleffects` packages also work for Bayesian models.
```{r bayesmargins, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
avg_slopes(m.b)
```

# Marginal effects
## Bayesian marginal effects
To obtain the information used in these calculations, we can compute the *expected value* of the outcome at different levels of predictors using `epred_draws`. 
```{r bayes3, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
data.range <- expand_grid(sex = c(0,1),
                         race = c("Black", "White", "Other"),
                         age = 18:75)

tidy_epred <- m.b %>% epred_draws(newdata = data.range)
```

# Marginal effects
## Bayesian marginal effects
Like everything else we obtain from a Bayesian model, these marginal effects have a posterior distribution.
```{r bayes.head, echo =TRUE, mysize=TRUE, size='\\footnotesize'}
tail(tidy_epred %>% select(sex, race, age, .epred))
```


# Marginal effects
## Bayesian marginal effects
```{r bayes4, echo =FALSE, size='\\footnotesize'}
tidy_epred$sex <- ifelse(tidy_epred$sex == 1, "Male", "Female")

ggplot(tidy_epred, aes(x = age, group =sex, color = sex, y = .epred)) +
  stat_lineribbon() +
  scale_fill_brewer() +
    facet_grid(~ race) +
    theme_tidybayes() + theme(legend.position="right", base_size = 9.5) +
    labs(y = "Predicted income (thousands)",
         x = "Age",
         color = "Sex",
         title = NULL)
```


# Marginal effects
## Marginal effects and generalized linear models
- For generalized linear models (GLMs)---which will be our main focus after spring break---coefficients often do not have clear interpretations on the outcome scale so marginal effects even more important for interpretation.

# Next week
## Topic
- Missing data and imputation
- Model specification and robustness

# Lab
- Specifying and interpreting interaction terms  