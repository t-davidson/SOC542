---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Count outcomes"
author: Thomas Davidson
institute: Rutgers University
date: April 7, 2025
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: FALSE
      incremental: FALSE
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")
library(ggplot2)
library(tidyverse)
library(latex2exp)
library(kableExtra)
library(modelsummary)
library(viridis)
library(reshape2)
library(rstanarm)
library(scico)
# library(huxtable) # Causes problems knitting, including ! LaTeX Error: Environment centerbox undefined.

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

kable <- function(data) {
  knitr::kable(data, digits = 3) %>% 
    kable_styling(position = "center")
}

set.seed(08901)
options("modelsummary_format_numeric_latex" = "plain")
```


# Course updates
## Homework
- Homework 3 grades released
- Homework 4 released after class, due next Friday, 4/18
    - Count outcomes
    - Categorical and ordered outcomes (next week)

# Course updates
## Projects
- Preliminary results due 4/25
    - One  or more tables or figures of descriptive statistics
    - One or more regression tables showing
        - Bivariate results
        - Multivariate results
    - Must include at least one figure showing estimates (e.g. coefficients, predictions, marginal effects)
    - Draft write up of methodology and results
- Presentations on 5/5

# Plan
- Count outcomes
- Poisson regression
- Overdispersion and negative-binomial regression
- Offsets
- Zero-inflated models

# Count outcomes
- Count outcomes are variables defined as *non-negative integers*.
    - Values must be 0 or greater.
    - Numbers must not contain any fractional component.
    
    
# Count outcomes   
- In general, we obtain count variables by counting discrete events over space and time. Many social processes produce counts:
    - How many people currently live in a census tract? 
    - How many siblings does someone currently have?
    - How many times has someone ever been arrested?
    - How many sexual partners reported in one year?
    
# Count outcomes
## Modeling counts using OLS
- We could treat counts like continuous variables and model them using OLS.
- Such a strategy might be appropriate if a count variable is normally distributed.
- But like the LPM, we might run into problems when making predictions.
    - Predictions not constrained to be positive or counts.

# Data
## Twitter and political parties in Europe
- Data from Twitter accounts of 190 political parties in 28 countries in Europe
- Includes cumulative number of tweets and engagements (likes, replies, retweets) from 2018
- Data on left-right ideology (0-10 scale) and % of parliamentary seats held
```{r load-data, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
data <- read_csv("../../2022/slides/data/twitter_parties_2016.csv")#
data <- data %>%
    replace_na(list(left_right = 5,
                    seats_per = 0)) %>%
    filter(tweet_count > 0 & seats_per > 0)
```

<!-- Removed table, this causes a crash when trying to knit, somethign about iteration numbers
# Data
## Twitter and political parties in Europe
```{r descriptives, echo = FALSE, mysize=TRUE, size='\\footnotesize', eval = FALSE}
datasummary_skim(data = data %>% dplyr::select(tweet_count, retweet_total, populist, left_right, seats_per),
                 type = "numeric",
                 fmt = 2, # Show 2 decimal places 
                 histogram = F,
                 title = "Descriptive statistics",
                 output = "huxtable")
```

I also tried to drop unique and missing columns. I tried to follow instructions https://vincentarelbundock.github.io/modelsummary/articles/datasummary.html but get error when I try to specify a formua like in the examples
-->

# Data
## Twitter and political parties in Europe
```{r hist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(aes(x = tweet_count), data = data) +
    geom_histogram(bins = 50) +
    theme_classic() + labs(y = "Number of parties", x = "Tweet count")
```

# Count outcome
## Modeling counts using OLS
```{r ols, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ols.s <- lm(tweet_count ~ seats_per + left_right,
         data = data)
ols.fe <- lm(tweet_count ~ seats_per + left_right + country,
         data = data)
ols.fe.log <- lm(log(tweet_count) ~ seats_per + left_right +  country,
         data = data)
```

```{r table1, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("OLS" = ols.s, "OLS FE" = ols.fe, "OLS FE (Log)" = ols.fe.log), stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), gof_omit = "AIC|BIC|RMSE|Log.Lik.", output = "latex", coef_omit = "country*", note = "Country FE omitted.", coef_map = c("seats_per" = "Seats %","left_right" = "Ideology [0-10]", "(Intercept)" = "Intercept"))
```

# Count outcome
## Making predictions with OLS
```{r hist-ols, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(predict(ols.fe, data), breaks = 50)
```


# Count outcome
## Making predictions with OLS
```{r hist-log, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(exp(predict(ols.fe.log, data)), breaks = 50)
```

# Count outcomes
## Analyzing predictions
```{r min-pred, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
min(predict(ols.fe, data))
min(exp(predict(ols.fe.log, data)))
```

# Poisson regression
## Modeling counts as Poisson processes
- The \textbf{Poisson} distribution is a discrete probability distribution that indicates the count of events in a fixed interval. These counts can be considered as rates of events per unit.\footnote{The distribution gets its name from French mathematician SimÃ©on Denis Poisson [1781-1840].}
- The *probability mass function* is defined by a single parameter $\lambda$, where the probability of observing $k$ events is equal to

$$P(x=k) = \frac{\lambda^ke^-\lambda}{k!}$$

- For any Poisson distributed random variable, $x$

$$E(x) = \lambda = Var(x)$$


# Poisson regression
## Modeling counts as Poisson processes
- Let's say the average number of visits to doctor each year is 1.6.
- We can model the probabilities of observing different numbers of visits given $\lambda = 1.6$:

$$P(k \text{ visits a year}) = \frac{1.6^k e^{-1.6}}{k!}$$

$$P(0 \text{ visits a year}) = \frac{1.6^0 e^{-1.6}}{0!} = \frac{e^{1.6}}{1} \approx 0.2$$

$$P(1 \text{ visits a year}) = \frac{1.6^1 e^{-1.6}}{1!} = \frac{1.6e^{-1.6}}{1} \approx 0.4$$
<!--$$P(2 \text{ visits a year}) = \frac{1.6^2 e^{-1.6}}{2!} = \frac{2.56 e^{-1.6}}{1} \approx 0.52$$-->

# Poisson regression
## Poisson distributions
```{r poisson-dist, echo = FALSE, mysize=TRUE, size='\\footnotesize' }
x <- rpois(1000, 1.6)
X <- as.data.frame(x)
colnames(X) <- c("x")
ggplot(aes(x = x), data = X) + geom_histogram(bins = max(x), fill = "lightblue") + theme_classic() + labs(x = "Visits", y = "Count", caption = TeX("1000 draws from Poisson($\\lambda = 1.6$)"))  + scale_x_continuous(breaks=0:max(x))
```

# Poisson regression
## Poisson distributions, $E[x] = \lambda = Var(x)$
```{r poisson-dist-stats, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
round(mean(x),2)
round(var(x),2)
```

# Poisson regression
- The Poisson regression model assumes that the outcome is Poisson distributed, conditional on the observed predictors. 

$$y \sim Poisson(\lambda)$$

- To ensure that our estimates are positive, we can use a logarithmic *link function*, thus

$$y = log(\lambda) = \beta_0 + \beta_1x_{1} + \beta_2x_{1} + ... + \beta_kx_{k}$$

- Like logistic regression, this equation can equivalently be expressed using the *inverse* of the logarithm function:

$$\lambda = e^{\beta_0 + \beta_1x_{1} + \beta_2x_{1} + ... + \beta_kx_{k}}$$

# Poisson regression
## Fitting a model
```{r poisson-reg, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pois <- glm(tweet_count ~ seats_per + left_right +
                 country,
         data = data, family = poisson(link = "log"))
```

# Poisson regression
```{r poisson-table, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("OLS FE (Log)" = ols.fe.log, "Poisson" = pois), stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), gof_omit = "AIC|BIC|RMSE|F", output = "latex", coef_omit = "country*", note = "Country FE omitted.", coef_map = c("seats_per" = "Seats %","left_right" = "Ideology [0-10]", "(Intercept)" = "Intercept"))
```

# Poisson regression
## Interpretation
- The intercept $\beta_0$ is the *logged* average value of the outcome when all other predictors are equal to zero.
- Each coefficient $\beta_i$ indicates the effect of a unit change of $x_i$ on the *logarithm* of the outcome.
    - e.g., $\beta_{seats\%} = 0.013$ implies that the expected log number of tweets increases by 0.02 in response to a 1-unit, or 1% increase in parliamentary seats held by a party.
- Coefficients can be interpreted as *multiplicative* changes after exponentiation
    - e.g., $e^{\beta_{seats\%}} = e^{0.013} \approx 1.013$. This implies that a ~1.3% increase in tweets.
    - These coefficients are sometimes referred to as \textbf{incident rate ratios (IRRs)}.

# Poisson regression
```{r poisson-table-ex, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson (Exponentiated)" = pois), stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), gof_omit = "AIC|BIC|RMSE", output = "latex", coef_omit = "country*", note = "Country FE omitted.", exponentiate = T, coef_map = c("seats_per" = "Seats %","left_right" = "Ideology [0-10]", "(Intercept)" = "Intercept"))
```

# Poisson regression
```{r poisson-preds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
resids.p <- residuals(pois,type = "response")
preds.p <- predict(pois,type = "response")
observed <- pois$data$tweet_count
left_right <- pois$data$left_right
temp <- as.data.frame(cbind(resids.p, preds.p, observed, left_right))
ggplot(aes(x = observed, y = preds.p, color =left_right), data = temp) + geom_point() + theme_classic() +  scale_color_scico(palette = "vik") + labs(color = "Ideology", y = "Predicted tweets", x = "Observed tweets")
```


# Poisson regression
```{r poisson-preds2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(aes(x = observed, y = preds.p, color =left_right), data = temp) + geom_point() + scale_y_log10() + scale_x_log10() + theme_classic() +  scale_color_scico(palette = "vik") + labs(color = "Ideology", y = "Predicted tweets", x = "Observed tweets")
```


# Poisson regression
```{r poisson-resids, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(aes(x = observed, y = resids.p, color =left_right), data = temp) + geom_point() + scale_x_log10() + theme_classic() + scale_color_scico(palette = "vik") + labs(color = "Ideology", y = "Residual", x = "Observed tweets")
```


# Poisson regression
```{r poisson-density, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
temp2 <- melt(temp %>% dplyr::select(observed, preds.p))
ggplot(aes(x = value, group = variable, fill = variable), data = temp2) + geom_histogram(alpha = 0.8, bins = 10) + theme_classic() +  scale_fill_viridis_d(option = "cividis") + labs(color = "Variable", y = "Density", x = "Tweet count")
```


# Overdispersion
- A random variable is \textbf{overdispersed} if the observed variability is greater than the variability expected by the underlying probability model.
- \textbf{Underdispersion} occurs if the variability is lower than expected, but it is rarely an issue.

# Overdispersion
```{r poisson-negbin-dist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(MASS)
X$x2 <- rnegbin(1000, mu = 1.6, theta=0.5)
X.tmp <- melt(X)
levels(X.tmp$variable) <- c("Poisson", "Overdispersed")
ggplot(aes(x = value, group = variable, fill = variable), data = X.tmp) + geom_histogram(bins = max(X$x2), alpha = 0.8) + theme_classic() + labs(x = "Visits", y = "Count", caption = TeX("Poisson($\\lambda = 1.6$) and NegBin($\\mu = 1.6$, $\\theta = 0.5$)"), fill = "DGP")  + scale_x_continuous(breaks=seq(from=0, to = max(X$x2),by =2)) + scale_fill_viridis_d(option = "turbo")
```


# Overdispersion
## Negative binomial distribution and regression
- The \textbf{negative binomial} distribution (aka the gamma-Poisson distribution) includes an additional parameter $\theta$ to account for dispersion, referred to as a \textbf{scale parameter}.

$$y = NegativeBinomial(\lambda, \theta)$$

- In negative binomial regression, $\theta$ is estimated from the data. The value must be positive.
    - Lower values indicate greater overdispersion.
    - Negative binomial becomes identical to Poisson as $\lim_{\theta \rightarrow \infty}$.

# Overdispersion
## Fitting a negative binomial regression
Negative binomial regression is not implemented in `glm`. Instead, we can use the `glm.nb` function from the `MASS` package.\footnote{The ``fixest`` package has an implementation, ``fenegbin`` that is more suitable for these data as it can also cluster standard errors.}
```{r nb-reg, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(MASS)
nb <- glm.nb(tweet_count ~ seats_per + left_right + country,
         data = data)
```

# Comparing Poisson and negative binomial regression
```{r negbin-table, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson" = pois, "Negative binomial" = nb), stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), gof_omit = "AIC|BIC|RMSE", output = "latex", coef_omit = "country*", note = "Country FE omitted. Exponentiated coefficients.", exponentiate = F, coef_map = c("seats_per" = "Seats %","left_right" = "Ideology [0-10]", "(Intercept)" = "Intercept"))
```

# Negative binomial regression
```{r negbin-theta, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
nb$theta
nb$SE.theta
```


# Negative binomial regression
```{r negbin-preds2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
preds.nb <- predict(nb, data, type = "response")
resids.nb <- residuals(nb, data, type = "response")
left_right <- nb$model$left_right
observed <- nb$model$tweet_count
temp.nb <- as.data.frame(cbind(resids.nb, preds.nb, observed, left_right))
ggplot(aes(x = observed, y = resids.nb, color =left_right), data = temp.nb) + geom_point() + scale_x_log10() + theme_classic() +  scale_color_scico(palette = "vik") + labs(color = "Ideology", y = "Residuals", x = "Observed tweets")
```

<!-- Breaking change?
# Negative binomial regression
#```{r negbin-density, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
#library(reshape2)
#temp2 <- melt(temp %>% dplyr::select(preds.p, preds.nb))
#ggplot(aes(x = value, group = variable, fill = variable), data = temp2) + geom_density(alpha = 0.5) + #theme_classic() + labs(color = "Variable", y = "Density", x = "Tweet count")
#```
-->


# Negative binomial regression
## Bayesian estimation
```{r nb-bayes, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
pois.b <- stan_glm(tweet_count ~ seats_per + left_right +
                        country,
        data = data,
        family = poisson,
       seed = 08901, chains = 1, iter = 4000, refresh = 0)

nb.b <- stan_glm(tweet_count ~ seats_per + left_right +
                      country,
        data = data,
        family = neg_binomial_2(),
       seed = 08901, chains = 1, iter = 4000, refresh = 0)
```


# Poisson posterior predictive check
```{r poisson-ppc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(bayesplot)
ppc_dens_overlay(y = data$tweet_count,
                 yrep = posterior_predict(pois.b, draws = 100)) + xlim(0,50000)
```


# Negative binomial posterior predictive check
```{r nb-ppc, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ppc_dens_overlay(y = data$tweet_count,
                 yrep = posterior_predict(nb.b, draws = 100)) + xlim(0, 50000)
```


# Poisson PSIS plot
```{r loo1, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
l.pois <- loo(pois.b)
plot(l.pois)
```


# Negative binomial PSIS plot
```{r loo2, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
l.nb <- loo(nb.b)
plot(l.nb)
```

# Negative binomial regression
## Comparing Poisson and negative binomial models
```{r loo-compare, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
loo_compare(l.pois, l.nb)
```


# Negative binomial regression
## Bayesian estimate of $\theta$
```{r plot-theta, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
library(tidybayes)
nb.b %>% gather_draws(reciprocal_dispersion) %>% 
    ggplot(aes(x = .value)) +
  stat_halfeye() + theme_tidybayes() + labs(y = "Posterior density", x = TeX("$\\theta$"), title = 
                                                TeX("Posterior dist. of $\\theta$"))
```


# Offsets
## Intuition
- Assume a count outcome $y$ is measured over varying time intervals $t$. The level of $y$ will vary both as a function of the underlying count process and the length of \textbf{exposure}.\footnote{\tiny The same logic would apply if we measured quantities over varying spatial units, e.g. counting people in blocks versus census tracts.}
- We can add an \textbf{offset} to our model to account for varying exposures.
- The outcome of a model with an offset is now $\frac{y}{t}$.


# Offsets
## Explanation
- The mean of a Poisson process, $\lambda$ is implicitly $\lambda = \frac{\mu}{\tau}$, the expected number of events, $\mu$, over the duration $\tau$.
- Assume a Poisson process where $\lambda_i$ is the expected number of events for the $i^{th}$ observation. We can write the link function as 

$$y = Poisson(\lambda)$$
$$log(\lambda) = log(\frac{\mu}{\tau}) = \beta_0 + \beta_1x$$

- This can be re-written as

$$ = log(\mu) -log(\tau) = \beta_0 + \beta_1x$$


# Offsets
## Explanation
- We can think of $\tau$ as the number of \textbf{exposures} for each observation. Thus, we can write out a new model for $\mu$:

$$y \sim Poisson(\mu)$$

$$log(\mu) = log(\tau) + \beta_0 + \beta_1x$$


# Offsets
## Example: Predicting retweet rates
- The number of retweets depends on the number of times a party tweeted
    - No tweets, no retweets
    - More tweets, more retweets?
- Two specifications
    - No offset
    - Log(tweets) included as offset
    

# Offsets
## Example: Predicting retweet rates
```{r offset-model, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
nb.rt <- glm.nb(retweet_total ~ 
                    seats_per + left_right +  country,
                    data = data)
nb.rt.o <- glm.nb(retweet_total ~ offset(log(tweet_count)) +
                      seats_per + left_right +  country,
                      data = data)
```

# Offsets
```{r retweet, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("NB" = nb.rt, "NB (Offset)" = nb.rt.o), stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001), gof_omit = "AIC|BIC|RMSE", output = "latex", exponentiate = T, coef_omit = c("country*"), coef_map = c("seats_per" = "Seats %","left_right" = "Ideology [0-10]", "(Intercept)" = "Intercept"))
```

# Offsets
## Using offsets
- Offsets allow models to be interpreted as rates rather than counts.
- Always include an offset if there are differences in measurement intervals across observations.
- Offsets can also be included when intervals are constant if a rate is more informative.


# Zero-inflated models
## Intuition
- Some count outcomes have high rates of zeros. What if zeros are generated by a different process compared to non-zeros?
- \textbf{Zero-inflated models} allow us to jointly model the process determining whether counts are non-zero and the expected count for each non-zero observation.

# Zero-inflated models
## Specification
- The zero-inflated Poisson model consists of a mixture of two linear models, a logistic regression predicting the probability of a zero and a Poisson model predicting the count outcome.

$$y_i = ZIPoisson(p, \lambda)$$
$$logit(p) = \gamma_{0} + \gamma_{1}z$$

$$log(\lambda)  = \beta_{0} + \beta_{1}x$$

- Each model has its own set of regression parameters. These can be specified differently to model each process.

# Zero-inflated models
## Example: Books borrowed from the library
- Are you borrowing any books from the library?
    - If so, how many?

# Zero-inflated models
## Example: Books borrowed from the library
```{r library, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
N <- 1000 # N

z <- rnorm(N, 0.5, 1) # Random variable determines library usage=
p_lib <- 1/(1 + (exp(1)^-(z))) # Convert to probability 

lib <- rep(0,N) # Generate binary library variable
for (i in 1:N) {
    lib[i] <- rbinom(1, 1, p_lib[i])
}

sum(lib)/N
```

# Zero-inflated models
## Example: Books borrowed from the library
```{r library-books, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
x <- rnorm(N) # Random variable for books borrowed

books <- c() # Store number of books borrowed for each student
for (i in 1:N) {
    if (lib[i] == 1) { # Borrow books if library visitor
        books[i] <- rpois(1, lambda = exp(0.5 + x[i]))
    } else {books[i] <- 0} # Otherwise zero
}
mean(books)
max(books)
```

# Zero-inflated models
## Two kinds of zeros
```{r books-zeros, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
sum(books == 0)
sum(books == 0 & lib == 1)
sum(books == 0 & lib == 0)
```

# Zero-inflated models
```{r books-hist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(books, breaks = max(books))
```

# Zero-inflated models
## Estimating a Poisson model
```{r library-books-nb, echo = TRUE,   mysize=TRUE, size='\\footnotesize'}
book.data <- as.data.frame(cbind(books, x, z))

pois.m <- stan_glm(books ~ x, data = book.data, family = poisson(),
       seed = 08901, chains = 1, refresh = 0)
```



# Poisson posterior predictive checks
```{r library-books-nb-preds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ppc_bars(y = books,
         yrep = posterior_predict(pois.m, draws = 100)) +
  coord_cartesian(xlim = c(0, 10)) +
  labs(caption = TeX('Truncated to y \\leq 10'), 
       title = "Poisson")
```


# Poisson posterior predictive checks
```{r library-books-nb-preds-z, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
pred.books <- posterior_predict(pois.m)
ppc_stat(y=books, pred.books, stat=function(y) mean(y==0)) + labs(title = "Predicted proportion of zeros")
```


# Zero-inflated models
## Estimating a zero-inflated Poisson model
We must use the `brms` library to implement Bayesian zero-inflated Poisson regression.
```{r zinb, echo = TRUE,  mysize=TRUE, message=FALSE, warnings=FALSE, size='\\footnotesize'}
library(brms)
zip <- brm(bf(books ~ x,
           zi ~ z),
              data = book.data,
              family = zero_inflated_poisson(link = "log",
                                            link_zi = "logit"),
              seed = 08901, refresh = 0, chains = 1, backend = "cmdstanr")
```

# Zero-inflated models
```{r zinb-sum, echo = FALSE,  mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Poisson" = pois.m, "ZI Poisson" = zip), statistic = "conf.int", gof_omit = "Log.Lik.|R2|WAIC|RMSE|LOOIC",
             output = "latex")
```

# Posterior predictive checks
```{r zip-preds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ppc_bars(y = books,
         yrep = posterior_predict(zip, draws = 100)) +
  coord_cartesian(xlim = c(0, 10)) +
  labs(caption = TeX('Truncated to y \\leq 10'), 
       title = "Zero-inflated Poisson")
```

# Posterior predictive checks
```{r zip-preds-z, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
pred.books <- posterior_predict(zip)
ppc_stat(y=books, pred.books, stat=function(y) mean(y==0)) + labs(title = "Predicted number of zeros")
```


# Zero-inflated models
## Comparing standard and zero-inflated Poisson models
```{r zip-loo, echo = FALSE, mysize=TRUE, size='\\footnotesize', warnings = F}
loo_compare(loo(zip), loo(pois.m))
```

# Summary
- Standard linear models are generally unsuitable for count data
- Poisson regression can be used for many count outcomes
- Overdispersion occurs when variation higher than expected under Poisson model
    - Negative binomial regression includes a scale parameter to model this
- Offsets transform from counts to rates and should be used when measurement intervals vary
- Zero-inflated models can decompose processes generating zeros and counts

# Next week
- Categorical outcomes
    - Multinomial and ordered logistic regression