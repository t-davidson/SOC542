---
title: "SOC542 Statistical Methods in Sociology II" 
subtitle: "Categorical outcomes"
author: Thomas Davidson
institute: Rutgers University
date: April 14, 2025
urlcolor: blue
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: FALSE
      incremental: FALSE
      fig_width: 3.5
      fig_height: 2.5
header-includes:
  - \usepackage{hyperref}
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \newcolumntype{d}{S[input-symbols = ()]}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")
library(ggplot2)
library(tidyverse)
library(latex2exp)
library(kableExtra)
library(modelsummary)
library(viridis)
library(cowplot)
library(mice)
library(reshape2)
library(scales)
# library(huxtable) # Causes problems knitting, including ! LaTeX Error: Environment centerbox undefined.

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

knitr::opts_chunk$set(
 fig.width = 4,
 fig.asp = 0.8,
 out.width = "70%",
 fig.align = "center"
)

kable <- function(data) {
  knitr::kable(data, digits = 3) %>% 
    kable_styling(position = "center")
}

set.seed(08901)

options("modelsummary_format_numeric_latex" = "plain")
```


# Course updates
- Homework 4 due Friday, 4/18 at 5pm
- Project preliminary results due next Friday, 4/25 at 5pm
- Presentations in class on 5/1

# Course updates
- Labs
    - This week is final lab on lecture material
    - Remaining labs are project workshops
        - Attendance is still mandatory, but use the time to work on your analyses and troubleshoot

# Plan
- Categorical outcomes
- Multinomial logistic regression
- Ordered logistic regression

# Categorical outcomes
## Categories of categories
- A categorical outcome consists of *three or more discrete categories*
- *Ordered* categorical outcomes
    - e.g. Very good, good, okay, bad, very bad.
- *Unordered* (or nominal) categorical outcomes
    - e.g. Single, in a relationship, married, divorced, it's complicated.
    
# Categorical outcomes
## Intervals
- If a categorical variable is *ordered* then there should be an \textbf{interval} between categories such that each category can be positioned on a single dimension.
    - These intervals may vary between categories:
        - e.g. The difference between good and very good may be larger than difference between good and okay.
- Categories without any ordering do not have clearly defined intervals between categories.
    
# Categorical outcomes
## Modeling categories using existing approaches
- OLS regression
    - Only suitable if there are many categories and intervals are *evenly spaced*
- *One-versus-rest* logistic regression models
    - One model for each category with a binary outcome
    - Limitations: Loss of information
    
# Data
## GSS 2018
- Two outcomes from the GSS 2018:
    - Unordered: Marital status
        - Married, widowed, divorced, separated, never
    - Ordered: Self-reported health
        - Excellent, good, fair, poor

# Models for categorical outcomes
- We will be considering two different approaches using variations of logistic regression:
    1. Unordered outcomes modeled using \textbf{multinomial} logistic regression
    2. Ordered outcomes modeled using \textbf{ordinal} logistic regression

# Multinomial logistic regression
- \textbf{Multinomial logistic regression} models generalizes logistic regression to *unordered* categorical outcomes.
- For a set of $K$ outcomes, we can model the linear propensity for outcome $k$ using a linear model with $n$ predictors.

$$\lambda_k = \beta_{0k} + \beta_{1k}x_1 + ... + \beta_{nk}x_n$$

- Jointly estimate a set of equations, one for each category.

# Multinomial logistic regression
-  The probability of outcome $y_k$ is represented by the \textbf{softmax} link function.\footnote{\tiny The approach is therefore sometimes referred to as \textbf{softmax regression}.} The probability of outcome $k$ is the exponentiated linear propensity of outcome $k$ relative to the sum of exponentiated linear propensities of all outcomes in the set $K$ (Kruschke 2015: 650).

$$P(y = k|X) = \text{softmax}_K(\lambda_k) = \frac{e^{\lambda_k}}{\sum_{i \in K}e^{\lambda_i}}$$

# Multinomial logistic regression
- Due to the constraints on the system, one category will always produce the following equation:

$$\lambda_r = \beta_{0r} + \beta_{1r}x_1 + ... + \beta_{nr}x_n = 0 + 0x_1 + ... + 0x_n = 0 $$

- We therefore select a category to leave out as the *reference category*.
- Estimated coefficients can thus be interpreted as the log odds of each outcome, relative to the reference category.
        
```{r load-data, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
gss <- haven::read_dta("../../2022/labs/lab-data/GSS2018.dta") %>%
    filter(age <= 89) %>% haven::zap_labels() %>%
    mutate(sex = ifelse(sex == 1, "Male", "Female"),
           race = as.factor(race))
gss$race <- recode_factor(gss$race, "1" = "White", "2" = "Black", "3" = "Other")
gss$marital <- recode_factor(gss$marital, "1" = "Married", "2" = "Widowed", "3" = "Divorced", "4" = "Separated", "5" = "Never")
gss$health <- recode_factor(gss$health, "Excellent", "Good", "Fair", "Poor")
gss <- gss %>% select(age, sex, race, realrinc, educ, marital, health)
```

    
# Multinomial logistic regression
## Estimation
- These models are more complex than other GLMs due to the estimation of multiple equations.
- Maximum likelihood models can be estimated using `nnet::multinom`\footnote{\tiny Other packages are available but require additional data manipulation before modeling. See \href{https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/}{this blog} for further discussion.}
- Bayesian models can be estimated using the `brms` package and `family = categorical(link = "logit")`.

# Data: Marital status
```{r load-married-plot, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
gss$marital <- relevel(gss$marital, ref = "Never")
ggplot(gss %>% drop_na(age, sex, realrinc, educ, marital), aes(x=marital, fill = marital)) + geom_bar() + theme_minimal() +
    labs(x= "", y = "Frequency", fill = "") + scale_fill_viridis_d(option = "magma") + theme(legend.position = "none")
```

# Multinomial logistic regression
## Estimation
```{r est-mlr, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
library(nnet)
gss$marital <- relevel(gss$marital, ref = "Never")
m1 <- multinom(marital ~ age + sex + log(realrinc) + educ, data = gss)
```

# Multinomial logistic regression
```{r results1, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(m1, stars = c("*"=0.05, "**"=0.01, "***" = 0.001),
             shape = term + statistic ~ response,
             output = "latex",
             note = "Ref: Never married.",
             gof_omit = "BIC|RMSE")
```

# Multinomial logistic regression
```{r results-exp, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(m1, stars = c("*"=0.05, "**"=0.01, "***" = 0.001),
             shape = term + statistic ~ response,
             output = "latex",
             note = "Ref: Never married.", exponentiate = T,
             gof_omit = "BIC|RMSE")
```

# Multinomial logistic regression
## Interpretation
- Each column is an equation for a specified category comparing a group to the *baseline* (Never married).
- For example, the first column represents the following equation:

$$log(\frac{y = \text{married}}{ y = \text{never married}}) = \beta_{10} + \beta_{11}Age + \beta_{12}Sex  + \beta_{13}Income + \beta_{14}Educ$$

# Multinomial logistic regression
## Interpretation
- $\beta_{11}$ indicates that a one-year increase in age is associated with a .092 change in the log odds of being married compared to never married.
- Like standard logistic regression $e^{\beta_{11}}$ can be interpreted as an odds ratio.
    - In this case, it is the \textbf{relative risk ratio} of being married vs. never married.
 
# Multinomial logistic regression
## Predictions  
The `predict` function returns a factor variable containing the highest probability category for each observation.
```{r mlr-preds, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
preds <- predict(m1, gss %>% drop_na(age, sex, realrinc, educ, marital))
preds %>% head(20)
```

# Multinomial logistic regression
```{r mlr-preds-plot, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
# Note: None predicted as separated, but I want it to show in plot
# Current solution is to append an observation to separated
# Since it is a factor this must be done using function from forcats
library(forcats)
preds <- fct_c(preds, as.factor(c("Separated")))
ggplot(data.frame(preds), aes(x=preds, fill = preds)) + geom_bar() + theme_minimal() +
    labs(x= "", y = "Frequency", fill = "") + scale_fill_viridis_d(option = "magma") + theme(legend.position = "none")
```


# Multinomial logistic regression
## Predictions
- The model predicts almost all people as never married or married. 
- It rarely predicts widowed or divorced and did not predict any people to be separated.
- Data imbalances make never/married the most likely categories; additional variables may help to predict other categories.

# Multinomial logistic regression
## Predictions  
Setting `type = "probs"` returns a vector of probabilities for each observation. Each element indicates $P(y_i = k)$.
```{r mlr-probs, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
probs <- predict(m1, type = "probs", gss %>% drop_na())
probs %>% round(3) %>% head(5)
```

# Multinomial logistic regression
## Predictions  
The probabilities for each observation sum to one, a feature of the softmax function.
```{r mlr-probs2, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
probs %>% head(5) %>% rowSums() %>% as.numeric()
```

# Multinomial logistic regression
## Limitations
- Larger samples required compared to more simple models, particularly when categories are imbalanced
- Difficult to evaluate model fit
- Unstable if some variables perfectly predict category membership or have no overlap with certain categories.

# Ordinal logistic regression
- The multinomial framework could be used for ordinal data, but it ignores any information about the order of categories.
- \textbf{Ordinal} logistic regression accounts for ordering by using \textbf{cutpoints} to map the intervals between categories onto a linear scale.

# Ordinal logistic regression
- Methodology:
    - Map categorical outcome onto cumulative probability scale using cumulative link.
    - Convert to log-cumulative-odds, analogue of the logit link for cumulative scale.
    - Construct a linear model to examine association between predictors and outcome, while maintaining information on order.
    

# Data: Self-reported health
```{r health-plot, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(gss %>% drop_na(age, sex, health, race), aes(x=health, fill = health)) + geom_bar() + theme_minimal() +
    labs(x= "", y = "Frequency", fill = "") + scale_fill_viridis_d(option = "plasma") + theme(legend.position = "none")
```

# Ordinal logistic regression
## Cumulative probabilities of each class
```{r cum-probs, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
gss.h <- gss %>% drop_na(age, sex, health, race)
N <- dim(gss.h)[1]
props <- gss.h %>% group_by(health) %>% summarize(prop = n()/N) %>%
    mutate(cm = cumsum(prop))
props$n <- c(1,2,3,4)
ggplot(props, aes(y = cm, x=n)) + geom_line() + theme_minimal() +
    theme(legend.position = "none") + labs(y = "Cumulative probability", x = "") + scale_x_continuous(breaks = 1:4, labels=c("Excellent", "Good", "Fair", "Poor"))
props$cm %>% round(3) %>% print()
```

# Ordinal logistic regression
## Log cumulative odds
```{r log-cum-odds, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
props$log.cm.odds <- log(props$cm/(1-props$cm))
ggplot(props, aes(y = log.cm.odds, x=n)) + geom_line() + theme_minimal() +
    theme(legend.position = "none") + labs(y = "Log-cumulative-odds", x = "") + scale_x_continuous(breaks = 1:4, labels=c("Excellent", "Good", "Fair", "Poor"))
props$log.cm.odds %>% round(3) %>% print()
```

# Ordinal logistic regression
## Estimation
- Each cutpoint represents the log-cumulative-odds that $y_i$ is less than or equal to some value $k$. These are analogous to *group-level intercepts*.

$$log(\frac{P(y_i \leq k)}{1- P(y_i \leq k)}) = \alpha_k$$


- The intercept for the final value is $\infty$ since $log(\frac{1}{1-1}) = \infty$. Therefore we only need $K-1$ intercepts.


# Ordinal logistic regression
## Estimation
- If we use the inverse link, we can go back from cumulative-log-odds to cumulative probabilities. The likelihood of $k$ is expressed as

$$p_k = P(y_i = k) = P(y_i \leq k) - P(y_i \leq k - 1)$$

- In the context of your example, we could express the likelihood of "Good" health as 

$$p_{\text{good}} = P(y_i = \text{good}) = P(y_i \leq \text{good}) - P(y_i \leq  \text{excellent})$$

# Ordinal logistic regression
## Estimation 
- Given this $K-1$ length vector of intercepts, $\alpha_{k \in K-1}$, we can use a linear model to predict the log-cumulative-odds that $y_i = k$ given a matrix of predictors $X$:

$$\phi_i = \beta X_{i}$$
$$log(\frac{P(y_i \leq k)}{1 - P(y_i \leq k)}) = \alpha_k - \phi_i$$

# Ordinal logistic regression
## Estimation
- Once again, we cannot fit the model using `glm`. Instead, we can use the `polr` function from the `MASS` package.
- `rstanarm` includes a Bayesian implementation, `stan_polr`

# Ordinal logistic regression
## Estimation
```{r health-polr, echo = TRUE, mysize=TRUE, size='\\footnotesize', warning = F}
library(MASS)
m2 <- polr(health ~ age + I(log(realrinc)) + educ + sex + race,
                     data = gss, Hess = TRUE)
```
The argument `Hess = TRUE` ensures the Hessian matrix is stored. This is necessary for subsequent model evaluation.


# Ordinal logistic regression\footnote{\tiny Significance tests are not provided as standard in ordinal regression output from \texttt{polr} so no stars are displayed here.}
```{r health-polr-out, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
modelsummary(list("Log odds" = m2, "Odds ratios" = m2), output = "latex", exponentiate = c(F, T), coef_omit = "Excellent*|Good*|Fair*", gof_omit = "RMSE|BIC|edf")
```

# Ordinal logistic regression
## Predictions
```{r polr-preds, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
preds2 <- predict(m2, gss %>% drop_na(health, age, sex, race, realrinc, educ))
preds2 %>% head(20)
```

# Ordinal logistic regression
## Predictions
```{r polr-preds-plot, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
# Same as above, some categories never predicted
# At some point figure out how to keep zeros rather than adding ones here
preds2 <- fct_c(preds2, as.factor(c("Excellent", "Fair", "Poor")))
ggplot(data.frame(preds2), aes(x=preds2, fill = preds2)) + geom_bar() + theme_minimal() +
    labs(x= "", y = "Frequency", fill = "") + scale_fill_viridis_d(option = "plasma") + theme(legend.position = "none")
```

# Ordinal logistic regression
## Predictions
```{r polr-probs, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
probs2 <- predict(m2, type = "prob", 
                  gss %>% 
                      drop_na(health, age, sex, race, realrinc, educ))
probs2 %>% round(3) %>% head(5)
```

# Ordinal logistic regression
```{r polr-probs-hist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
tmp <- melt(probs2)
ggplot(tmp, aes(x = value, group = Var2, fill = Var2)) + geom_density(alpha = 0.6) + scale_fill_viridis_d(option = "plasma") + theme_minimal() + labs(y = "Density", x = TeX("$P(y_i = k)$"), fill = "") + xlim(min(probs2), max(probs2))
```

# Ordinal logistic regression
## More predictions
We can easily generate predictions for all combinations of predictors.
```{r more-preds, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
newdat <- expand_grid(
  race = c("Black", "White", "Other"),
  sex = c("Female", "Male"),
  educ = 12,
  realrinc = c(50000),
  age = 18:65)

newpreds <- predict(m2, newdat, type = "probs")
head(newpreds, 5) %>% round(3)
```

# Ordinal logistic regression
<!--TODO: Get figure to render in correct proportions-->
```{r plot-final-preds, echo = FALSE, fig.width = 6, fig.height = 4}
newdat <- cbind(newdat, newpreds)
tmp <- melt(newdat, id.vars = c("race", "sex", "age", "educ", "realrinc"),
  variable.name = "Variable", value.name="Probability")

ggplot(tmp, aes(x = age, y = Probability, colour = Variable)) +
  geom_line() + facet_grid(sex ~ race, labeller="label_both") + theme_cowplot() + 
    scale_color_viridis_d() + labs(color = "Health", x = "Age")
```

# Ordinal logistic regression
## Cutpoints
The cutpoints can be extracted using the `zeta` parameter.
```{r cuts, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
cuts <- m2$zeta
print(cuts)
```

# Ordinal logistic regression
## Cutpoints
We can obtain the probability associated with each cutpoint by using the inverse logit function, $\frac{e^x}{1 + e^x}$. 
```{r cuts-convert, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
inv.logit <- function(x) {
    return(exp(1)^x / (1 + exp(1)^x))
    }

cut.probs  <- inv.logit(cuts)
cut.probs %>% round(3) %>% print()
``` 

# Ordinal logistic regression
## Latent variables
One way to understand the model is to extract a *latent variable* representing the predicted position of each outcome on the cumulative probability scale without subtracting the intercepts. We can then observe where each observation falls between the cutpoints.
```{r cuts-inv, echo = TRUE, mysize=TRUE, size='\\footnotesize'}
z <- m2$lp  %>% inv.logit()
z %>% head(10) %>% round(3)
```

# Ordinal logistic regression
```{r cuts-hist, echo = FALSE, mysize=TRUE, size='\\footnotesize'}
hist(z, breaks = 100)
```


# Ordinal logistic regression
```{r cuts-plot, echo = FALSE, mysize=TRUE, size='\\footnotesize', warning = F, message = F, results = F}
df <- as.data.frame(z)
p <- ggplot(df, aes(x = z)) + geom_density()# + theme_minimal() + labs(y = "Density", x = "z", fill = "")
d <- ggplot_build(p)$data[[1]]

cols <- viridis(4, option = "plasma")
p <- p +  geom_area(data = subset(d, x < cut.probs[1]), aes(x=x, y=y), fill = cols[1]) +
    geom_area(data = subset(d, x > cut.probs[1] & x <= cut.probs[2]), aes(x=x, y=y), fill = cols[2]) +
    geom_area(data = subset(d, x > cut.probs[2] & x <= cut.probs[3]), aes(x=x, y=y), fill = cols[3])# +
    geom_area(data = subset(d, x > 0.850), aes(x=x, y=y), fill = "red") # last part redundant due to zero coverage, commented out
p + theme_minimal() + labs(y = "density", x = "z") + xlim(0,1) + geom_vline(xintercept = cut.probs[1], linetype = "dotted") + 
    geom_vline(xintercept = cut.probs[2], linetype = "dotted") + geom_vline(xintercept = cut.probs[3], linetype = "dotted") + geom_vline(xintercept = cut.probs[4], linetype = "dotted")
# Note: Min prob is >0, so shading doesn't go to zero. 
# TODO: Add viridis colors from palette
```


<!--TODO: See GHV p. 276 for how to plot this effectively with the latent variable specification.
I want a single predicted outcome and to show the cutpoints.

Discuss proportional odds assumption

This post nicely explains how predictions are derived from the mode
https://stats.stackexchange.com/questions/41006/predicting-ordered-logit-in-r
-->

# Ordinal logistic regression
## Limitations
- Similar to multinomial logistic regression
    - Larger samples required compared to more simple models
    - Difficult to evaluate model fit
    - Unstable if some variables perfectly predict category membership or have no overlap with certain categories
    
# Ordinal logistic regression
## Proportional odds assumption
- Assumes the relationship between the predictors and each pair of outcomes is the same (hence one set of coefficients). - Additional tests are required to verify this is met.\footnote{\tiny See the \href{https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/}{UCLA stats blog} for details.}

# Categorical outcomes
## Frequentist and Bayesian approaches
- Due to the complexity of the models, many frequentist approaches require additional testing and analysis to diagnose issues and assess model fit
- In contrast, we can use the same tools to evaluate Bayesian models:
    - Trace plots and MCMC diagnostics for estimation issues
    - LOO-CV and ELPD for fit
    - PSIS diagnostics for outliers
    - Posterior predictive checks for predictions and fit
- Either way, these models are more cumbersome to work with than other single-equation GLMs

# Summary
- Categorical outcomes can be modeled using specialized types of generalized linear models
- Unordered categories
    - Multinomial logistic regression
- Ordered categories
    - Ordinal logistic regression
    - OLS if many categories and equal intervals
- These models are complex and more difficult to fit and interpret than previous models we have covered

<!--
# Next week
- Data structures
    - Clustering and nesting
        - Standard errors
        - Fixed effects
        - Random effects
    - Autocorrelation
        - Time
        - Space
        - Networks
- Project workshop
-->