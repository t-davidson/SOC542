---
title: "Week 5 - Regression with Categorical and Nonlinear Variables"
author: "Brent Hoagland, Lab TA"
date: "2/24/2025"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)

library(tidyverse)
library(haven)
library(modelsummary)
library(rstanarm)
library(tidybayes)

if (!require("fastDummies")) {  
  install.packages("fastDummies", dependencies = TRUE)

}
library(fastDummies)

set.seed(1234) # Optional, for reproducibility
seed <- 1234
```


# Return of the GSS

## Data Loading 

Let's bring back our old friend, the General Social Survey!

Today, we're going to investigate the effects that hours of work, occupational prestige, educational degree, sex, race, age, and political party have on a person's earned income. 
```{r gss-load-select}

# Write the code or change your working directory to properly load the GSS2022.dta file and assign it to the object gss2022
gss2022 <- read_dta("DATASETS/GSS/GSS2022.dta")

gss <- gss2022 %>% 
  select(conrinc,                 # DV: Inflation-adjusted personal income 
         hrs1, prestg10, degree,  # IVs: career variables (number of hours worked last week, r's occupational prestige score, r's highest degree )
         sex, race, age, partyid,
         wtssnrps) # IVs: some demographic information

gss <- haven::zap_label(gss) # This function takes away any long labels that NORC might have included in the dataset; it just helps for readability for view(gss)

```

So far this semester, we haven't spent much time recoding variables. However, recoding variables is especially crucial when dealing with categorical/nominal variables. You'll see that 60% of your time will probably be dedicated to the preparation of data, including handling missing values and recoding variables. Running a regression only takes a few lines of code, but the upfront data management is necessary to ensure that our analyses are accurate and meaningful in the end. Let's dive into some data wrangling techniques by recoding specific variables in our dataset.

## Recoding Data

```{r Preliminary check for age variable before recoding}

gss$age
mean(gss$age)
mean(gss$age, na.rm = TRUE)
summary(gss$age)

```

```{r Adjust age variable using base R coding}
# From GSS online data explorer (or for most other datasets the codebook): -100 = inapplicable, -99 = No answer, -98 = Do not Know/Cannot Choose
gss$age <- ifelse(gss$age == -100, NA, gss$age)
gss$age <- ifelse(gss$age == -99, NA, gss$age)
gss$age <- ifelse(gss$age == -98, NA, gss$age)

# Check 'age' variable after recoding
summary(gss$age)

# Question: Did any observations have the values of -100, -99 or -98 for the age variable?
# Answer: BLANK
```

```{r Recoding work hours using tidyvere logic and tools}

# Preliminary check for 'hrs1' variable before recoding
summary(gss$hrs1)

# Recode work hours using tidyverse package: 
# From GSS online data explorer (or for most other datasets the codebook): -100 = inapplicable, -99 = No answer, -98 = Do not Know/Cannot Choose, -97 = Skipped on Web, 89 = 89+ hours
gss <- gss %>%
  mutate(
    workhrs = case_when(
      hrs1 == 89 ~ NA, # Recoding to remove possible outliers and preserve linearity and. This is decision you will probably consider in your own data
      hrs1 <= -97 ~ NA, # Recoding all negative values to NA
      TRUE ~ hrs1
    )
  ) 

# Check the variable after recoding
summary(gss$hrs1)
summary(gss$workhrs)
  
```

The above are two code chunks represent effective ways to recode data. The first uses the logic of base R and the second the logic and tools of tidyverse and piping. Both work and you will develop a preference. While it can look more complicated, I recommend building your skills to recode using tidyverse. 

```{r recoding practice}
# Rename the `conrinc` as a numeric variable "income", `prestg10` to "prestige" and wtssnrps to "weights"
gss <- gss %>% 
  mutate(BLANK = as.numeric(BLANK), 
         BLANK = as.numeric(prestg10),
         weights  BLANK)

# Recode the degree variable into a factor with labels corresponding to "Less_HS", "HS", "Assoc", "Bach", "Grad"
 gss <- gss BLANK 
   mutate(
      degree = factor(degree,
                    levels = c(0,1,2,3,4),
                    labels = c(BLANK)
    )
  )
 

# Recode the sex variable into a factor variable
gss <- gss BLANK
  BLANK(
      sex = factor(sex,
                    levels = c(1,2),
                    labels = c(BLANK)
                    )
      )

# Recoding race variable into descriptive character vector.
gss <- gss %>%
  mutate(
    BLANK  
    )
  )
  
# Recoding partyid variable into political affiliation categories.
gss <- gss %>%
  mutate(
      partyid = case_when(
      partyid %in% c(0:2) ~ "BLANK", # "Dem"
      partyid %in% c(4:6) ~ "BLANK", # "Rep"
      partyid %in% c(3,7) ~ "BLANK"  # "Other" - accounting for major and other political affiliations
    )
  )

```

```{r recoding removing uncessary parts}

gss <- gss %>% 
  select(-hrs1, -conrinc, -prestg10, -wtssnrps)  # Removing the original work hours and weird income name from the dataset we will be working with

gss <- gss %>% drop_na() # Dropping any rows missing on any variable. The way this is implemented here works the same as keeping only the complete.cases

```

## Data Summary
This week, we're going to use a sibling function, `datasummary_balance()`. This function will give us the mean and standard deviation for our continuous data, and below it, give us the counts and percentages for our categorical data.

The formula is `datasummary_balance( ~ 1, ...)`. The "`~ 1`" tells it to create this double-table for all variables in the data. 

```{r data-sum-table}
datasummary_balance( ~ 1,
                    title = "Sample Descriptive Statistics",
                    notes = "Data: 2022 General Social Survey",
                    data = gss)
```

# Regression Models

## Model Specification

Today, we're going to use GSS data to examine factors associated with a person's earned annual income. We're think that a person's age will play a role in this, since people who have worked longer probably will make more money. Similarly, a person who works more hours will earn more. We also think that people with higher-prestige careers (physicians, engineers, etc) will be compensated for such prestige. Lastly, we want to account for any differences by race and sex. 

Let's start with a simple model that only include three continuous variables and two categorical ones. Our model takes the form: $$\widehat{income} = \beta_0 + \hat{\beta}_{Age} + \hat{\beta}_{Work Hours} + \hat{\beta}_{Prestige} + \hat{\beta}_{Sex} + \hat{\beta}_{Race} + u$$


```{r ols-base-model}
ols1 <- lm(income ~ BLANK + BLANK + BLANK + BLANK + BLANK,
                   data = gss)
summary(ols1)
```

## Interpretation 

Looking at our model, we have a series of coefficient estimates, standard errors, t-values, and p-values for each of our independent variables. Let's walk through these one at a time.

1. `age`: For each additional year of age, we can estimate that a person's income will increase by $`r round(ols1[["coefficients"]][["age"]],2)`, holding all else constant.
2. `prestige`: For each additional point of occupational prestige, we can estimate that a person's income will increase by $`r round(ols1[["coefficients"]][["prestige"]],2)`, holding all else constant.
3. `workhrs`: For each additional hour that a person worked in the week before they were interviewed, we can estimate that a person's income will increase by $`r round(ols1[["coefficients"]][["workhrs"]],2)`, holding all else constant.

Note, however, that male, white, and black each have their own coefficients. Instead of having a $\hat{\beta}$ for the categories of race or sex, we create a new one for each dummy: $$\widehat{income} = \beta_0 + \hat{\beta}_{Age} + \hat{\beta}_{Work Hours} + \hat{\beta}_{Prestige} + \hat{\beta}_{Female} + \hat{\beta}_{White} + \hat{\beta}_{Other} + u$$

So instead of multiplying `r round(ols1[["coefficients"]][["sexMale"]],2)` by some abstract value of `sex`, we multiply it times 1 if the person is male, and by 0 if they are female. Essentially, we're adjusting the intercept term, not the slope. 

And the same goes for race: We multiply `r round(ols1[["coefficients"]][["raceWhite"]],2)` times 1 if they are white, or by 0 if they are Black or another race, then  multiply `r round(ols1[["coefficients"]][["raceOther"]],2)` times 1 if they are "other", or by 0 if they are white or Black. 

Noting that these are all in reference to whatever category was dropped (we'll get to that more in a bit), we can finish our interpretation as follows:

4. `sexMale`: A person who is `Male` can expect to make $`r round(ols1[["coefficients"]][["sexMale"]],2)` compared to a person who is `Female`. 
5. `raceOther`:A person who is `Other` (race) can expect to make $`r round(ols1[["coefficients"]][["raceOther"]],2)` compared to a person who is `Black`. 
6. `raceWhite`: A person who is `White` can expect to make $`r round(ols1[["coefficients"]][["raceWhite"]],2)` compared to a person who is `Black`. 


## Prediction

When predicting using our values, we can plug in the effects, just like from our models last week.

For example, let's estimate a person's income, knowing that they were a 57 year-old white female with an occupational prestige of 70 and who worked 40 hours last week. We can simply input that information into the equation.

While it's possible to do this by hand, it's even easier to input the new person's information into a dataframe with these variables and `predict()` the value using R. 

Try modifying the values to see how the predicted income changes. (NOTE: With `predict()`, you have to include values for every independent variable included in the model.)
```{r ols-predict}
newperson <- data.frame(age = 57,
                        prestige = 70,
                        workhrs = 40,
                        sex = "Female",
                        race = "White")
predict(ols1, newperson)
```

\newpage 

# Data Transformation Effects

## Dummies vs Categories

Another way to think about the above interpretation is by creating a series of "dummy variables." These operate where each variable is split into a series of zero-one variables. For example, our variable `sex` has two levels, "male" and "female." Turning it into a dummy would then create a new variable `male` where males have the coding `1` and females have the coding `0`. The `fastdummies` package has a function `dummy_cols()` that makes this easy.

Let's try this now, turning the variables `sex`, `race`, `partyid`, and `degree` into a series of dummies.

```{r dummy-create, warning=FALSE}

gss <- gss %>% 
  fastDummies::dummy_cols(select_columns = c("sex", "race", "partyid", "degree"))

names(gss)

```


Now, let's try out running models with our new dummies. Note, too, that it doesn't matter which dummy we leave out as long as one of them is left out. Changing the dummy will only change the intercept, not the slope. 
```{r ols-dummy-test}
ols_nodummy <- lm(income ~ age + prestige + workhrs + race + sex,
                  data = gss)
ols_dummy1 <- lm(income ~ age + prestige + workhrs + 
                   race_White + race_Other +
                   sex_Female,
                 data = gss)
ols_dummy2 <- lm(income ~ age + prestige + workhrs +  
                   race_White + race_Other +
                   sex_Male,
                   data = gss)

modelsummary(list(ols_nodummy, ols_dummy1, ols_dummy2),
             stars = T, title = "Testing Dummy Variables",
             gof_omit = "F|Log|IC|RMSE")

```

As we can see from these results, there is no difference in whether you use your own dummies or use the defaults. Just note, however, that R will drop whatever the first category is. In the case of `sex`, the first one was `male`, since I coded it as a `factor`. In the case of `race`, however, it was simply a `character`-type variable, so the first category, and the one that was dropped, was `black`. Creating and using your own dummies gives you more control over this, but isn't necessary. 

If we'd like to change the "reference category," AKA the category that is dropped, one option is to turn the variable into a factor with new levels (1 = White, 2 = Black, 3 = Other). Another is by putting in ourselves which levels to include in the model: Including Black and Other makes White the reference. Finally, another option is to use the `stats::relevel()` function to choose a new base category. (`relevel(var, ref = new_reference_level)`)

# Exercise: Specifiy a multivarite model excluding white, males, and less than a high school degree. Interpret the intercept.
```{r exercise 2}

names(gss)

ols_dummy3 <- lm(income ~ age + prestige + workhrs + 
                   BLANK + BLANK +
                   BLANK + 
                   BLANK + BLANK + BLANK + BLANK,
                 data = gss)

modelsummary(ols_dummy3,
             estimate = "{estimate}{stars}",
             fmt = 0,
             statistic = NULL,
             gof_omit = "IC|Log|alg|pss|F|RMSE",
             notes = "Notes: + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001",
             output="huxtable")

```

```{r lets make it a picture}

modelplot(ols_dummy3,
          coef_omit = "Inter") +
  geom_vline(aes(xintercept = 0), linetype = "dashed") +
  theme(legend.position = "bottom")

```
## Scaling and Non-Linearization 

### Income 
If we look at our income variable, our value is very skewed. It's also in large dollar amounts that aren't easy to manage. Let's see what happens if we made two transformations. First, let's think of income in thousands of dollars instead of in total dollar amounts. Then, let's also take the natural log of our variable so we arrive at something more normally distributed. Finally, let's see what happens if we do both. (Note: by default, `base::log()` takes the natural log. If you want to take the $log_{10}$ or $log_2$, you should use `log10()` or `log2()`. You can also use `log(x, base)` for others.)

Taking the log of a term does two things. 

1. Methodologically, it gives us a distribution that is more normally distributed, improving the overall fit.
2. Theoretically, we use it when we think the effect is *increasing at a decreasing rate OR decreasing at a decreasing rate*. 


```{r plot-log, fig.cap="Visualizing Transformed Income Variables"}
gss <- gss %>% 
  mutate(logincome = log(income),
         thouinc = income/1000,
         logthouinc = log(income/1000)
         )

origincplot <- ggplot(gss) + geom_density(aes(income))
thouincplot <- ggplot(gss) + geom_density(aes(thouinc))
logincplot <- ggplot(gss) + geom_density(aes(logincome))
logthouincplot <- ggplot(gss) + geom_density(aes(logthouinc))

ggpubr::ggarrange(origincplot, thouincplot, logincplot, logthouincplot)
``` 

Now, let's try a series of regressions to see what differences, if any, exist when we make these changes. 

```{r ols-transform-inc}
gss %>% 
  select(income, thouinc, logincome, logthouinc) %>% 
  datasummary_skim(type="numeric",title = "Summary Statistics of Transformed Income Variables")

```


```{r ols-transform-inc-test}
ols_inc <- lm(income ~ age + prestige + workhrs + sex, data = gss)
ols_thou <- lm(thouinc ~ age + prestige + workhrs + sex, data = gss)
ols_log <- lm(logincome ~ age + prestige + workhrs + sex, data = gss)
ols_log_thou <- lm(logthouinc ~ age + prestige + workhrs + sex, data = gss)

modelsummary(list("Original" = ols_inc, "Inc/1k" = ols_thou, 
                  "Log(Inc)" = ols_log, "Log(Inc/1k)" = ols_log_thou),
             title = "Effect of Data Transformations: Logarithmic Terms",
             stars = T, gof_omit = "F|RMSE|Log|IC")

```

We can see here that scaling only shifts the direction of variables, but has no effect on the final model statistics. Overall, the model with the logged term (`logincome`) worked best ($r^2 = .3$) and is the most interpretable. (Remember, an increase in the natural-logged amount equates to a 1% increase in the raw amount. Scaling the logged amount doesn't have any effect since the increase in percent is stable across the board.) 

### Work Experience 

Now, let's try another transformation. This time, let's estimate the number of year's a person has been working by subtracting 18 years from their age. While we're here, let's also see what happens if we square the term. 

We use a square term when we hypothesize a U shaped or inverse-U shaped relationship. That is, when we suspect the effect will be non-linear. 

```{r data-man-trans}
gss <- gss %>% 
  mutate(workexp = age - 18,
         worksq = workexp^2)
gss %>% 
  select(age, workexp, worksq) %>%
  datasummary_skim(title = "Summary Statistics of Transformed Work Experience Variables")
```

Let's now test out whether these two data transformations affect our models. Below I compare age and work experience (equal to $age - 18$). Lastly, I include a model with both work experience and work experience squared. 

```{r ols-transform-age}

ols_age <- lm(logincome ~ age + workhrs + sex,
                   data = gss)
ols_work <- lm(logincome ~ workexp + workhrs + sex,
                   data = gss)
ols_work_worksq <- lm(logincome ~ workexp + worksq + workhrs + sex,
                   data = gss)
wrkcoefs <- c("age" = "Age", "workexp" = "Work Experience",
              "worksq" = "Work Experience ^2", "sexMale" = "Male", "workhrs" = "Work Hours")
modelsummary(list(ols_age, ols_work,  ols_work_worksq),
             coef_map = wrkcoefs, stars = T, gof_omit = "F|RMSE|Log|IC",
             title = "Effect of Data Transformations: Square Terms")

```
Looking at our table, we can see that the simple shift of $work\:experience = age - 18$ also had no real effect on model fit. 

Meanwhile, adding the square term increased our model's prediction power (via the $r^2$).

With these in mind, we can tell that shifting or dividing a variable doesn't change the relationship; it only changes the amount. But changing the shape of the line (via taking the log) does change the relationship and therefore the income. 

Okay, so what does this mean for prediction?

Our equation now is $$log_e(\widehat{income}) = \beta_0 + \hat{\beta}_{Work Experience} + \hat{\beta}_{WorkExperience^2} + ... + u $$ 

It might also be better to graph this to visualize the changing effects. I'll first save the coefficients from the previous model as `coef_workexp` and `coef_worksq`. Then, I write a function that shows how the coefficients are multiplied by `workexp` and `workexp$^2$`. Finally, I use the `stat_function()` function within my `ggplot()` command to graph the line. 

```{r plot_workexp}
coef_workexp <- ols_work_worksq$coefficients["workexp"]
coef_worksq <- ols_work_worksq$coefficients["worksq"]
int_worksq <- ols_work_worksq$coefficients["(Intercept)"]

func_wrkexp2 <- function(workexp) coef_workexp * workexp + coef_worksq * workexp^2 + int_worksq

ggplot() + 
  stat_function(fun = func_wrkexp2, linewidth = 1, aes(color = "Predicted")) +
  scale_x_continuous(name = "Work Experience (Years Since Age 18)", 
                     limits = c(0,70)) +
  geom_smooth(data = gss, aes(x = workexp, y = logincome, color = "Actual")) + 
  labs(y = "log(Income)", title = "Effects of Work Experience on Income",
       caption = "Holding constant work hours and sex", color = "") +
  theme_light() + theme(plot.title = element_text(hjust =.5))
```


## Data Weights 

Lastly, let's compare our models with and without weights on our data. Weighting the data assigns importance in the regression to different observations/individuals/respondents based on their probability of being selected in our sample. Rarely are there large effects, but you may be surprised. (Smaller effects point to better sampling of the population, while larger effects point to worse performance of the sampling procedure.)

```{r weight-test}
ols_final_noweight <- lm(logincome ~ workexp + worksq + prestige + workhrs + 
                           sex + race + partyid + degree,
                   data = gss)
ols_final_weight <- lm(logincome ~ workexp + worksq + prestige + workhrs + 
                         sex + race + partyid + degree,
                   data = gss, weights = weights)
modelsummary(list("Unweighted" = ols_final_noweight, 
                  "Weighted" = ols_final_weight),
             stars = T, gof_omit = "F|RMSE|Log|IC",
             title = "Effect of Data Transformations: Effect of Data Weights")

```

We can also visualize these changes like before.

``` {r weight-test-viz}
modelplot(list("Unweighted" = ols_final_noweight, 
                  "Weighted" = ols_final_weight),
          coef_omit = "Inter") +
  geom_vline(aes(xintercept = 0), linetype = "dashed") +
  theme(legend.position = "bottom")
  
```

Note that the changes are very small, but still noticeable for some variables. Regardless, when using surveys we should use any provided weights so that our estimates from the sample more accurately reflect the population. 

# Bayesian Regression

Now that we have a final, weighted model, let's estimate it using `stan_glm()`. Fortunately, the code is the same as last week, but with the same equation as earlier today. That is, there is no difference in entering a regression model with categorical data or weights. Let's try one version with weights and one without, and we'll go with the default priors. 

```{r bayes-mod}

bayes_noweight <- stan_glm(logincome ~ workexp + worksq + prestige + 
                             workhrs + sex + race + partyid + degree,
                           data = gss, seed = seed, 
                           chains = 1, refresh = 0)
bayes_weight <- stan_glm(logincome ~ workexp + worksq + prestige + 
                           workhrs + sex + race + partyid + degree,
                         data = gss,  seed = seed,
                         weights = weights, ### WEIGHTS USE THIS ARGUMENT 
                         chains = 1, refresh = 0)

```


\newpage 

# Model Comparisons 

Ultimately, we can produce a final table that compares our OLS and Bayesian estimates. Notice in the final command that, should you want to print the `modelsummary` table to a word document, you can uncomment that line and it will produce a .docx file for you in your working directory. However, this will then not produce it in your PDF file. 

```{r comp-table}
model_list <- list("OLS - Weighted" = ols_final_weight, 
                   "Bayes - Unweighted" = bayes_noweight, 
                   "Bayes - Weighted" = bayes_weight)

coef_names <- c("age" = "Age",
                "workexp" = "Yrs Work Exp",
                "worksq" = "Yrs Work Exp ^2",
                "prestige" = "Prestige",
                "sexFemale" = "Female",
                "raceWhite" = "Race: White",
                "raceOther" = "Race: Other",
                "partyidRep" = "Party: Republican",
                "partyidOther" = "Party: Other",
                
                "(Intercept)" = "Constant")

modelsummary(model_list,
             estimate = c("{estimate}{stars}", # We want stars for the first model ONLY
                          "estimate",          # And only the estimate for the other two models
                          "estimate"),
             notes = c("+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001", # Explaining the stars
                       "Not shown: Work hours, sex, degree",
                       "Reference Categories: Male, Black, Democrat"),
             coef_map = coef_names,
             gof_omit = "IC|Log|alg|pss|RMSE|F",
             # output = "week5_final_table.docx",
             title = "Frequentist vs Bayesian Regression Model Output")
```

### Posterior Predictive Check
```{r pp_checks}
wpp <- pp_check(bayes_weight) + xlim(4.5,17) + 
  theme_light() + ggtitle("Weighted Bayes Model")
upp <- pp_check(bayes_noweight) + xlim(4.5,17) + 
  theme_light() + ggtitle("Unweighted Bayes Model")
ggpubr::ggarrange(wpp, upp, nrow = 2)
```

